<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>bRPC – RPC in depth</title>
    <link>https://brpc.incubator.apache.org/docs/rpc-in-depth/</link>
    <description>Recent content in RPC in depth on bRPC</description>
    <generator>Hugo -- gohugo.io</generator>
    <lastBuildDate>Thu, 12 Aug 2021 00:00:00 +0000</lastBuildDate>
    
	  <atom:link href="https://brpc.incubator.apache.org/docs/rpc-in-depth/index.xml" rel="self" type="application/rss+xml" />
    
    
      
        
      
    
    
    <item>
      <title>Docs: New Protocol</title>
      <link>https://brpc.incubator.apache.org/docs/rpc-in-depth/new-protocol/</link>
      <pubDate>Thu, 12 Aug 2021 00:00:00 +0000</pubDate>
      
      <guid>https://brpc.incubator.apache.org/docs/rpc-in-depth/new-protocol/</guid>
      <description>
        
        
        &lt;h1 id=&#34;multi-protocol-support-in-the-server-side&#34;&gt;Multi-protocol support in the server side&lt;/h1&gt;
&lt;p&gt;brpc server supports all protocols in the same port, and it makes deployment and maintenance more convenient in most of the time. Since the format of different protocols is very different, it is hard to support all protocols in the same port unambiguously. In consider of decoupling and extensibility, it is also hard to build a multiplexer for all protocols. Thus our way is to classify all protocols into three categories and try one by one:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;First-class protocol: Special characters are marked in front of the protocol data, for example, the data of protocol &lt;a href=&#34;https://github.com/apache/incubator-brpc/blob/master/docs/cn/baidu_std.md&#34;&gt;baidu_std&lt;/a&gt; and hulu_pbrpc begins with &amp;lsquo;PRPC&amp;rsquo; and &amp;lsquo;HULU&amp;rsquo; respectively. Parser just check first four characters to know whether the protocol is matched. This class of protocol is checked first and all these protocols can share one TCP connection.&lt;/li&gt;
&lt;li&gt;Second-class protocol: Some complex protocols without special marked characters can only be detected after several input data are parsed. Currently only HTTP is classified into this category.&lt;/li&gt;
&lt;li&gt;Third-class protocol: Special characters are in the middle of the protocol data, such as the magic number of nshead protocol is the 25th-28th characters. It is complex to handle this case because without reading first 28 bytes, we cannot determine whether the protocol is nshead. If it is tried before http, http messages less than 28 bytes may not be parsed, since the parser consider it as an incomplete nshead message.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Considering that there will be only one protocol in most connections, we record the result of last selection so that it will be tried first when further data comes. It reduces the overhead of matching protocols to nearly zero for long connections. Although the process of matching protocols will be run every time for short connections, the bottleneck of short connections is not in here and this method is still fast enough. If there are lots of new protocols added into brpc in the future, we may consider some heuristic methods to match protocols.&lt;/p&gt;
&lt;h1 id=&#34;multi-protocol-support-in-the-client-side&#34;&gt;Multi-protocol support in the client side&lt;/h1&gt;
&lt;p&gt;Unlike the server side that protocols must be dynamically determined based on the data on the connection, the client side as the originator, naturally know their own protocol format. As long as the protocol data is sent through connection pool or short connection, which means it has exclusive usage of that connection, then the protocol can have any complex (or bad) format. Since the client will record the protocol when sending the data, it will use that recorded protocol to parse the data without any matching overhead when responses come back. There is no magic number in some protocols like memcache, redis, it is hard to distinguish them in the server side, but it has no problem in the client side.&lt;/p&gt;
&lt;h1 id=&#34;support-new-protocols&#34;&gt;Support new protocols&lt;/h1&gt;
&lt;p&gt;brpc is designed to add new protocols at any time, just proceed as follows:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The protocol that begins with nshead has unified support, just read &lt;a href=&#34;https://github.com/apache/incubator-brpc/blob/master/docs/cn/nshead_service.md&#34;&gt;this&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;add-protocoltype&#34;&gt;add ProtocolType&lt;/h2&gt;
&lt;p&gt;Add new protocol type in ProtocolType in &lt;a href=&#34;https://github.com/brpc/brpc/blob/master/src/brpc/options.proto&#34;&gt;options.proto&lt;/a&gt;. If you need to add new protocol, please contact us to add it for you to make sure there is no conflict with protocols of others.&lt;/p&gt;
&lt;p&gt;Currently we support in ProtocolType(at the middle of 2018):&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-c++&#34; data-lang=&#34;c++&#34;&gt;&lt;span style=&#34;color:#204a87;font-weight:bold&#34;&gt;enum&lt;/span&gt; &lt;span style=&#34;color:#000&#34;&gt;ProtocolType&lt;/span&gt; &lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;{&lt;/span&gt;
    &lt;span style=&#34;color:#000&#34;&gt;PROTOCOL_UNKNOWN&lt;/span&gt; &lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#0000cf;font-weight:bold&#34;&gt;0&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;;&lt;/span&gt;
    &lt;span style=&#34;color:#000&#34;&gt;PROTOCOL_BAIDU_STD&lt;/span&gt; &lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#0000cf;font-weight:bold&#34;&gt;1&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;;&lt;/span&gt;
    &lt;span style=&#34;color:#000&#34;&gt;PROTOCOL_STREAMING_RPC&lt;/span&gt; &lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#0000cf;font-weight:bold&#34;&gt;2&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;;&lt;/span&gt;
    &lt;span style=&#34;color:#000&#34;&gt;PROTOCOL_HULU_PBRPC&lt;/span&gt; &lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#0000cf;font-weight:bold&#34;&gt;3&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;;&lt;/span&gt;
    &lt;span style=&#34;color:#000&#34;&gt;PROTOCOL_SOFA_PBRPC&lt;/span&gt; &lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#0000cf;font-weight:bold&#34;&gt;4&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;;&lt;/span&gt;
    &lt;span style=&#34;color:#000&#34;&gt;PROTOCOL_RTMP&lt;/span&gt; &lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#0000cf;font-weight:bold&#34;&gt;5&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;;&lt;/span&gt;
    &lt;span style=&#34;color:#000&#34;&gt;PROTOCOL_HTTP&lt;/span&gt; &lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#0000cf;font-weight:bold&#34;&gt;6&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;;&lt;/span&gt;
    &lt;span style=&#34;color:#000&#34;&gt;PROTOCOL_PUBLIC_PBRPC&lt;/span&gt; &lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#0000cf;font-weight:bold&#34;&gt;7&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;;&lt;/span&gt;
    &lt;span style=&#34;color:#000&#34;&gt;PROTOCOL_NOVA_PBRPC&lt;/span&gt; &lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#0000cf;font-weight:bold&#34;&gt;8&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;;&lt;/span&gt;
    &lt;span style=&#34;color:#000&#34;&gt;PROTOCOL_NSHEAD_CLIENT&lt;/span&gt; &lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#0000cf;font-weight:bold&#34;&gt;9&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;;&lt;/span&gt;        &lt;span style=&#34;color:#8f5902;font-style:italic&#34;&gt;// implemented in brpc-ub
&lt;/span&gt;&lt;span style=&#34;color:#8f5902;font-style:italic&#34;&gt;&lt;/span&gt;    &lt;span style=&#34;color:#000&#34;&gt;PROTOCOL_NSHEAD&lt;/span&gt; &lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#0000cf;font-weight:bold&#34;&gt;10&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;;&lt;/span&gt;
    &lt;span style=&#34;color:#000&#34;&gt;PROTOCOL_HADOOP_RPC&lt;/span&gt; &lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#0000cf;font-weight:bold&#34;&gt;11&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;;&lt;/span&gt;
    &lt;span style=&#34;color:#000&#34;&gt;PROTOCOL_HADOOP_SERVER_RPC&lt;/span&gt; &lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#0000cf;font-weight:bold&#34;&gt;12&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;;&lt;/span&gt;
    &lt;span style=&#34;color:#000&#34;&gt;PROTOCOL_MONGO&lt;/span&gt; &lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#0000cf;font-weight:bold&#34;&gt;13&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;;&lt;/span&gt;               &lt;span style=&#34;color:#8f5902;font-style:italic&#34;&gt;// server side only
&lt;/span&gt;&lt;span style=&#34;color:#8f5902;font-style:italic&#34;&gt;&lt;/span&gt;    &lt;span style=&#34;color:#000&#34;&gt;PROTOCOL_UBRPC_COMPACK&lt;/span&gt; &lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#0000cf;font-weight:bold&#34;&gt;14&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;;&lt;/span&gt;
    &lt;span style=&#34;color:#000&#34;&gt;PROTOCOL_DIDX_CLIENT&lt;/span&gt; &lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#0000cf;font-weight:bold&#34;&gt;15&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;;&lt;/span&gt;         &lt;span style=&#34;color:#8f5902;font-style:italic&#34;&gt;// Client side only
&lt;/span&gt;&lt;span style=&#34;color:#8f5902;font-style:italic&#34;&gt;&lt;/span&gt;    &lt;span style=&#34;color:#000&#34;&gt;PROTOCOL_REDIS&lt;/span&gt; &lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#0000cf;font-weight:bold&#34;&gt;16&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;;&lt;/span&gt;               &lt;span style=&#34;color:#8f5902;font-style:italic&#34;&gt;// Client side only
&lt;/span&gt;&lt;span style=&#34;color:#8f5902;font-style:italic&#34;&gt;&lt;/span&gt;    &lt;span style=&#34;color:#000&#34;&gt;PROTOCOL_MEMCACHE&lt;/span&gt; &lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#0000cf;font-weight:bold&#34;&gt;17&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;;&lt;/span&gt;            &lt;span style=&#34;color:#8f5902;font-style:italic&#34;&gt;// Client side only
&lt;/span&gt;&lt;span style=&#34;color:#8f5902;font-style:italic&#34;&gt;&lt;/span&gt;    &lt;span style=&#34;color:#000&#34;&gt;PROTOCOL_ITP&lt;/span&gt; &lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#0000cf;font-weight:bold&#34;&gt;18&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;;&lt;/span&gt;
    &lt;span style=&#34;color:#000&#34;&gt;PROTOCOL_NSHEAD_MCPACK&lt;/span&gt; &lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#0000cf;font-weight:bold&#34;&gt;19&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;;&lt;/span&gt;
    &lt;span style=&#34;color:#000&#34;&gt;PROTOCOL_DISP_IDL&lt;/span&gt; &lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#0000cf;font-weight:bold&#34;&gt;20&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;;&lt;/span&gt;            &lt;span style=&#34;color:#8f5902;font-style:italic&#34;&gt;// Client side only
&lt;/span&gt;&lt;span style=&#34;color:#8f5902;font-style:italic&#34;&gt;&lt;/span&gt;    &lt;span style=&#34;color:#000&#34;&gt;PROTOCOL_ERSDA_CLIENT&lt;/span&gt; &lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#0000cf;font-weight:bold&#34;&gt;21&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;;&lt;/span&gt;        &lt;span style=&#34;color:#8f5902;font-style:italic&#34;&gt;// Client side only
&lt;/span&gt;&lt;span style=&#34;color:#8f5902;font-style:italic&#34;&gt;&lt;/span&gt;    &lt;span style=&#34;color:#000&#34;&gt;PROTOCOL_UBRPC_MCPACK2&lt;/span&gt; &lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#0000cf;font-weight:bold&#34;&gt;22&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;;&lt;/span&gt;       &lt;span style=&#34;color:#8f5902;font-style:italic&#34;&gt;// Client side only
&lt;/span&gt;&lt;span style=&#34;color:#8f5902;font-style:italic&#34;&gt;&lt;/span&gt;    &lt;span style=&#34;color:#8f5902;font-style:italic&#34;&gt;// Reserve special protocol for cds-agent, which depends on FIFO right now
&lt;/span&gt;&lt;span style=&#34;color:#8f5902;font-style:italic&#34;&gt;&lt;/span&gt;    &lt;span style=&#34;color:#000&#34;&gt;PROTOCOL_CDS_AGENT&lt;/span&gt; &lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#0000cf;font-weight:bold&#34;&gt;23&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;;&lt;/span&gt;           &lt;span style=&#34;color:#8f5902;font-style:italic&#34;&gt;// Client side only
&lt;/span&gt;&lt;span style=&#34;color:#8f5902;font-style:italic&#34;&gt;&lt;/span&gt;    &lt;span style=&#34;color:#000&#34;&gt;PROTOCOL_ESP&lt;/span&gt; &lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#0000cf;font-weight:bold&#34;&gt;24&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;;&lt;/span&gt;                 &lt;span style=&#34;color:#8f5902;font-style:italic&#34;&gt;// Client side only
&lt;/span&gt;&lt;span style=&#34;color:#8f5902;font-style:italic&#34;&gt;&lt;/span&gt;    &lt;span style=&#34;color:#000&#34;&gt;PROTOCOL_THRIFT&lt;/span&gt; &lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#0000cf;font-weight:bold&#34;&gt;25&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;;&lt;/span&gt;              &lt;span style=&#34;color:#8f5902;font-style:italic&#34;&gt;// Server side only
&lt;/span&gt;&lt;span style=&#34;color:#8f5902;font-style:italic&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;implement-callbacks&#34;&gt;Implement Callbacks&lt;/h2&gt;
&lt;p&gt;All callbacks are defined in struct Protocol, which is defined in &lt;a href=&#34;https://github.com/brpc/brpc/blob/master/src/brpc/protocol.h&#34;&gt;protocol.h&lt;/a&gt;. Among all these callbacks, &lt;code&gt;parse&lt;/code&gt; is a callback that must be implemented. Besides, &lt;code&gt;process_request&lt;/code&gt; must be implemented in the server side and &lt;code&gt;serialize_request&lt;/code&gt;, &lt;code&gt;pack_request&lt;/code&gt;, &lt;code&gt;process_response&lt;/code&gt; must be implemented in the client side.&lt;/p&gt;
&lt;p&gt;It is difficult to implement callbacks of the protocol. These codes are not like the codes that ordinary users use which has good prompts and protections. You have to figure it out how to handle similar code in other protocols and implement your own protocol, then send it to us to do code review.&lt;/p&gt;
&lt;h3 id=&#34;parse&#34;&gt;parse&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-c++&#34; data-lang=&#34;c++&#34;&gt;&lt;span style=&#34;color:#204a87;font-weight:bold&#34;&gt;typedef&lt;/span&gt; &lt;span style=&#34;color:#000&#34;&gt;ParseResult&lt;/span&gt; &lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;(&lt;/span&gt;&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;*&lt;/span&gt;&lt;span style=&#34;color:#000&#34;&gt;Parse&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;)(&lt;/span&gt;&lt;span style=&#34;color:#000&#34;&gt;butil&lt;/span&gt;&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;::&lt;/span&gt;&lt;span style=&#34;color:#000&#34;&gt;IOBuf&lt;/span&gt;&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;*&lt;/span&gt; &lt;span style=&#34;color:#000&#34;&gt;source&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;,&lt;/span&gt; &lt;span style=&#34;color:#000&#34;&gt;Socket&lt;/span&gt; &lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;*&lt;/span&gt;&lt;span style=&#34;color:#000&#34;&gt;socket&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;,&lt;/span&gt; &lt;span style=&#34;color:#204a87;font-weight:bold&#34;&gt;bool&lt;/span&gt; &lt;span style=&#34;color:#000&#34;&gt;read_eof&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;,&lt;/span&gt; &lt;span style=&#34;color:#204a87;font-weight:bold&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#204a87;font-weight:bold&#34;&gt;void&lt;/span&gt; &lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;*&lt;/span&gt;&lt;span style=&#34;color:#000&#34;&gt;arg&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;);&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;This function is used to cut messages from source. Client side and server side must share the same parse function. The returned message will be passed to &lt;code&gt;process_request&lt;/code&gt;(server side) or &lt;code&gt;process_response&lt;/code&gt;(client side).&lt;/p&gt;
&lt;p&gt;Argument: source is the binary content from remote side, socket is the corresponding connection, read_eof is true iff the connection is closed by remote, arg is a pointer to the corresponding server in server client and NULL in client side.&lt;/p&gt;
&lt;p&gt;ParseResult could be an error or a cut message, its possible value contains:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;PARSE_ERROR_TRY_OTHERS: current protocol is not matched, the framework would try next protocol. The data in source cannot be comsumed.&lt;/li&gt;
&lt;li&gt;PARSE_ERROR_NOT_ENOUGH_DATA: the input data hasn&amp;rsquo;t violated the current protocol yet, but the whole message cannot be detected as well. When there is new data from connection, new data will be appended to source and parse function is called again. If we can determine that data fits current protocol, the content of source can also be transferred to the internal state of protocol. For example, if source doesn&amp;rsquo;t contain a whole http message, it will be consumed by http parser to avoid repeated parsing.&lt;/li&gt;
&lt;li&gt;PARSE_ERROR_TOO_BIG_DATA: message size is too big, the connection will be closed to protect server.&lt;/li&gt;
&lt;li&gt;PARSE_ERROR_NO_RESOURCE: internal error, such as resource allocation failure. Connections will be closed.&lt;/li&gt;
&lt;li&gt;PARSE_ERROR_ABSOLUTELY_WRONG: it is supposed to be some protocols(magic number is matched), but the format is not as expected. Connection will be closed.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;serialize_request&#34;&gt;serialize_request&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-c++&#34; data-lang=&#34;c++&#34;&gt;&lt;span style=&#34;color:#204a87;font-weight:bold&#34;&gt;typedef&lt;/span&gt; &lt;span style=&#34;color:#000&#34;&gt;bool&lt;/span&gt; &lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;(&lt;/span&gt;&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;*&lt;/span&gt;&lt;span style=&#34;color:#000&#34;&gt;SerializeRequest&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;)(&lt;/span&gt;&lt;span style=&#34;color:#000&#34;&gt;butil&lt;/span&gt;&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;::&lt;/span&gt;&lt;span style=&#34;color:#000&#34;&gt;IOBuf&lt;/span&gt;&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;*&lt;/span&gt; &lt;span style=&#34;color:#000&#34;&gt;request_buf&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;,&lt;/span&gt;
                                 &lt;span style=&#34;color:#000&#34;&gt;Controller&lt;/span&gt;&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;*&lt;/span&gt; &lt;span style=&#34;color:#000&#34;&gt;cntl&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;,&lt;/span&gt;
                                 &lt;span style=&#34;color:#204a87;font-weight:bold&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#000&#34;&gt;google&lt;/span&gt;&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;::&lt;/span&gt;&lt;span style=&#34;color:#000&#34;&gt;protobuf&lt;/span&gt;&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;::&lt;/span&gt;&lt;span style=&#34;color:#000&#34;&gt;Message&lt;/span&gt;&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;*&lt;/span&gt; &lt;span style=&#34;color:#000&#34;&gt;request&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;);&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;This function is used to serialize request into request_buf that client must implement. It happens before a RPC call and will only be called once. Necessary information needed by some protocols(such as http) is contained in cntl. Return true if succeed, otherwise false.&lt;/p&gt;
&lt;h3 id=&#34;pack_request&#34;&gt;pack_request&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-c++&#34; data-lang=&#34;c++&#34;&gt;&lt;span style=&#34;color:#204a87;font-weight:bold&#34;&gt;typedef&lt;/span&gt; &lt;span style=&#34;color:#000&#34;&gt;int&lt;/span&gt; &lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;(&lt;/span&gt;&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;*&lt;/span&gt;&lt;span style=&#34;color:#000&#34;&gt;PackRequest&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;)(&lt;/span&gt;&lt;span style=&#34;color:#000&#34;&gt;butil&lt;/span&gt;&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;::&lt;/span&gt;&lt;span style=&#34;color:#000&#34;&gt;IOBuf&lt;/span&gt;&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;*&lt;/span&gt; &lt;span style=&#34;color:#000&#34;&gt;msg&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;,&lt;/span&gt; 
                           &lt;span style=&#34;color:#204a87;font-weight:bold&#34;&gt;uint64_t&lt;/span&gt; &lt;span style=&#34;color:#000&#34;&gt;correlation_id&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;,&lt;/span&gt;
                           &lt;span style=&#34;color:#204a87;font-weight:bold&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#000&#34;&gt;google&lt;/span&gt;&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;::&lt;/span&gt;&lt;span style=&#34;color:#000&#34;&gt;protobuf&lt;/span&gt;&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;::&lt;/span&gt;&lt;span style=&#34;color:#000&#34;&gt;MethodDescriptor&lt;/span&gt;&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;*&lt;/span&gt; &lt;span style=&#34;color:#000&#34;&gt;method&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;,&lt;/span&gt;
                           &lt;span style=&#34;color:#000&#34;&gt;Controller&lt;/span&gt;&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;*&lt;/span&gt; &lt;span style=&#34;color:#000&#34;&gt;controller&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;,&lt;/span&gt;
                           &lt;span style=&#34;color:#204a87;font-weight:bold&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#000&#34;&gt;butil&lt;/span&gt;&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;::&lt;/span&gt;&lt;span style=&#34;color:#000&#34;&gt;IOBuf&lt;/span&gt;&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;&amp;amp;&lt;/span&gt; &lt;span style=&#34;color:#000&#34;&gt;request_buf&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;,&lt;/span&gt;
                           &lt;span style=&#34;color:#204a87;font-weight:bold&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#000&#34;&gt;Authenticator&lt;/span&gt;&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;*&lt;/span&gt; &lt;span style=&#34;color:#000&#34;&gt;auth&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;);&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;This function is used to pack request_buf into msg, which is called every time before sending messages to server(including retrying). When auth is not NULL, authentication information is also needed to be packed. Return 0 if succeed, otherwise -1.&lt;/p&gt;
&lt;h3 id=&#34;process_request&#34;&gt;process_request&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-c++&#34; data-lang=&#34;c++&#34;&gt;&lt;span style=&#34;color:#204a87;font-weight:bold&#34;&gt;typedef&lt;/span&gt; &lt;span style=&#34;color:#000&#34;&gt;void&lt;/span&gt; &lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;(&lt;/span&gt;&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;*&lt;/span&gt;&lt;span style=&#34;color:#000&#34;&gt;ProcessRequest&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;)(&lt;/span&gt;&lt;span style=&#34;color:#000&#34;&gt;InputMessageBase&lt;/span&gt;&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;*&lt;/span&gt; &lt;span style=&#34;color:#000&#34;&gt;msg_base&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;);&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;This function is used to parse request messages in the server side that server must implement. It and parse() may run in different threads. Multiple process_request may run simultaneously. After the processing is done, msg_base-&amp;gt;Destroy() must be called. In order to prevent forgetting calling Destroy, consider using DestroyingPtr&amp;lt;&amp;gt;.&lt;/p&gt;
&lt;h3 id=&#34;process_response&#34;&gt;process_response&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-c++&#34; data-lang=&#34;c++&#34;&gt;&lt;span style=&#34;color:#204a87;font-weight:bold&#34;&gt;typedef&lt;/span&gt; &lt;span style=&#34;color:#000&#34;&gt;void&lt;/span&gt; &lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;(&lt;/span&gt;&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;*&lt;/span&gt;&lt;span style=&#34;color:#000&#34;&gt;ProcessResponse&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;)(&lt;/span&gt;&lt;span style=&#34;color:#000&#34;&gt;InputMessageBase&lt;/span&gt;&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;*&lt;/span&gt; &lt;span style=&#34;color:#000&#34;&gt;msg&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;);&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;This function is used to parse message response in client side that client must implement. It and parse() may run in different threads. Multiple process_request may run simultaneously. After the processing is done, msg_base-&amp;gt;Destroy() must be called. In order to prevent forgetting calling Destroy, consider using DestroyingPtr&amp;lt;&amp;gt;.&lt;/p&gt;
&lt;h3 id=&#34;verify&#34;&gt;verify&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-c++&#34; data-lang=&#34;c++&#34;&gt;&lt;span style=&#34;color:#204a87;font-weight:bold&#34;&gt;typedef&lt;/span&gt; &lt;span style=&#34;color:#000&#34;&gt;bool&lt;/span&gt; &lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;(&lt;/span&gt;&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;*&lt;/span&gt;&lt;span style=&#34;color:#000&#34;&gt;Verify&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;)(&lt;/span&gt;&lt;span style=&#34;color:#204a87;font-weight:bold&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#000&#34;&gt;InputMessageBase&lt;/span&gt;&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;*&lt;/span&gt; &lt;span style=&#34;color:#000&#34;&gt;msg&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;);&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;This function is used to authenticate connections, it is called when the first message is received. It is must be implemented by servers that need authentication, otherwise the function pointer can be NULL. Return true if succeed, otherwise false.&lt;/p&gt;
&lt;h3 id=&#34;parse_server_address&#34;&gt;parse_server_address&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-c++&#34; data-lang=&#34;c++&#34;&gt;&lt;span style=&#34;color:#204a87;font-weight:bold&#34;&gt;typedef&lt;/span&gt; &lt;span style=&#34;color:#000&#34;&gt;bool&lt;/span&gt; &lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;(&lt;/span&gt;&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;*&lt;/span&gt;&lt;span style=&#34;color:#000&#34;&gt;ParseServerAddress&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;)(&lt;/span&gt;&lt;span style=&#34;color:#000&#34;&gt;butil&lt;/span&gt;&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;::&lt;/span&gt;&lt;span style=&#34;color:#000&#34;&gt;EndPoint&lt;/span&gt;&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;*&lt;/span&gt; &lt;span style=&#34;color:#000&#34;&gt;out&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;,&lt;/span&gt; &lt;span style=&#34;color:#204a87;font-weight:bold&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#204a87;font-weight:bold&#34;&gt;char&lt;/span&gt;&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;*&lt;/span&gt; &lt;span style=&#34;color:#000&#34;&gt;server_addr_and_port&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;);&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;This function converts server_addr_and_port(an argument of Channel.Init) to butil::EndPoint, which is optional. Some protocols may differ in the expression and understanding of server addresses.&lt;/p&gt;
&lt;h3 id=&#34;get_method_name&#34;&gt;get_method_name&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-c++&#34; data-lang=&#34;c++&#34;&gt;&lt;span style=&#34;color:#204a87;font-weight:bold&#34;&gt;typedef&lt;/span&gt; &lt;span style=&#34;color:#204a87;font-weight:bold&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#000&#34;&gt;std&lt;/span&gt;&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;::&lt;/span&gt;&lt;span style=&#34;color:#000&#34;&gt;string&lt;/span&gt;&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;&amp;amp;&lt;/span&gt; &lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;(&lt;/span&gt;&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;*&lt;/span&gt;&lt;span style=&#34;color:#000&#34;&gt;GetMethodName&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;)(&lt;/span&gt;&lt;span style=&#34;color:#204a87;font-weight:bold&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#000&#34;&gt;google&lt;/span&gt;&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;::&lt;/span&gt;&lt;span style=&#34;color:#000&#34;&gt;protobuf&lt;/span&gt;&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;::&lt;/span&gt;&lt;span style=&#34;color:#000&#34;&gt;MethodDescriptor&lt;/span&gt;&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;*&lt;/span&gt; &lt;span style=&#34;color:#000&#34;&gt;method&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;,&lt;/span&gt;
                                            &lt;span style=&#34;color:#204a87;font-weight:bold&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#000&#34;&gt;Controller&lt;/span&gt;&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;*&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;);&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;This function is used to customize method name, which is optional.&lt;/p&gt;
&lt;h3 id=&#34;supported_connection_type&#34;&gt;supported_connection_type&lt;/h3&gt;
&lt;p&gt;Used to mark the supported connection method. If all connection methods are supported, this value should set to CONNECTION_TYPE_ALL. If connection pools and short connections are supported, this value should set to CONNECTION_TYPE_POOLED_AND_SHORT.&lt;/p&gt;
&lt;h3 id=&#34;name&#34;&gt;name&lt;/h3&gt;
&lt;p&gt;The name of the protocol, which appears in the various configurations and displays, should be as short as possible and must be a string constant.&lt;/p&gt;
&lt;h2 id=&#34;register-to-global&#34;&gt;Register to global&lt;/h2&gt;
&lt;p&gt;RegisterProtocol should be called to &lt;a href=&#34;https://github.com/brpc/brpc/blob/master/src/brpc/global.cpp&#34;&gt;register implemented protocol&lt;/a&gt; to brpc, just like:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-c++&#34; data-lang=&#34;c++&#34;&gt;&lt;span style=&#34;color:#000&#34;&gt;Protocol&lt;/span&gt; &lt;span style=&#34;color:#000&#34;&gt;http_protocol&lt;/span&gt; &lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;{&lt;/span&gt; &lt;span style=&#34;color:#000&#34;&gt;ParseHttpMessage&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;,&lt;/span&gt;
                           &lt;span style=&#34;color:#000&#34;&gt;SerializeHttpRequest&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;,&lt;/span&gt; &lt;span style=&#34;color:#000&#34;&gt;PackHttpRequest&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;,&lt;/span&gt;
                           &lt;span style=&#34;color:#000&#34;&gt;ProcessHttpRequest&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;,&lt;/span&gt; &lt;span style=&#34;color:#000&#34;&gt;ProcessHttpResponse&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;,&lt;/span&gt;
                           &lt;span style=&#34;color:#000&#34;&gt;VerifyHttpRequest&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;,&lt;/span&gt; &lt;span style=&#34;color:#000&#34;&gt;ParseHttpServerAddress&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;,&lt;/span&gt;
                           &lt;span style=&#34;color:#000&#34;&gt;GetHttpMethodName&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;,&lt;/span&gt;
                           &lt;span style=&#34;color:#000&#34;&gt;CONNECTION_TYPE_POOLED_AND_SHORT&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;,&lt;/span&gt;
                           &lt;span style=&#34;color:#4e9a06&#34;&gt;&amp;#34;http&amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;};&lt;/span&gt;
&lt;span style=&#34;color:#204a87;font-weight:bold&#34;&gt;if&lt;/span&gt; &lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;(&lt;/span&gt;&lt;span style=&#34;color:#000&#34;&gt;RegisterProtocol&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;(&lt;/span&gt;&lt;span style=&#34;color:#000&#34;&gt;PROTOCOL_HTTP&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;,&lt;/span&gt; &lt;span style=&#34;color:#000&#34;&gt;http_protocol&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;)&lt;/span&gt; &lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;!=&lt;/span&gt; &lt;span style=&#34;color:#0000cf;font-weight:bold&#34;&gt;0&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;)&lt;/span&gt; &lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;{&lt;/span&gt;
    &lt;span style=&#34;color:#000&#34;&gt;exit&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;(&lt;/span&gt;&lt;span style=&#34;color:#0000cf;font-weight:bold&#34;&gt;1&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;);&lt;/span&gt;
&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
      </description>
    </item>
    
    <item>
      <title>Docs: Atomic instructions</title>
      <link>https://brpc.incubator.apache.org/docs/rpc-in-depth/atomic-instructions/</link>
      <pubDate>Thu, 12 Aug 2021 00:00:00 +0000</pubDate>
      
      <guid>https://brpc.incubator.apache.org/docs/rpc-in-depth/atomic-instructions/</guid>
      <description>
        
        
        &lt;p&gt;We know that locks are extensively used in multi-threaded programming to avoid &lt;a href=&#34;http://en.wikipedia.org/wiki/Race_condition&#34;&gt;race conditions&lt;/a&gt; when modifying shared data. When the lock becomes a bottleneck, we try to walk around it by using atomic instructions. But it is difficult to write correct code with atomic instructions in generally and it is even hard to understand race conditions, &lt;a href=&#34;https://en.wikipedia.org/wiki/ABA_problem&#34;&gt;ABA problems&lt;/a&gt; and &lt;a href=&#34;https://en.wikipedia.org/wiki/Memory_barrier&#34;&gt;memory fences&lt;/a&gt;. This article tries to introduce basics on atomic instructions(under &lt;a href=&#34;http://en.wikipedia.org/wiki/Symmetric_multiprocessing&#34;&gt;SMP&lt;/a&gt;). Since &lt;a href=&#34;http://en.cppreference.com/w/cpp/atomic/atomic&#34;&gt;Atomic instructions&lt;/a&gt; are formally introduced in C++11, we use the APIs directly.&lt;/p&gt;
&lt;p&gt;As the name implies, atomic instructions cannot be divided into more sub-instructions. For example, &lt;code&gt;x.fetch(n)&lt;/code&gt; atomically adds n to x, any internal state is not observable &lt;strong&gt;to software&lt;/strong&gt;. Common atomic instructions are listed below:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Atomic Instructions(type of x is std::atomic&amp;lt;int&amp;gt;)&lt;/th&gt;
&lt;th&gt;Descriptions&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;x.load()&lt;/td&gt;
&lt;td&gt;return the value of x.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;x.store(n)&lt;/td&gt;
&lt;td&gt;store n to x and return nothing.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;x.exchange(n)&lt;/td&gt;
&lt;td&gt;set x to n and return the value just before the modification&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;x.compare_exchange_strong(expected_ref, desired)&lt;/td&gt;
&lt;td&gt;If x is equal to expected_ref, set x to desired and return true. Otherwise write current x to expected_ref and return false.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;x.compare_exchange_weak(expected_ref, desired)&lt;/td&gt;
&lt;td&gt;may have &lt;a href=&#34;http://en.wikipedia.org/wiki/Spurious_wakeup&#34;&gt;spurious wakeup&lt;/a&gt; comparing to compare_exchange_strong&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;x.fetch_add(n), x.fetch_sub(n)&lt;/td&gt;
&lt;td&gt;do x += n, x-= n atomically. Return the value just before the modification.&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;You can already use these instructions to count something atomically, such as counting number of operations from multiple threads. However two problems may arise:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The operation is not as fast as expected.&lt;/li&gt;
&lt;li&gt;Even if multi-threaded accesses to some resources are controlled by a few atomic instructions that seem to be correct, the program still has great chance to crash.&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;cacheline&#34;&gt;Cacheline&lt;/h1&gt;
&lt;p&gt;An atomic instruction is fast when there&amp;rsquo;s no contentions or it&amp;rsquo;s accessed only by one thread. &amp;ldquo;Contentions&amp;rdquo; happen when multiple threads access the same &lt;a href=&#34;https://en.wikipedia.org/wiki/CPU_cache#Cache_entries&#34;&gt;cacheline&lt;/a&gt;. Modern CPU extensively use caches and divide caches into multiple levels to get high performance with a low price. The Intel E5-2620 widely used in Baidu has 32K L1 dcache and icache, 256K L2 cache and 15M L3 cache. L1 and L2 cache is owned by each core, while L3 cache is shared by all cores. Although it is very fast for one core to write data into its own L1 cache(4 cycles, ~2ns), make the data in L1 cache seen by other cores is not, because cachelines touched by the data need to be synchronized to other cores. This process is atomic and transparent to software and no instructions can be interleaved between. Applications have to wait for the completion of &lt;a href=&#34;https://en.wikipedia.org/wiki/Cache_coherence&#34;&gt;cache coherence&lt;/a&gt;, which takes much longer time than writing local cache. It involves a complicated hardware algorithm and makes atomic instructions slow under high contentions. A single fetch_add may take more than 700ns in E5-2620 when a few threads are highly contented on the instruction. Accesses to the memory frequently shared and modified by multiple threads are not fast generally. For example, even if a critical section looks small, the spinlock protecting it may still not perform well. The cause is that the instructions used in spinlock such as exchange, fetch_add etc, need to wait for latest cachelines. It&amp;rsquo;s not surprising to see that one or two instructions take several microseconds.&lt;/p&gt;
&lt;p&gt;In order to improve performance, we need to avoid frequently synchronizing cachelines, which not only affects performance of the atomic instruction itself, but also overall performance of the program. The most effective solution is straightforward: &lt;strong&gt;avoid sharing as much as possible&lt;/strong&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A program relying on a global multiple-producer-multiple-consumer(MPMC) queue is hard to scale well on many CPU cores, because throughput of the queue is limited by delays of cache coherence, rather than number of cores. It would be better to use multiple SPMC or MPSC queues, or even SPSC queues instead, to avoid contentions from the very beginning.&lt;/li&gt;
&lt;li&gt;Another example is counters. If all threads modify a counter frequently, the performance will be poor because all cores are busy synchronizing the same cacheline. If the counter is only used for printing logs periodically or something low-priority like that, we can let each thread modify its own thread-local variables and combine all thread-local data before reading, yielding &lt;a href=&#34;../../bvar/bvar/&#34;&gt;much better performance&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;A related programming trap is false sharing: Accesses to infrequently updated or even read-only variables are significantly slowed down because other variables in the same cacheline are frequently updated. Variables used in multi-threaded environment should be grouped by accessing frequencies or patterns, variables that are modified by that other threads frequently should be put into separated cachelines. To align a variable or struct by cacheline, &lt;code&gt;include &amp;lt;butil/macros.h&amp;gt;&lt;/code&gt; and tag it with macro &lt;code&gt;BAIDU_CACHELINE_ALIGNMENT&lt;/code&gt;, grep source code of brpc for examples.&lt;/p&gt;
&lt;h1 id=&#34;memory-fence&#34;&gt;Memory fence&lt;/h1&gt;
&lt;p&gt;Just atomic counting cannot synchronize accesses to resources, simple structures like &lt;a href=&#34;https://en.wikipedia.org/wiki/Spinlock&#34;&gt;spinlock&lt;/a&gt; or &lt;a href=&#34;https://en.wikipedia.org/wiki/Reference_counting&#34;&gt;reference counting&lt;/a&gt; that seem correct may crash as well. The key is &lt;strong&gt;instruction reordering&lt;/strong&gt;, which may change the order of read/write and cause instructions behind to be reordered to front if there are no dependencies. &lt;a href=&#34;http://preshing.com/20120625/memory-ordering-at-compile-time/&#34;&gt;Compiler&lt;/a&gt; and &lt;a href=&#34;https://en.wikipedia.org/wiki/Out-of-order_execution&#34;&gt;CPU&lt;/a&gt; both may reorder.&lt;/p&gt;
&lt;p&gt;The motivation is natural: CPU wants to fill each cycle with instructions and execute as many as possible instructions within given time. As said in above section, an instruction for loading memory may cost hundreds of nanoseconds due to cacheline synchronization. A efficient solution for synchronizing multiple cachelines is to move them simultaneously rather than one-by-one. Thus modifications to multiple variables by a thread may be visible to another thread in a different order. On the other hand, different threads need different data, synchronizing on-demand is reasonable and may also change order between cachelines.&lt;/p&gt;
&lt;p&gt;For example: the first variable plays the role of switch, controlling accesses to following variables. When these variables are synchronized to other CPU cores, new values may be visible in a different order, and the first variable may not be the first updated, which causes other threads to think that the following variables are still valid, which are actually not, causing undefined behavior. Check code snippet below:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-c++&#34; data-lang=&#34;c++&#34;&gt;&lt;span style=&#34;color:#8f5902;font-style:italic&#34;&gt;// Thread 1
&lt;/span&gt;&lt;span style=&#34;color:#8f5902;font-style:italic&#34;&gt;// ready was initialized to false
&lt;/span&gt;&lt;span style=&#34;color:#8f5902;font-style:italic&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#000&#34;&gt;p&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;.&lt;/span&gt;&lt;span style=&#34;color:#000&#34;&gt;init&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;();&lt;/span&gt;
&lt;span style=&#34;color:#000&#34;&gt;ready&lt;/span&gt; &lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#204a87&#34;&gt;true&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-c++&#34; data-lang=&#34;c++&#34;&gt;&lt;span style=&#34;color:#8f5902;font-style:italic&#34;&gt;// Thread2
&lt;/span&gt;&lt;span style=&#34;color:#8f5902;font-style:italic&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#204a87;font-weight:bold&#34;&gt;if&lt;/span&gt; &lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;(&lt;/span&gt;&lt;span style=&#34;color:#000&#34;&gt;ready&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;)&lt;/span&gt; &lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;{&lt;/span&gt;
    &lt;span style=&#34;color:#000&#34;&gt;p&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;.&lt;/span&gt;&lt;span style=&#34;color:#000&#34;&gt;bar&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;();&lt;/span&gt;
&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;From a human perspective, the code is correct because thread2 only accesses &lt;code&gt;p&lt;/code&gt; when &lt;code&gt;ready&lt;/code&gt; is true which means p is initialized according to the logic in thread1. But the code may not run as expected on multi-core machines:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;ready = true&lt;/code&gt; in thread1 may be reordered before &lt;code&gt;p.init()&lt;/code&gt; by compiler or CPU, making thread2 see uninitialized &lt;code&gt;p&lt;/code&gt; when  &lt;code&gt;ready&lt;/code&gt; is true. The same reordering may happen in thread2 as well. Some instructions in &lt;code&gt;p.bar()&lt;/code&gt; may be reordered before checking &lt;code&gt;ready&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Even if above reorderings do not happen, cachelines of &lt;code&gt;ready&lt;/code&gt; and &lt;code&gt;p&lt;/code&gt; may be synchronized independently to the CPU core that thread2 runs, making thread2 see unitialized &lt;code&gt;p&lt;/code&gt; when &lt;code&gt;ready&lt;/code&gt; is true.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Note: On x86/x64, &lt;code&gt;load&lt;/code&gt; has acquire semantics and &lt;code&gt;store&lt;/code&gt; has release semantics by default, the code above may run correctly provided that reordering by compiler is turned off.&lt;/p&gt;
&lt;p&gt;With this simple example, you may get a glimpse of the complexity of atomic instructions. In order to solve the reordering issue, CPU and compiler offer &lt;a href=&#34;http://en.wikipedia.org/wiki/Memory_barrier&#34;&gt;memory fences&lt;/a&gt; to let programmers decide the visibility order between instructions. boost and C++11 conclude memory fence into following types:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;memory order&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;memory_order_relaxed&lt;/td&gt;
&lt;td&gt;there are no synchronization or ordering constraints imposed on other reads or writes, only this operation&amp;rsquo;s atomicity is guaranteed&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;memory_order_consume&lt;/td&gt;
&lt;td&gt;no reads or writes in the current thread dependent on the value currently loaded can be reordered before this load.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;memory_order_acquire&lt;/td&gt;
&lt;td&gt;no reads or writes in the current thread can be reordered before this load.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;memory_order_release&lt;/td&gt;
&lt;td&gt;no reads or writes in the current thread can be reordered after this store.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;memory_order_acq_rel&lt;/td&gt;
&lt;td&gt;No memory reads or writes in the current thread can be reordered before or after this store.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;memory_order_seq_cst&lt;/td&gt;
&lt;td&gt;Any operation with this memory order is both an acquire operation and a release operation, plus a single total order exists in which all threads observe all modifications in the same order.&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Above example can be modified as follows:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-c++&#34; data-lang=&#34;c++&#34;&gt;&lt;span style=&#34;color:#8f5902;font-style:italic&#34;&gt;// Thread1
&lt;/span&gt;&lt;span style=&#34;color:#8f5902;font-style:italic&#34;&gt;// ready was initialized to false
&lt;/span&gt;&lt;span style=&#34;color:#8f5902;font-style:italic&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#000&#34;&gt;p&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;.&lt;/span&gt;&lt;span style=&#34;color:#000&#34;&gt;init&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;();&lt;/span&gt;
&lt;span style=&#34;color:#000&#34;&gt;ready&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;.&lt;/span&gt;&lt;span style=&#34;color:#000&#34;&gt;store&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;(&lt;/span&gt;&lt;span style=&#34;color:#204a87&#34;&gt;true&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;,&lt;/span&gt; &lt;span style=&#34;color:#000&#34;&gt;std&lt;/span&gt;&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;::&lt;/span&gt;&lt;span style=&#34;color:#000&#34;&gt;memory_order_release&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;);&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-c++&#34; data-lang=&#34;c++&#34;&gt;&lt;span style=&#34;color:#8f5902;font-style:italic&#34;&gt;// Thread2
&lt;/span&gt;&lt;span style=&#34;color:#8f5902;font-style:italic&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#204a87;font-weight:bold&#34;&gt;if&lt;/span&gt; &lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;(&lt;/span&gt;&lt;span style=&#34;color:#000&#34;&gt;ready&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;.&lt;/span&gt;&lt;span style=&#34;color:#000&#34;&gt;load&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;(&lt;/span&gt;&lt;span style=&#34;color:#000&#34;&gt;std&lt;/span&gt;&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;::&lt;/span&gt;&lt;span style=&#34;color:#000&#34;&gt;memory_order_acquire&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;))&lt;/span&gt; &lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;{&lt;/span&gt;
    &lt;span style=&#34;color:#000&#34;&gt;p&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;.&lt;/span&gt;&lt;span style=&#34;color:#000&#34;&gt;bar&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;();&lt;/span&gt;
&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The acquire fence in thread2 matches the release fence in thread1, making thread2 see all memory operations before the release fence in thread1 when thread2 sees &lt;code&gt;ready&lt;/code&gt; being set to true.&lt;/p&gt;
&lt;p&gt;Note that memory fence does not guarantee visibility. Even if thread2 reads &lt;code&gt;ready&lt;/code&gt; just after thread1 sets it to true, thread2 is not guaranteed to see the new value, because cache synchronization takes time. Memory fence guarantees ordering between visibilities: &amp;ldquo;If I see the new value of a, I should see the new value of b as well&amp;rdquo;.&lt;/p&gt;
&lt;p&gt;A related problem: How to know whether a value is updated or not? Two cases in generally:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The value is special. In above example, &lt;code&gt;ready=true&lt;/code&gt; is a special value. Once &lt;code&gt;ready&lt;/code&gt; is true, &lt;code&gt;p&lt;/code&gt; is ready. Reading special values or not both mean something.&lt;/li&gt;
&lt;li&gt;Increasing-only values. Some situations do not have special values, we can use instructions like &lt;code&gt;fetch_add&lt;/code&gt; to increase variables. As long as the value range is large enough, new values are different from old ones for a long period of time, so that we can distinguish them from each other.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;More examples can be found in &lt;a href=&#34;http://www.boost.org/doc/libs/1_56_0/doc/html/atomic/usage_examples.html&#34;&gt;boost.atomic&lt;/a&gt;. Official descriptions of atomics can be found &lt;a href=&#34;http://en.cppreference.com/w/cpp/atomic/atomic&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h1 id=&#34;wait-free--lock-free&#34;&gt;wait-free &amp;amp; lock-free&lt;/h1&gt;
&lt;p&gt;Atomic instructions provide two important properties: &lt;a href=&#34;http://en.wikipedia.org/wiki/Non-blocking_algorithm#Wait-freedom&#34;&gt;wait-free&lt;/a&gt; and &lt;a href=&#34;http://en.wikipedia.org/wiki/Non-blocking_algorithm#Lock-freedom&#34;&gt;lock-free&lt;/a&gt;. Wait-free means no matter how OS schedules, all threads are doing useful jobs; lock-free, which is weaker than wait-free, means no matter how OS schedules, at least one thread is doing useful jobs. If locks are used, the thread holding the lock might be paused by OS, in which case all threads trying to hold the lock are blocked. So code using locks are neither lock-free nor wait-free. To make tasks done within given time, critical paths in real-time OS is at least lock-free. Miscellaneous online services inside Baidu also pose serious restrictions on running time. If the critical path in brpc is wait-free or lock-free, many services are benefited from better and stable QoS. Actually, both read(in the sense of even dispatching) and write in brpc are wait-free, check &lt;a href=&#34;../io&#34;&gt;IO&lt;/a&gt; for details.&lt;/p&gt;
&lt;p&gt;Note that it is common to think that wait-free or lock-free algorithms are faster, which may not be true, because:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;More complex race conditions and ABA problems must be handled in lock-free and wait-free algorithms, which means the code is often much more complicated than the one using locks. More code, more running time.&lt;/li&gt;
&lt;li&gt;Mutex solves contentions by backoff, which means that when contention happens, another branch is entered to avoid the contention temporarily. Threads failed to lock a mutex are put into sleep, making the thread holding the mutex complete the task or even follow-up tasks exclusively, which may increase the overall throughput.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Low performance caused by mutex is either because of too large critical sections (which limit the concurrency), or too heavy contentions (overhead of context switches becomes dominating). The real value of lock-free/wait-free algorithms is that they guarantee progress of the system, rather than absolutely high performance. Of course lock-free/wait-free algorithms perform better in some situations: if an algorithm is implemented by just one or two atomic instructions, it&amp;rsquo;s probably faster than the one using mutex which needs more atomic instructions in total.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: IO</title>
      <link>https://brpc.incubator.apache.org/docs/rpc-in-depth/io/</link>
      <pubDate>Thu, 12 Aug 2021 00:00:00 +0000</pubDate>
      
      <guid>https://brpc.incubator.apache.org/docs/rpc-in-depth/io/</guid>
      <description>
        
        
        &lt;p&gt;There are three mechanisms to operate IO in general:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Blocking IO: once an IO operation is issued, the calling thread is blocked until the IO ends, which is a kind of synchronous IO, such as default actions of posix &lt;a href=&#34;http://linux.die.net/man/2/read&#34;&gt;read&lt;/a&gt; and &lt;a href=&#34;http://linux.die.net/man/2/write&#34;&gt;write&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Non-blocking IO: If there is nothing to read or too much to write, APIs that would block return immediately with an error code. Non-blocking IO is often used with IO multiplexing(&lt;a href=&#34;http://linux.die.net/man/2/poll&#34;&gt;poll&lt;/a&gt;, &lt;a href=&#34;http://linux.die.net/man/2/select&#34;&gt;select&lt;/a&gt;, &lt;a href=&#34;http://linux.die.net/man/4/epoll&#34;&gt;epoll&lt;/a&gt; in Linux or &lt;a href=&#34;https://www.freebsd.org/cgi/man.cgi?query=kqueue&amp;amp;sektion=2&#34;&gt;kqueue&lt;/a&gt; in BSD).&lt;/li&gt;
&lt;li&gt;Asynchronous IO: Start a read or write operation with a callback, which will be called when the IO is done, such as &lt;a href=&#34;https://msdn.microsoft.com/en-us/library/windows/desktop/ms684342(v=vs.85).aspx&#34;&gt;OVERLAPPED&lt;/a&gt; + &lt;a href=&#34;https://msdn.microsoft.com/en-us/library/windows/desktop/aa365198(v=vs.85).aspx&#34;&gt;IOCP&lt;/a&gt; in Windows. Native AIO in Linux only supports files.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Non-blocking IO is usually used for increasing IO concurrency in Linux. When the IO concurrency is low, non-blocking IO is not necessarily more efficient than blocking IO, which is handled completely by the kernel. System calls like read/write are highly optimized and probably more efficient. But when IO concurrency increases, the drawback of blocking-one-thread in blocking IO arises: the kernel keeps switching between threads to do effective jobs, and a CPU core may only do a little bit of work before being replaced by another thread, causing CPU cache not fully utilized. In addition a large number of threads decrease performance of code dependent on thread-local variables, such as tcmalloc. Once malloc slows down, the overall performance of the program decreases as well. As a contrast, non-blocking IO is typically composed with a relatively small number of event dispatching threads and worker threads(running user code), which are reused by different tasks (in another word, part of scheduling work is moved to userland). Event dispatchers and workers run on different CPU cores simultaneously without frequent switches in the kernel. There&amp;rsquo;s no need to have many threads, so the use of thread-local variables is also more adequate. All these factors make non-blocking IO faster than blocking IO. But non-blocking IO also has its own problems, one of which is more system calls, such as &lt;a href=&#34;http://man7.org/linux/man-pages/man2/epoll_ctl.2.html&#34;&gt;epoll_ctl&lt;/a&gt;. Since epoll is implemented as a red-black tree, epoll_ctl is not a very fast operation, especially in multi-threaded environments. Implementations heavily dependent on epoll_ctl is often confronted with multi-core scalability issues. Non-blocking IO also has to solve a lot of multi-threaded problems, producing more complex code than blocking IO.&lt;/p&gt;
&lt;h1 id=&#34;receiving-messages&#34;&gt;Receiving messages&lt;/h1&gt;
&lt;p&gt;A message is a bounded binary data read from a connection, which may be a request from upstream clients or a response from downstream servers. brpc uses one or several &lt;a href=&#34;https://github.com/brpc/brpc/blob/master/src/brpc/event_dispatcher.cpp&#34;&gt;EventDispatcher&lt;/a&gt;(referred to as EDISP) to wait for events from file descriptors. Unlike the common &amp;ldquo;IO threads&amp;rdquo;, EDISP is not responsible for reading or writing. The problem of IO threads is that one thread can only read one fd at a given time, other reads may be delayed when many fds in one IO thread are busy. Multi-tenancy, complicated load balancing and &lt;a href=&#34;../../client/streaming-rpc/&#34;&gt;Streaming RPC&lt;/a&gt; worsen the problem. Under high workloads, regular long delays on one fd may slow down all fds in the IO thread, causing more long tails.&lt;/p&gt;
&lt;p&gt;Because of a &lt;a href=&#34;https://patchwork.kernel.org/patch/1970231/&#34;&gt;bug&lt;/a&gt; of epoll (at the time of developing brpc) and overhead of epoll_ctl, edge triggered mode is used in EDISP. After receiving an event, an atomic variable associated with the fd is added by one atomically. If the variable is zero before addition, a bthread is started to handle the data from the fd. The pthread worker in which EDISP runs is yielded to the newly created bthread to make it start reading ASAP and have a better cache locality. The bthread in which EDISP runs will be stolen to another pthread and keep running, this mechanism is work stealing used in bthreads. To understand exactly how that atomic variable works, you can read &lt;a href=&#34;../atomic-instructions/&#34;&gt;atomic instructions&lt;/a&gt; first, then check &lt;a href=&#34;https://github.com/brpc/brpc/blob/master/src/brpc/socket.cpp&#34;&gt;Socket::StartInputEvent&lt;/a&gt;. These methods make contentions on dispatching events of one fd &lt;a href=&#34;http://en.wikipedia.org/wiki/Non-blocking_algorithm#Wait-freedom&#34;&gt;wait-free&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/brpc/brpc/blob/master/src/brpc/input_messenger.h&#34;&gt;InputMessenger&lt;/a&gt; cuts messages and uses customizable callbacks to handle different format of data. &lt;code&gt;Parse&lt;/code&gt; callback cuts messages from binary data and has relatively stable running time; &lt;code&gt;Process&lt;/code&gt; parses messages further(such as parsing by protobuf) and calls users&#39; callbacks, which vary in running time. If n(n &amp;gt; 1) messages are read from the fd, InputMessenger launches n-1 bthreads to handle first n-1 messages respectively, and processes the last message in-place. InputMessenger tries protocols one by one. Since one connections often has only one type of messages, InputMessenger remembers current protocol to avoid trying for protocols next time.&lt;/p&gt;
&lt;p&gt;It can be seen that messages from different fds or even same fd are processed concurrently in brpc, which makes brpc good at handling large messages and reducing long tails on processing messages from different sources under high workloads.&lt;/p&gt;
&lt;h1 id=&#34;sending-messages&#34;&gt;Sending Messages&lt;/h1&gt;
&lt;p&gt;A message is a bounded binary data written to a connection, which may be a response to upstream clients or a request to downstream servers. Multiple threads may send messages to a fd at the same time, however writing to a fd is non-atomic, so how to queue writes from different thread efficiently is a key technique. brpc uses a special wait-free MPSC list to solve the issue. All data ready to write is put into a node of a singly-linked list, whose next pointer points to a special value(&lt;code&gt;Socket::WriteRequest::UNCONNECTED&lt;/code&gt;). When a thread wants to write out some data, it tries to atomically exchange the node with the list head(Socket::_write_head) first. If the head before exchange is empty, the caller gets the right to write and writes out the data in-place once. Otherwise there must be another thread writing. The caller points the next pointer to the head returned to make the linked list connected. The thread that is writing will see the new head later and write new data.&lt;/p&gt;
&lt;p&gt;This method makes the writing contentions wait-free. Although the thread that gets the right to write is not wait-free nor lock-free in principle and may be blocked by a node that is still UNCONNECTED(the thread issuing write is swapped out by OS just after atomic exchange and before setting the next pointer, within execution time of just one instruction), the blocking rarely happens in practice. In current implementations, if the data cannot be written fully in one call, a KeepWrite bthread is created to write the remaining data. This mechanism is pretty complicated and the principle is depicted below. Read &lt;a href=&#34;https://github.com/brpc/brpc/blob/master/src/brpc/socket.cpp&#34;&gt;socket.cpp&lt;/a&gt; for more details.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://brpc.incubator.apache.org/images/docs/write.png&#34; alt=&#34;img&#34;&gt;&lt;/p&gt;
&lt;p&gt;Since writes in brpc always complete within short time, the calling thread can handle new tasks more quickly and background KeepWrite threads also get more tasks to write in one batch, forming pipelines and increasing the efficiency of IO at high throughputs.&lt;/p&gt;
&lt;h1 id=&#34;socket&#34;&gt;Socket&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/brpc/brpc/blob/master/src/brpc/socket.h&#34;&gt;Socket&lt;/a&gt; contains data structures related to fd and is one of the most complex structure in brpc. The unique feature of this structure is that it uses 64-bit SocketId to refer to a Socket object to facilitate usages of fd in multi-threaded environments. Commonly used methods:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Create: create a Socket and return its SocketId.&lt;/li&gt;
&lt;li&gt;Address: retrieve Socket from an id, and wrap it into a unique_ptr(SocketUniquePtr) that will be automatically released. When Socket is set failed, the pointer returned is empty. As long as Address returns a non-null pointer, the contents are guaranteed to not change until the pointer is destructed. This function is wait-free.&lt;/li&gt;
&lt;li&gt;SetFailed: Mark a Socket as failed and Address() on corresponding SocketId will return empty pointer (until health checking resumes the socket). Sockets are recycled when no one is referencing it anymore. This function is lock-free.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We can see that, Socket is similar to &lt;a href=&#34;http://en.cppreference.com/w/cpp/memory/shared_ptr&#34;&gt;shared_ptr&lt;/a&gt; in the sense of referential counting and SocketId is similar to &lt;a href=&#34;http://en.cppreference.com/w/cpp/memory/weak_ptr&#34;&gt;weak_ptr&lt;/a&gt;. The unique &lt;code&gt;SetFailed&lt;/code&gt; prevents Socket from being addressed so that the reference count can hit zero finally. Simply using shared_ptr/weak_ptr cannot guarantee this. For example, when a server needs to quit when requests are still coming in frequently, the reference count of Socket may not hit zero and the server is unable to stop quickly. What&#39; more, weak_ptr cannot be directly put into epoll data, but SocketId can. These factors lead to design of Socket which is stable and rarely changed since 2014.&lt;/p&gt;
&lt;p&gt;Using SocketUniquePtr or SocketId depends on if a strong reference is needed. For example, Controller is used thoroughly inside RPC and has a lot of interactions with Socket, it uses SocketUniquePtr. Epoll notifies events on fds and events of a recycled socket can be ignored, so epoll uses SocketId.&lt;/p&gt;
&lt;p&gt;As long as SocketUniquePtr is valid, the Socket enclosed will not be changed so that users have no need to care about race conditions and ABA problems, being safer to operate the shared socket. This method also circumvents implicit referential counting and makes ownership of memory more clear, producing better-quality programs. brpc uses SocketUniquePtr and SocketId a lot to simplify related issues.&lt;/p&gt;
&lt;p&gt;In fact, Socket manages not only the native fd but also other resources, such as SubChannel in SelectiveChannel is also managed by Socket, making SelectiveChannel choose a SubChannel just like a normal channel choosing a downstream server. The faked Socket even implements health checking. Streaming RPC also uses Socket to reuse the code on wait-free write.&lt;/p&gt;
&lt;h1 id=&#34;the-full-picture&#34;&gt;The full picture&lt;/h1&gt;
&lt;p&gt;&lt;img src=&#34;https://brpc.incubator.apache.org/images/docs/rpc_flow.png&#34; alt=&#34;img&#34;&gt;&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: Threading Overview</title>
      <link>https://brpc.incubator.apache.org/docs/rpc-in-depth/threading-overview/</link>
      <pubDate>Thu, 12 Aug 2021 00:00:00 +0000</pubDate>
      
      <guid>https://brpc.incubator.apache.org/docs/rpc-in-depth/threading-overview/</guid>
      <description>
        
        
        &lt;h1 id=&#34;common-threading-models&#34;&gt;Common threading models&lt;/h1&gt;
&lt;h2 id=&#34;connections-own-threads-or-processes-exclusively&#34;&gt;Connections own threads or processes exclusively&lt;/h2&gt;
&lt;p&gt;In this model, a thread/process handles all messages from a connection and does not quit or do other jobs before the connection is closed. When number of connections increases, resources occupied by threads/processes and costs of context switches becomes more and more overwhelming, making servers perform poorly. This situation is summarized as the &lt;a href=&#34;http://en.wikipedia.org/wiki/C10k_problem&#34;&gt;C10K&lt;/a&gt; problem, which was common in early web servers but rarely present today.&lt;/p&gt;
&lt;h2 id=&#34;single-threaded-reactorhttpenwikipediaorgwikireactor_pattern&#34;&gt;Single-threaded &lt;a href=&#34;http://en.wikipedia.org/wiki/Reactor_pattern&#34;&gt;reactor&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Event-loop libraries such as &lt;a href=&#34;http://libevent.org/&#34;&gt;libevent&lt;/a&gt;, &lt;a href=&#34;http://software.schmorp.de/pkg/libev.html&#34;&gt;libev&lt;/a&gt; are typical examples. There&amp;rsquo;s usually an event dispatcher in this model responsible for waiting on different kinds of events and calling the corresponding event handler &lt;strong&gt;in-place&lt;/strong&gt; when an event occurs. After all handlers(that need to be called) are called, the dispatcher waits for more events again, which forms a &amp;ldquo;loop&amp;rdquo;. Essentially this model multiplexes(interleaves) code written in different handlers into a system thread. One event-loop can only utilize one core, so this kind of program is either IO-bound or each handler runs within short and deterministic time(such as http servers), otherwise one callback taking long time blocks the whole program and causes high delays. In practice this kind of program is not suitable for involving many developers, because just one person adding inappropriate blocking code may significantly slow down reactivities of all other code. Since event handlers don&amp;rsquo;t run simultaneously, race conditions between callbacks are relatively simple and in some scenarios locks are not needed. These programs are often scaled by deploying more processes.&lt;/p&gt;
&lt;p&gt;How single-threaded reactors work and the problems related are demonstrated below: (The Chinese characters in red: &amp;ldquo;Uncontrollable! unless the service is specialized&amp;rdquo;)&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://brpc.incubator.apache.org/images/docs/threading_overview_1.png&#34; alt=&#34;img&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;n1-threading-library&#34;&gt;N:1 threading library&lt;/h2&gt;
&lt;p&gt;Also known as &lt;a href=&#34;http://en.wikipedia.org/wiki/Fiber_(computer_science)&#34;&gt;Fiber&lt;/a&gt;. Typical examples are &lt;a href=&#34;http://www.gnu.org/software/pth/pth-manual.html&#34;&gt;GNU Pth&lt;/a&gt;, &lt;a href=&#34;http://state-threads.sourceforge.net/index.html&#34;&gt;StateThreads&lt;/a&gt;. This model maps N user threads into a single system thread, in which only one user thread runs at the same time and the running user thread does not switch to other user threads until a blocking primitive is called (cooperative). N:1 threading libraries are equal to single-threaded reactors on capabilities, except that callbacks are replaced by contexts (stacks, registers, signals) and running callbacks becomes jumping to contexts. Similar to event-loop libraries, a N:1 threading library cannot utilize multiple CPU cores, thus only suitable for specialized applications. However a single system thread is more friendly to CPU caches, with removal of the support for signal masks, context switches between user threads can be done very fast(100 ~ 200ns). N:1 threading libraries perform as well as event-loop libraries and are also scaled by deploying more processes in general.&lt;/p&gt;
&lt;h2 id=&#34;multi-threaded-reactor&#34;&gt;Multi-threaded reactor&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;http://www.boost.org/doc/libs/1_56_0/doc/html/boost_asio.html&#34;&gt;boost::asio&lt;/a&gt; is a typical example. One or several threads run event dispatchers respectively. When an event occurs, the event handler is queued into a worker thread to run. This model is extensible from single-threaded reactor intuitively and able to make use of multiple CPU cores. Since sharing memory addresses makes interactions between threads much cheaper, the worker threads are able to balance loads between each other frequently, as a contrast multiple single-threaded reactors basically depend on the front-end servers to distribute traffic. A well-implemented multi-threaded reactor is likely to utilize CPU cores more evenly than multiple single-threaded reactors on the same machine. However, due to &lt;a href=&#34;../atomic-instructions/#cacheline&#34;&gt;cache coherence&lt;/a&gt;, multi-threaded reactors are unlikely to achieve linear scalability on CPU cores. In particular scenarios, a badly implemented multi-threaded reactor running on 24 cores is even slower than a well-tuned single-threaded reactor. Because a multi-threaded reactor has multiple worker threads, one blocking event handler may not delay other handlers. As a result, event handlers are not required to be non-blocking unless all worker threads are blocked, in which case the overall progress is affected. In fact, most RPC frameworks are implemented in this model with event handlers that may block, such as synchronously waiting for RPCs to downstream servers.&lt;/p&gt;
&lt;p&gt;How multi-threaded reactors work and problems related are demonstrated below:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://brpc.incubator.apache.org/images/docs/threading_overview_2.png&#34; alt=&#34;img&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;mn-threading-library&#34;&gt;M:N threading library&lt;/h2&gt;
&lt;p&gt;This model maps M user threads into N system threads. A M:N threading library is able to decide when and where to run a piece of code and when to end the execution, which is more flexible at scheduling compared to multi-threaded reactors. But full-featured M:N threading libraries are difficult to implement and remaining as active research topics. The M:N threading library that we&amp;rsquo;re talking about is specialized for building online services, in which case, some of the requirements can be simplified, namely no (complete) preemptions and priorities. M:N threading libraries can be implemented either in userland or OS kernel. New programming languages prefer implementations in userland, such as GHC thread and goroutine, which is capable of adding brand-new keywords and intercepting related APIs on threading. Implementation in existing languages often have to modify the OS kernel, such as &lt;a href=&#34;https://msdn.microsoft.com/en-us/library/windows/desktop/dd627187(v=vs.85).aspx&#34;&gt;Windows UMS&lt;/a&gt; and google SwicthTo(which is 1:1, however M:N effects can be achieved based on it). Compared to N:1 threading libraries, usages of M:N threading libraries are more similar to system threads, which need locks or message passings to ensure thread safety.&lt;/p&gt;
&lt;h1 id=&#34;issues&#34;&gt;Issues&lt;/h1&gt;
&lt;h2 id=&#34;multi-core-scalability&#34;&gt;Multi-core scalability&lt;/h2&gt;
&lt;p&gt;Ideally capabilities of the reactor model are maximized when all source code is programmed in event-driven manner, but in reality because of the difficulties and maintainability, users are likely to mix usages: synchronous IO is often issued in callbacks, blocking worker threads from processing other requests. A request often goes through dozens of services, making worker threads spend a lot of time waiting for responses from downstream servers. Users have to launch hundreds of threads to maintain enough throughput, which imposes intensive pressure on scheduling and lowers efficiencies of TLS related code. Tasks are often pushed into a queue protected with a global mutex and condition, which performs poorly when many threads are contending for it. A better approach is to deploy more task queues and adjust the scheduling algorithm to reduce global contentions. Namely each system thread has its own runqueue, and one or more schedulers dispatch user threads to different runqueues. Each system thread runs user threads from its own runqueue before considering other runqueues, which is more complicated but more scalable than the global mutex+condition solution. This model is also easier to support NUMA.&lt;/p&gt;
&lt;p&gt;When an event dispatcher passes a task to a worker thread, the user code probably jumps from one CPU core to another, which may need to wait for synchronizations of relevant cachelines, which is not very fast. It would be better that the worker is able to run directly on the CPU core where the event dispatcher runs, since at most of the time, priority of running the worker ASAP is higher than getting new events from the dispatcher. Similarly, it&amp;rsquo;s better to wake up the user thread blocking on RPC on the same CPU core where the response is received.&lt;/p&gt;
&lt;h2 id=&#34;asynchronous-programming&#34;&gt;Asynchronous programming&lt;/h2&gt;
&lt;p&gt;Flow controls in asynchronous programming are even difficult for experts. Any suspending operation such as sleeping for a while or waiting for something to finish, implies that users have to save states explicitly and restore states in callbacks. Asynchronous code is often written as state machines. A few suspensions are troublesome, but still handleable. The problem is that once the suspension occurs inside a condition, loop or sub-function, it&amp;rsquo;s almost impossible to write such a state machine being understood and maintained by many people, although the scenario is quite common in distributed systems where a node often needs to interact with multiple nodes simultaneously. In addition, if the wakeup can be triggered by more than one events (such as either fd has data or timeout is reached), the suspension and resuming are prone to race conditions, which require good multi-threaded programming skills to solve. Syntactic sugars(such as lambda) just make coding less troublesome rather than reducing difficulty.&lt;/p&gt;
&lt;p&gt;Shared pointers are common in asynchronous programming, which seems convenient, but also makes ownerships of memory elusive. If the memory is leaked, it&amp;rsquo;s difficult to locate the code that forgot to release; if segment fault happens, where the double-free occurs is also unknown. Code with a lot of referential countings is hard to remain good-quality and may waste a lot of time on debugging memory related issues. If references are even counted manually, keeping quality of the code is harder and the maintainers are less willing to modify the code. &lt;a href=&#34;http://en.wikipedia.org/wiki/Resource_Acquisition_Is_Initialization&#34;&gt;RAII&lt;/a&gt; cannot be used in many scenarios in asynchronous programming, sometimes resources need to be locked before a callback and unlocked inside the callback, which is very error-prone in practice.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: Load Balancing</title>
      <link>https://brpc.incubator.apache.org/docs/rpc-in-depth/load-balancing/</link>
      <pubDate>Thu, 12 Aug 2021 00:00:00 +0000</pubDate>
      
      <guid>https://brpc.incubator.apache.org/docs/rpc-in-depth/load-balancing/</guid>
      <description>
        
        
        &lt;p&gt;上游一般通过命名服务发现所有的下游节点，并通过多种负载均衡方法把流量分配给下游节点。当下游节点出现问题时，它可能会被隔离以提高负载均衡的效率。被隔离的节点定期被健康检查，成功后重新加入正常节点。&lt;/p&gt;
&lt;h1 id=&#34;命名服务&#34;&gt;命名服务&lt;/h1&gt;
&lt;p&gt;在brpc中，&lt;a href=&#34;https://github.com/brpc/brpc/blob/master/src/brpc/naming_service.h&#34;&gt;NamingService&lt;/a&gt;用于获得服务名对应的所有节点。一个直观的做法是定期调用一个函数以获取最新的节点列表。但这会带来一定的延时（定期调用的周期一般在若干秒左右），作为通用接口不太合适。特别当命名服务提供事件通知时(比如zk)，这个特性没有被利用。所以我们反转了控制权：不是我们调用用户函数，而是用户在获得列表后调用我们的接口，对应&lt;a href=&#34;https://github.com/brpc/brpc/blob/master/src/brpc/naming_service.h&#34;&gt;NamingServiceActions&lt;/a&gt;。当然我们还是得启动进行这一过程的函数，对应NamingService::RunNamingService。下面以三个实现解释这套方式：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;bns：没有事件通知，所以我们只能定期去获得最新列表，默认间隔是&lt;a href=&#34;http://brpc.baidu.com:8765/flags/ns_access_interval&#34;&gt;5秒&lt;/a&gt;。为了简化这类定期获取的逻辑，brpc提供了&lt;a href=&#34;https://github.com/brpc/brpc/blob/master/src/brpc/periodic_naming_service.h&#34;&gt;PeriodicNamingService&lt;/a&gt; 供用户继承，用户只需要实现单次如何获取（GetServers）。获取后调用NamingServiceActions::ResetServers告诉框架。框架会对列表去重，和之前的列表比较，通知对列表有兴趣的观察者(NamingServiceWatcher)。这套逻辑会运行在独立的bthread中，即NamingServiceThread。一个NamingServiceThread可能被多个Channel共享，通过intrusive_ptr管理ownership。&lt;/li&gt;
&lt;li&gt;file：列表即文件。合理的方式是在文件更新后重新读取。&lt;a href=&#34;https://github.com/brpc/brpc/blob/master/src/brpc/policy/file_naming_service.cpp&#34;&gt;该实现&lt;/a&gt;使用&lt;a href=&#34;https://github.com/brpc/brpc/blob/master/src/butil/files/file_watcher.h&#34;&gt;FileWatcher&lt;/a&gt;关注文件的修改时间，当文件修改后，读取并调用NamingServiceActions::ResetServers告诉框架。&lt;/li&gt;
&lt;li&gt;list：列表就在服务名里（逗号分隔）。在读取完一次并调用NamingServiceActions::ResetServers后就退出了，因为列表再不会改变了。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;如果用户需要建立这些对象仍然是不够方便的，因为总是需要一些工厂代码根据配置项建立不同的对象，鉴于此，我们把工厂类做进了框架，并且是非常方便的形式：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;&amp;quot;protocol://service-name&amp;quot;
 
e.g.
bns://&amp;lt;node-name&amp;gt;            # baidu naming service
file://&amp;lt;file-path&amp;gt;           # load addresses from the file
list://addr1,addr2,...       # use the addresses separated by comma
http://&amp;lt;url&amp;gt;                 # Domain Naming Service, aka DNS.
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;这套方式是可扩展的，实现了新的NamingService后在&lt;a href=&#34;https://github.com/brpc/brpc/blob/master/src/brpc/global.cpp&#34;&gt;global.cpp&lt;/a&gt;中依葫芦画瓢注册下就行了，如下图所示：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://brpc.incubator.apache.org/images/docs/register_ns.png&#34; alt=&#34;img&#34;&gt;&lt;/p&gt;
&lt;p&gt;看到这些熟悉的字符串格式，容易联想到ftp:// zk:// galileo://等等都是可以支持的。用户在新建Channel时传入这类NamingService描述，并能把这些描述写在各类配置文件中。&lt;/p&gt;
&lt;h1 id=&#34;负载均衡&#34;&gt;负载均衡&lt;/h1&gt;
&lt;p&gt;brpc中&lt;a href=&#34;https://github.com/brpc/brpc/blob/master/src/brpc/load_balancer.h&#34;&gt;LoadBalancer&lt;/a&gt;从多个服务节点中选择一个节点，目前的实现见&lt;a href=&#34;../../client/basics/#load-balancer&#34;&gt;负载均衡&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;Load balancer最重要的是如何让不同线程中的负载均衡不互斥，解决这个问题的技术是&lt;a href=&#34;../locality-aware/#doublybuffereddata&#34;&gt;DoublyBufferedData&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;和NamingService类似，我们使用字符串来指代一个load balancer，在global.cpp中注册：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://brpc.incubator.apache.org/images/docs/register_lb.png&#34; alt=&#34;img&#34;&gt;&lt;/p&gt;
&lt;h1 id=&#34;健康检查&#34;&gt;健康检查&lt;/h1&gt;
&lt;p&gt;对于那些无法连接却仍在NamingService的节点，brpc会定期连接它们，成功后对应的Socket将被”复活“，并可能被LoadBalancer选择上，这个过程就是健康检查。注意：被健康检查或在LoadBalancer中的节点一定在NamingService中。换句话说，只要一个节点不从NamingService删除，它要么是正常的（会被LoadBalancer选上），要么在做健康检查。&lt;/p&gt;
&lt;p&gt;传统的做法是使用一个线程做所有连接的健康检查，brpc简化了这个过程：为需要的连接动态创建一个bthread专门做健康检查（Socket::HealthCheckThread）。这个线程的生命周期被对应连接管理。具体来说，当Socket被SetFailed后，健康检查线程就可能启动（如果SocketOptions.health_check_interval为正数的话）：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;健康检查线程先在确保没有其他人在使用Socket了后关闭连接。目前是通过对Socket的引用计数判断的。这个方法之所以有效在于Socket被SetFailed后就不能被Address了，所以引用计数只减不增。&lt;/li&gt;
&lt;li&gt;定期连接直到远端机器被连接上，在这个过程中，如果Socket析构了，那么该线程也就随之退出了。&lt;/li&gt;
&lt;li&gt;连上后复活Socket(Socket::Revive)，这样Socket就又能被其他地方，包括LoadBalancer访问到了（通过Socket::Address）。&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: Locality-aware</title>
      <link>https://brpc.incubator.apache.org/docs/rpc-in-depth/locality-aware/</link>
      <pubDate>Thu, 12 Aug 2021 00:00:00 +0000</pubDate>
      
      <guid>https://brpc.incubator.apache.org/docs/rpc-in-depth/locality-aware/</guid>
      <description>
        
        
        &lt;h1 id=&#34;概述&#34;&gt;概述&lt;/h1&gt;
&lt;p&gt;LALB全称Locality-aware load balancing，是一个能把请求及时、自动地送到延时最低的下游的负载均衡算法，特别适合混合部署环境。该算法产生自DP系统，现已加入brpc！&lt;/p&gt;
&lt;p&gt;LALB可以解决的问题：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;下游的机器配置不同，访问延时不同，round-robin和随机分流效果不佳。&lt;/li&gt;
&lt;li&gt;下游服务和离线服务或其他服务混部，性能难以预测。&lt;/li&gt;
&lt;li&gt;自动地把大部分流量送给同机部署的模块，当同机模块出问题时，再跨机器。&lt;/li&gt;
&lt;li&gt;优先访问本机房服务，出问题时再跨机房。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;…&lt;/strong&gt;&lt;/p&gt;
&lt;h1 id=&#34;背景&#34;&gt;背景&lt;/h1&gt;
&lt;p&gt;最常见的分流算法是round robin和随机。这两个方法的前提是下游的机器和网络都是类似的，但在目前的线上环境下，特别是混部的产品线中，已经很难成立，因为：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;每台机器运行着不同的程序组合，并伴随着一些离线任务，机器的可用资源在持续动态地变化着。&lt;/li&gt;
&lt;li&gt;机器配置不同。&lt;/li&gt;
&lt;li&gt;网络延时不同。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;这些问题其实一直有，但往往被OP辛勤的机器监控和替换给隐藏了。框架层面也有过一些努力，比如UB中的&lt;a href=&#34;https://svn.baidu.com/public/trunk/ub/ub_client/ubclient_weightstrategy.h&#34;&gt;WeightedStrategy&lt;/a&gt;是根据下游的cpu占用率来进行分流，但明显地它解决不了延时相关的问题，甚至cpu的问题也解决不了：因为它被实现为定期reload一个权值列表，可想而知更新频率高不了，等到负载均衡反应过来，一大堆请求可能都超时了。并且这儿有个数学问题：怎么把cpu占用率转为权值。假设下游差异仅仅由同机运行的其他程序导致，机器配置和网络完全相同，两台机器权值之比是cpu idle之比吗？假如是的，当我们以这个比例给两台机器分流之后，它们的cpu idle应该会更接近对吧？而这会导致我们的分流比例也变得接近，从而使两台机器的cpu idle又出现差距。你注意到这个悖论了吗？这些因素使得这类算法的实际效果和那两个基本算法没什么差距，甚至更差，用者甚少。&lt;/p&gt;
&lt;p&gt;我们需要一个能自适应下游负载、规避慢节点的通用分流算法。&lt;/p&gt;
&lt;h1 id=&#34;locality-aware&#34;&gt;Locality-aware&lt;/h1&gt;
&lt;p&gt;在DP 2.0中我们使用了一种新的算法: Locality-aware load balancing，能根据下游节点的负载分配流量，还能快速规避失效的节点，在很大程度上，这种算法的延时也是全局最优的。基本原理非常简单：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;以下游节点的吞吐除以延时作为分流权值。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;比如只有两台下游节点，W代表权值，QPS代表吞吐，L代表延时，那么W1 = QPS1 / L1和W2 = QPS2 / L2分别是这两个节点的分流权值，分流时随机数落入的权值区间就是流量的目的地了。&lt;/p&gt;
&lt;p&gt;一种分析方法如下：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;稳定状态时的QPS显然和其分流权值W成正比，即W1 / W2 ≈ QPS1 / QPS2。&lt;/li&gt;
&lt;li&gt;根据分流公式又有：W1 / W2 = QPS1 / QPS2 * (L2 / L1)。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;故稳定状态时L1和L2应当是趋同的。当L1小于L2时，节点1会更获得相比其QPS1更大的W1，从而在未来获得更多的流量，直到&lt;strong&gt;其延时高于平均值或没有更多的流量。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;注意这个算法并不是按照延时的比例来分流，不是说一个下游30ms，另一个60ms，它们的流量比例就是60 / 30。而是30ms的节点会一直获得流量直到它的延时高于60ms，或者没有更多流量了。以下图为例，曲线1和曲线2分别是节点1和节点2的延时与吞吐关系图，随着吞吐增大延时会逐渐升高，接近极限吞吐时，延时会飙升。左下的虚线标记了QPS=400时的延时，此时虽然节点1的延时有所上升，但还未高于节点2的基本延时（QPS=0时的延时），所以所有流量都会分给节点1，而不是按它们基本延时的比例（图中大约2:1）。当QPS继续上升达到1600时，分流比例会在两个节点延时相等时平衡，图中为9 : 7。很明显这个比例是高度非线性的，取决于不同曲线的组合，和单一指标的比例关系没有直接关联。在真实系统中，延时和吞吐的曲线也在动态变化着，分流比例更加动态。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://brpc.incubator.apache.org/images/docs/lalb_1.png&#34; alt=&#34;img&#34;&gt;&lt;/p&gt;
&lt;p&gt;我们用一个例子来看一下具体的分流过程。启动3台server，逻辑分别是sleep 1ms，2ms，3ms，对于client来说这些值就是延时。启动client（50个同步访问线程）后每秒打印的分流结果如下：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://brpc.incubator.apache.org/images/docs/lalb_2.png&#34; alt=&#34;img&#34;&gt;&lt;/p&gt;
&lt;p&gt;S[n]代表第n台server。由于S[1]和S[2]的平均延时大于1ms，LALB会发现这点并降低它们的权值。它们的权值会继续下降，直到被算法设定的最低值拦住。这时停掉server，反转延时并重新启动，即逻辑分别为sleep 3ms，2ms，1ms，运行一段时候后分流效果如下：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://brpc.incubator.apache.org/images/docs/lalb_3.png&#34; alt=&#34;img&#34;&gt;&lt;/p&gt;
&lt;p&gt;刚重连上server时，client还是按之前的权值把大部分流量都分给了S[0]，但由于S[0]的延时从1ms上升到了3ms，client的qps也降到了原来的1/3。随着数据积累，LALB逐渐发现S[2]才是最快的，而把大部分流量切换了过去。同样的服务如果用rr或random访问，则qps会显著下降：&lt;/p&gt;
&lt;p&gt;&amp;ldquo;rr&amp;rdquo; or &amp;ldquo;random&amp;rdquo;: &lt;img src=&#34;https://brpc.incubator.apache.org/images/docs/lalb_4.png&#34; alt=&#34;img&#34;&gt;&lt;/p&gt;
&lt;p&gt;&amp;ldquo;la&amp;rdquo; :                       &lt;img src=&#34;https://brpc.incubator.apache.org/images/docs/lalb_5.png&#34; alt=&#34;img&#34;&gt;&lt;/p&gt;
&lt;p&gt;真实的场景中不会有这么显著的差异，但你应该能看到差别了。&lt;/p&gt;
&lt;p&gt;这有很多应用场景：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;如果本机有下游服务，LALB会优先访问这些最近的节点。比如CTR应用中有一个计算在1ms左右的receiver模块，被model模块访问，很多model和receiver是同机部署的，以前的分流算法必须走网络，使得receiver的延时开销较大(3-5ms)，特别是在晚上由于离线任务起来，很不稳定，失败率偏高，而LALB会优先访问本机或最近的receiver模块，很多流量都不走网络了，成功率一下子提升了很多。&lt;/li&gt;
&lt;li&gt;如果同rack有下游服务，LALB也会优先访问，减少机房核心路由器的压力。甚至不同机房的服务可能不再需要隔离，LALB会优先走本机房的下游，当本机房下游出问题时再自动访问另一些机房。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;但我们也不能仅看到“基本原理”，这个算法有它复杂的一面：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;传统的经验告诉我们，不能把所有鸡蛋放一个篮子里，而按延时优化不可避免地会把很多流量送到同一个节点，如果这个节点出问题了，我们如何尽快知道并绕开它。&lt;/li&gt;
&lt;li&gt;对吞吐和延时的统计都需要统计窗口，窗口越大数据越可信，噪声越少，但反应也慢了，一个异常的请求可能对统计值造不成什么影响，等我们看到统计值有显著变化时可能已经太晚了。&lt;/li&gt;
&lt;li&gt;我们也不能只统计已经回来的，还得盯着路上的请求，否则我们可能会向一个已经出问题（总是不回）的节点傻傻地浪费请求。&lt;/li&gt;
&lt;li&gt;”按权值分流”听上去好简单，但你能写出多线程和可能修改节点的前提下，在O(logN)时间内尽量不互斥的查找算法吗？&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;这些问题可以归纳为以下几个方面。&lt;/p&gt;
&lt;h2 id=&#34;doublybuffereddata&#34;&gt;DoublyBufferedData&lt;/h2&gt;
&lt;p&gt;LoadBalancer是一个读远多于写的数据结构：大部分时候，所有线程从一个不变的server列表中选取一台server。如果server列表真是“不变的”，那么选取server的过程就不用加锁，我们可以写更复杂的分流算法。一个方法是用读写锁，但当读临界区不是特别大时（毫秒级），读写锁并不比mutex快，而实用的分流算法不可能到毫秒级，否则开销也太大了。另一个方法是双缓冲，很多检索端用类似的方法实现无锁的查找过程，它大概这么工作：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;数据分前台和后台。&lt;/li&gt;
&lt;li&gt;检索线程只读前台，不用加锁。&lt;/li&gt;
&lt;li&gt;只有一个写线程：修改后台数据，切换前后台，睡眠一段时间，以确保老前台（新后台）不再被检索线程访问。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;这个方法的问题在于它假定睡眠一段时间后就能避免和前台读线程发生竞争，这个时间一般是若干秒。由于多次写之间有间隔，这儿的写往往是批量写入，睡眠时正好用于积累数据增量。&lt;/p&gt;
&lt;p&gt;但这套机制对“server列表”不太好用：总不能插入一个server就得等几秒钟才能插入下一个吧，即使我们用批量插入，这个&amp;quot;冷却&amp;quot;间隔多少会让用户觉得疑惑：短了担心安全性，长了觉得没有必要。我们能尽量降低这个时间并使其安全么？&lt;/p&gt;
&lt;p&gt;我们需要&lt;strong&gt;写以某种形式和读同步，但读之间相互没竞争&lt;/strong&gt;。一种解法是，读拿一把thread-local锁，写需要拿到所有的thread-local锁。具体过程如下：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;数据分前台和后台。&lt;/li&gt;
&lt;li&gt;读拿到自己所在线程的thread-local锁，执行查询逻辑后释放锁。&lt;/li&gt;
&lt;li&gt;同时只有一个写：修改后台数据，切换前后台，&lt;strong&gt;挨个&lt;/strong&gt;获得所有thread-local锁并立刻释放，结束后再改一遍新后台（老前台）。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;我们来分析下这个方法的基本原理：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;当一个读正在发生时，它会拿着所在线程的thread-local锁，这把锁会挡住同时进行的写，从而保证前台数据不会被修改。&lt;/li&gt;
&lt;li&gt;在大部分时候thread-local锁都没有竞争，对性能影响很小。&lt;/li&gt;
&lt;li&gt;逐个获取thread-local锁并立刻释放是为了&lt;strong&gt;确保对应的读线程看到了切换后的新前台&lt;/strong&gt;。如果所有的读线程都看到了新前台，写线程便可以安全地修改老前台（新后台）了。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;其他特点：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;不同的读之间没有竞争，高度并发。&lt;/li&gt;
&lt;li&gt;如果没有写，读总是能无竞争地获取和释放thread-local锁，一般小于25ns，对延时基本无影响。如果有写，由于其临界区极小（拿到立刻释放），读在大部分时候仍能快速地获得锁，少数时候释放锁时可能有唤醒写线程的代价。由于写本身就是少数情况，读整体上几乎不会碰到竞争锁。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;完成这些功能的数据结构是&lt;a href=&#34;https://github.com/brpc/brpc/blob/master/src/butil/containers/doubly_buffered_data.h&#34;&gt;DoublyBufferedData&amp;lt;&amp;gt;&lt;/a&gt;，我们常简称为DBD。brpc中的所有load balancer都使用了这个数据结构，使不同线程在分流时几乎不会互斥。而其他rpc实现往往使用了全局锁，这使得它们无法写出复杂的分流算法：否则分流代码将会成为竞争热点。&lt;/p&gt;
&lt;p&gt;这个结构有广泛的应用场景：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;reload词典。大部分时候词典都是只读的，不同线程同时查询时不应互斥。&lt;/li&gt;
&lt;li&gt;可替换的全局callback。像butil/logging.cpp支持配置全局LogSink以重定向日志，这个LogSink就是一个带状态的callback。如果只是简单的全局变量，在替换后我们无法直接删除LogSink，因为可能还有都写线程在用。用DBD可以解决这个问题。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;weight-tree&#34;&gt;weight tree&lt;/h2&gt;
&lt;p&gt;LALB的查找过程是按权值分流，O(N)方法如下：获得所有权值的和total，产生一个间于[0, total-1]的随机数R，逐个遍历权值，直到当前权值之和不大于R，而下一个权值之和大于R。&lt;/p&gt;
&lt;p&gt;这个方法可以工作，也好理解，但当N达到几百时性能已经很差，这儿的主要因素是cache一致性：LALB是一个基于反馈的算法，RPC结束时信息会被反馈入LALB，被遍历的数据结构也一直在被修改。这意味着前台的O(N)读必须刷新每一行cacheline。当N达到数百时，一次查找过程可能会耗时百微秒，更别提更大的N了，LALB（将）作为brpc的默认分流算法，这个性能开销是无法接受的。&lt;/p&gt;
&lt;p&gt;另一个办法是用完全二叉树。每个节点记录了左子树的权值之和，这样我们就能在O(logN)时间内完成查找。当N为1024时，我们最多跳转10次内存，总耗时可控制在1微秒内，这个性能是可接受的。这个方法的难点是如何和DoublyBufferedData结合。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;我们不考虑不使用DoublyBufferedData，那样要么绕不开锁，要么写不出正确的算法。&lt;/li&gt;
&lt;li&gt;前台后必须共享权值数据，否则切换前后台时，前台积累的权值数据没法同步到后台。&lt;/li&gt;
&lt;li&gt;“左子树权值之和”也被前后台共享，但和权值数据不同，它和位置绑定。比如权值结构的指针可能从位置10移动到位置5，但“左子树权值之和”的指针不会移动，算法需要从原位置减掉差值，而向新位置加上差值。&lt;/li&gt;
&lt;li&gt;我们不追求一致性，只要最终一致即可，这能让我们少加锁。这也意味着“权值之和”，“左子树权值之和”，“节点权值”未必能精确吻合，查找算法要能适应这一点。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;最困难的部分是增加和删除节点，它们需要在整体上对前台查找不造成什么影响，详细过程请参考代码。&lt;/p&gt;
&lt;h2 id=&#34;base_weight&#34;&gt;base_weight&lt;/h2&gt;
&lt;p&gt;QPS和latency使用一个循环队列统计，默认容量128。我们可以使用这么小的统计窗口，是因为inflight delay能及时纠正过度反应，而128也具备了一定的统计可信度。不过，这么计算latency的缺点是：如果server的性能出现很大的变化，那么我们需要积累一段时间才能看到平均延时的变化。就像上节例子中那样，server反转延时后client需要积累很多秒的数据才能看到的平均延时的变化。目前我们并么有处理这个问题，因为真实生产环境中的server不太会像例子中那样跳变延时，大都是缓缓变慢。当集群有几百台机器时，即使我们反应慢点给个别机器少分流点也不会导致什么问题。如果在产品线中确实出现了性能跳变，并且集群规模不大，我们再处理这个问题。&lt;/p&gt;
&lt;p&gt;权值的计算方法是base_weight = QPS * WEIGHT_SCALE / latency ^ p。其中WEIGHT_SCALE是一个放大系数，为了能用整数存储权值，又能让权值有足够的精度，类似定点数。p默认为2，延时的收敛速度大约为p=1时的p倍，选项quadratic_latency=false可使p=1。&lt;/p&gt;
&lt;p&gt;权值计算在各个环节都有最小值限制，为了防止某个节点的权值过低而使其完全没有访问机会。即使一些延时远大于平均延时的节点，也应该有足够的权值，以确保它们可以被定期访问，否则即使它们变快了，我们也不会知道。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;除了待删除节点，所有节点的权值绝对不会为0。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;这也制造了一个问题：即使一个server非常缓慢（但没有断开连接），它的权值也不会为0，所以总会有一些请求被定期送过去而铁定超时。当qps不高时，为了降低影响面，探测间隔必须拉长。比如为了把对qps=1000的影响控制在1%%内，故障server的权值必须低至使其探测间隔为10秒以上，这降低了我们发现server变快的速度。这个问题的解决方法有：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;什么都不干。这个问题也许没有想象中那么严重，由于持续的资源监控，线上服务很少出现“非常缓慢”的情况，一般性的变慢并不会导致请求超时。&lt;/li&gt;
&lt;li&gt;保存一些曾经发向缓慢server的请求，用这些请求探测。这个办法的好处是不浪费请求。但实现起来耦合很多，比较麻烦。&lt;/li&gt;
&lt;li&gt;强制backup request。&lt;/li&gt;
&lt;li&gt;再选一次。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;inflight-delay&#34;&gt;inflight delay&lt;/h2&gt;
&lt;p&gt;我们必须追踪还未结束的RPC，否则我们就必须等待到超时或其他错误发生，而这可能会很慢（超时一般会是正常延时的若干倍），在这段时间内我们可能做出了很多错误的分流。最简单的方法是统计未结束RPC的耗时：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;选择server时累加发出时间和未结束次数。&lt;/li&gt;
&lt;li&gt;反馈时扣除发出时间和未结束次数。&lt;/li&gt;
&lt;li&gt;框架保证每个选择总对应一次反馈。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;这样“当前时间 - 发出时间之和 / 未结束次数”便是未结束RPC的平均耗时，我们称之为inflight delay。当inflight delay大于平均延时时，我们就线性地惩罚节点权值，即weight = base_weight * avg_latency / inflight_delay。当发向一个节点的请求没有在平均延时内回来时，它的权值就会很快下降，从而纠正我们的行为，这比等待超时快多了。不过这没有考虑延时的正常抖动，我们还得有方差，方差可以来自统计，也可简单线性于平均延时。不管怎样，有了方差bound后，当inflight delay &amp;gt; avg_latency + max(bound * 3, MIN_BOUND)时才会惩罚权值。3是正态分布中的经验数值。&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: Consistent Hashing</title>
      <link>https://brpc.incubator.apache.org/docs/rpc-in-depth/consistent-hashing/</link>
      <pubDate>Thu, 12 Aug 2021 00:00:00 +0000</pubDate>
      
      <guid>https://brpc.incubator.apache.org/docs/rpc-in-depth/consistent-hashing/</guid>
      <description>
        
        
        &lt;h1 id=&#34;概述&#34;&gt;概述&lt;/h1&gt;
&lt;p&gt;一些场景希望同样的请求尽量落到一台机器上，比如访问缓存集群时，我们往往希望同一种请求能落到同一个后端上，以充分利用其上已有的缓存，不同的机器承载不同的稳定working set。而不是随机地散落到所有机器上，那样的话会迫使所有机器缓存所有的内容，最终由于存不下形成颠簸而表现糟糕。 我们都知道hash能满足这个要求，比如当有n台服务器时，输入x总是会发送到第hash(x) % n台服务器上。但当服务器变为m台时，hash(x) % n和hash(x) % m很可能都不相等，这会使得几乎所有请求的发送目的地都发生变化，如果目的地是缓存服务，所有缓存将失效，继而对原本被缓存遮挡的数据库或计算服务造成请求风暴，触发雪崩。一致性哈希是一种特殊的哈希算法，在增加服务器时，发向每个老节点的请求中只会有一部分转向新节点，从而实现平滑的迁移。&lt;a href=&#34;http://blog.phpdr.net/wp-content/uploads/2012/08/Consistent-Hashing-and-Random-Trees.pdf&#34;&gt;这篇论文&lt;/a&gt;中提出了一致性hash的概念。&lt;/p&gt;
&lt;p&gt;一致性hash满足以下四个性质：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;平衡性 (Balance) : 每个节点被选到的概率是O(1/n)。&lt;/li&gt;
&lt;li&gt;单调性 (Monotonicity) : 当新节点加入时， 不会有请求在老节点间移动， 只会从老节点移动到新节点。当有节点被删除时，也不会影响落在别的节点上的请求。&lt;/li&gt;
&lt;li&gt;分散性 (Spread) : 当上游的机器看到不同的下游列表时(在上线时及不稳定的网络中比较常见),  同一个请求尽量映射到少量的节点中。&lt;/li&gt;
&lt;li&gt;负载 (Load) : 当上游的机器看到不同的下游列表的时候， 保证每台下游分到的请求数量尽量一致。&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;实现方式&#34;&gt;实现方式&lt;/h1&gt;
&lt;p&gt;所有server的32位hash值在32位整数值域上构成一个环(Hash Ring)，环上的每个区间和一个server唯一对应，如果一个key落在某个区间内， 它就被分流到对应的server上。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://brpc.incubator.apache.org/images/docs/chash.png&#34; alt=&#34;img&#34;&gt;&lt;/p&gt;
&lt;p&gt;当删除一个server的，它对应的区间会归属于相邻的server，所有的请求都会跑过去。当增加一个server时，它会分割某个server的区间并承载落在这个区间上的所有请求。单纯使用Hash Ring很难满足我们上节提到的属性，主要两个问题：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;在机器数量较少的时候， 区间大小会不平衡。&lt;/li&gt;
&lt;li&gt;当一台机器故障的时候， 它的压力会完全转移到另外一台机器， 可能无法承载。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;为了解决这个问题，我们为每个server计算m个hash值，从而把32位整数值域划分为n*m个区间，当key落到某个区间时，分流到对应的server上。那些额外的hash值使得区间划分更加均匀，被称为虚拟节点（Virtual Node）。当删除一个server时，它对应的m个区间会分别合入相邻的区间中，那个server上的请求会较为平均地转移到其他server上。当增加server时，它会分割m个现有区间，从对应server上分别转移一些请求过来。&lt;/p&gt;
&lt;p&gt;由于节点故障和变化不常发生，我们选择了修改复杂度为O(n)的有序数组来存储hash ring，每次分流使用二分查找来选择对应的机器，由于存储是连续的，查找效率比基于平衡二叉树的实现高。线程安全性请参照&lt;a href=&#34;../locality-aware/#doublybuffereddata&#34;&gt;Double Buffered Data&lt;/a&gt;章节.&lt;/p&gt;
&lt;h1 id=&#34;使用方式&#34;&gt;使用方式&lt;/h1&gt;
&lt;p&gt;我们内置了分别基于murmurhash3和md5两种hash算法的实现，使用要做两件事：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;在Channel.Init 时指定&lt;em&gt;load_balancer_name&lt;/em&gt;为 &amp;ldquo;c_murmurhash&amp;rdquo; 或 &amp;ldquo;c_md5&amp;rdquo;。&lt;/li&gt;
&lt;li&gt;发起rpc时通过Controller::set_request_code(uint64_t)填入请求的hash code。&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;request的hash算法并不需要和lb的hash算法保持一致，只需要hash的值域是32位无符号整数。由于memcache默认使用md5，访问memcached集群时请选择c_md5保证兼容性，其他场景可以选择c_murmurhash以获得更高的性能和更均匀的分布。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h1 id=&#34;虚拟节点个数&#34;&gt;虚拟节点个数&lt;/h1&gt;
&lt;p&gt;通过-chash_num_replicas可设置默认的虚拟节点个数，默认值为100。对于某些特殊场合，对虚拟节点个数有自定义的需求，可以通过将&lt;em&gt;load_balancer_name&lt;/em&gt;加上参数replicas=&lt;num&gt;配置，如：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-c++&#34; data-lang=&#34;c++&#34;&gt;&lt;span style=&#34;color:#000&#34;&gt;channel&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;.&lt;/span&gt;&lt;span style=&#34;color:#000&#34;&gt;Init&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;(&lt;/span&gt;&lt;span style=&#34;color:#4e9a06&#34;&gt;&amp;#34;http://...&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;,&lt;/span&gt; &lt;span style=&#34;color:#4e9a06&#34;&gt;&amp;#34;c_murmurhash:replicas=150&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;,&lt;/span&gt; &lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;&amp;amp;&lt;/span&gt;&lt;span style=&#34;color:#000&#34;&gt;options&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;);&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
      </description>
    </item>
    
    <item>
      <title>Docs: Memory Management</title>
      <link>https://brpc.incubator.apache.org/docs/rpc-in-depth/memory-management/</link>
      <pubDate>Thu, 12 Aug 2021 00:00:00 +0000</pubDate>
      
      <guid>https://brpc.incubator.apache.org/docs/rpc-in-depth/memory-management/</guid>
      <description>
        
        
        &lt;p&gt;内存管理总是程序中的重要一环，在多线程时代，一个好的内存分配大都在如下两点间权衡：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;线程间竞争少。内存分配的粒度大都比较小，对性能敏感，如果不同的线程在大多数分配时会竞争同一份资源或同一把锁，性能将会非常糟糕，原因无外乎和cache一致性有关，已被大量的malloc方案证明。&lt;/li&gt;
&lt;li&gt;浪费的空间少。如果每个线程各申请各的，速度也许不错，但万一一个线程总是申请，另一个线程总是释放，内存就爆炸了。线程之间总是要共享内存的，如何共享就是方案的关键了。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;一般的应用可以使用&lt;a href=&#34;http://goog-perftools.sourceforge.net/doc/tcmalloc.html&#34;&gt;tcmalloc&lt;/a&gt;、&lt;a href=&#34;https://github.com/jemalloc/jemalloc&#34;&gt;jemalloc&lt;/a&gt;等成熟的内存分配方案，但这对于较为底层，关注性能长尾的应用是不够的。多线程框架广泛地通过传递对象的ownership来让问题异步化，如何让分配这些小对象的开销变的更小是值得研究的问题。其中的一个特点较为显著：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;大多数结构是等长的。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;这个属性可以大幅简化内存分配的过程，获得比通用malloc更稳定、快速的性能。brpc中的ResourcePool&lt;T&gt;和ObjectPool&lt;T&gt;即提供这类分配。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;这篇文章不鼓励用户使用ResourcePool&lt;T&gt;或ObjectPool&lt;T&gt;，事实上我们反对用户在程序中使用这两个类。因为”等长“的副作用是某个类型独占了一部分内存，这些内存无法再被其他类型使用，如果不加控制的滥用，反而会在程序中产生大量彼此隔离的内存分配体系，既浪费内存也不见得会有更好的性能。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h1 id=&#34;resourcepoolt&#34;&gt;ResourcePool&lt;T&gt;&lt;/h1&gt;
&lt;p&gt;创建一个类型为T的对象并返回一个偏移量，这个偏移量可以在O(1)时间内转换为对象指针。这个偏移量相当于指针，但它的值在一般情况下小于2^32，所以我们可以把它作为64位id的一部分。对象可以被归还，但归还后对象并没有删除，也没有被析构，而是仅仅进入freelist。下次申请时可能会取到这种使用过的对象，需要重置后才能使用。当对象被归还后，通过对应的偏移量仍可以访问到对象，即ResourcePool只负责内存分配，并不解决ABA问题。但对于越界的偏移量，ResourcePool会返回空。&lt;/p&gt;
&lt;p&gt;由于对象等长，ResourcePool通过批量分配和归还内存以避免全局竞争，并降低单次的开销。每个线程的分配流程如下：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;查看thread-local free block。如果还有free的对象，返回。没有的话步骤2。&lt;/li&gt;
&lt;li&gt;尝试从全局取一个free block，若取到的话回到步骤1，否则步骤3。&lt;/li&gt;
&lt;li&gt;从全局取一个block，返回其中第一个对象。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;原理是比较简单的。工程实现上数据结构、原子变量、memory fence等问题会复杂一些。下面以bthread_t的生成过程说明ResourcePool是如何被应用的。&lt;/p&gt;
&lt;h1 id=&#34;objectpoolt&#34;&gt;ObjectPool&lt;T&gt;&lt;/h1&gt;
&lt;p&gt;这是ResourcePool&lt;T&gt;的变种，不返回偏移量，而直接返回对象指针。内部结构和ResourcePool类似，一些代码更加简单。对于用户来说，这就是一个多线程下的对象池，brpc里也是这么用的。比如Socket::Write中把每个待写出的请求包装为WriteRequest，这个对象就是用ObjectPool&lt;WriteRequest&gt;分配的。&lt;/p&gt;
&lt;h1 id=&#34;生成bthread_t&#34;&gt;生成bthread_t&lt;/h1&gt;
&lt;p&gt;用户期望通过创建bthread获得更高的并发度，所以创建bthread必须很快。 在目前的实现中创建一个bthread的平均耗时小于200ns。如果每次都要从头创建，是不可能这么快的。创建过程更像是从一个bthread池子中取一个实例，我们又同时需要一个id来指代一个bthread，所以这儿正是ResourcePool的用武之地。bthread在代码中被称作Task，其结构被称为TaskMeta，定义在&lt;a href=&#34;https://github.com/brpc/brpc/blob/master/src/bthread/task_meta.h&#34;&gt;task_meta.h&lt;/a&gt;中，所有的TaskMeta由ResourcePool&lt;TaskMeta&gt;分配。&lt;/p&gt;
&lt;p&gt;bthread的大部分函数都需要在O(1)时间内通过bthread_t访问到TaskMeta，并且当bthread_t失效后，访问应返回NULL以让函数做出返回错误。解决方法是：bthread_t由32位的版本和32位的偏移量组成。版本解决&lt;a href=&#34;http://en.wikipedia.org/wiki/ABA_problem&#34;&gt;ABA问题&lt;/a&gt;，偏移量由ResourcePool&lt;TaskMeta&gt;分配。查找时先通过偏移量获得TaskMeta，再检查版本，如果版本不匹配，说明bthread失效了。注意：这只是大概的说法，在多线程环境下，即使版本相等，bthread仍可能随时失效，在不同的bthread函数中处理方法都是不同的，有些函数会加锁，有些则能忍受版本不相等。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://brpc.incubator.apache.org/images/docs/resource_pool.png&#34; alt=&#34;img&#34;&gt;&lt;/p&gt;
&lt;p&gt;这种id生成方式在brpc中应用广泛，brpc中的SocketId，bthread_id_t也是用类似的方法分配的。&lt;/p&gt;
&lt;h1 id=&#34;栈&#34;&gt;栈&lt;/h1&gt;
&lt;p&gt;使用ResourcePool加快创建的副作用是：一个pool中所有bthread的栈必须是一样大的。这似乎限制了用户的选择，不过基于我们的观察，大部分用户并不关心栈的具体大小，而只需要两种大小的栈：尺寸普通但数量较少，尺寸小但数量众多。所以我们用不同的pool管理不同大小的栈，用户可以根据场景选择。两种栈分别对应属性BTHREAD_ATTR_NORMAL（栈默认为1M）和BTHREAD_ATTR_SMALL（栈默认为32K）。用户还可以指定BTHREAD_ATTR_LARGE，这个属性的栈大小和pthread一样，由于尺寸较大，bthread不会对其做caching，创建速度较慢。server默认使用BTHREAD_ATTR_NORMAL运行用户代码。&lt;/p&gt;
&lt;p&gt;栈使用&lt;a href=&#34;http://linux.die.net/man/2/mmap&#34;&gt;mmap&lt;/a&gt;分配，bthread还会用mprotect分配4K的guard page以检测栈溢出。由于mmap+mprotect不能超过max_map_count（默认为65536），当bthread非常多后可能要调整此参数。另外当有很多bthread时，内存问题可能不仅仅是栈，也包括各类用户和系统buffer。&lt;/p&gt;
&lt;p&gt;goroutine在1.3前通过&lt;a href=&#34;https://gcc.gnu.org/wiki/SplitStacks&#34;&gt;segmented stacks&lt;/a&gt;动态地调整栈大小，发现有&lt;a href=&#34;https://docs.google.com/document/d/1wAaf1rYoM4S4gtnPh0zOlGzWtrZFQ5suE8qr2sD8uWQ/pub&#34;&gt;hot split&lt;/a&gt;问题后换成了变长连续栈（类似于vector resizing，只适合内存托管的语言）。由于bthread基本只会在64位平台上使用，虚存空间庞大，对变长栈需求不明确。加上segmented stacks的性能有影响，bthread暂时没有变长栈的计划。&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: Timer keeping</title>
      <link>https://brpc.incubator.apache.org/docs/rpc-in-depth/timer-keeping/</link>
      <pubDate>Thu, 12 Aug 2021 00:00:00 +0000</pubDate>
      
      <guid>https://brpc.incubator.apache.org/docs/rpc-in-depth/timer-keeping/</guid>
      <description>
        
        
        &lt;p&gt;在几点几分做某件事是RPC框架的基本需求，这件事比看上去难。&lt;/p&gt;
&lt;p&gt;让我们先来看看系统提供了些什么： posix系统能以&lt;a href=&#34;http://man7.org/linux/man-pages/man2/timer_create.2.html&#34;&gt;signal方式&lt;/a&gt;告知timer触发，不过signal逼迫我们使用全局变量，写&lt;a href=&#34;https://docs.oracle.com/cd/E19455-01/806-5257/gen-26/index.html&#34;&gt;async-signal-safe&lt;/a&gt;的函数，在面向用户的编程框架中，我们应当尽力避免使用signal。linux自2.6.27后能以&lt;a href=&#34;http://man7.org/linux/man-pages/man2/timerfd_create.2.html&#34;&gt;fd方式&lt;/a&gt;通知timer触发，这个fd可以放到epoll中和传输数据的fd统一管理。唯一问题是：这是个系统调用，且我们不清楚它在多线程下的表现。&lt;/p&gt;
&lt;p&gt;为什么这么关注timer的开销?让我们先来看一下RPC场景下一般是怎么使用timer的：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;在发起RPC过程中设定一个timer，在超时时间后取消还在等待中的RPC。几乎所有的RPC调用都有超时限制，都会设置这个timer。&lt;/li&gt;
&lt;li&gt;RPC结束前删除timer。大部分RPC都由正常返回的response导致结束，timer很少触发。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;你注意到了么，在RPC中timer更像是”保险机制”，在大部分情况下都不会发挥作用，自然地我们希望它的开销越小越好。一个几乎不触发的功能需要两次系统调用似乎并不理想。那在应用框架中一般是如何实现timer的呢？谈论这个问题需要区分“单线程”和“多线程”:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;在单线程框架中，比如以&lt;a href=&#34;http://libevent.org/&#34;&gt;libevent&lt;/a&gt;&lt;a href=&#34;http://en.wikipedia.org/wiki/Reactor_pattern&#34;&gt;, &lt;/a&gt;&lt;a href=&#34;http://software.schmorp.de/pkg/libev.html&#34;&gt;libev&lt;/a&gt;为代表的eventloop类库，或以&lt;a href=&#34;http://www.gnu.org/software/pth/pth-manual.html&#34;&gt;GNU Pth&lt;/a&gt;, &lt;a href=&#34;http://state-threads.sourceforge.net/index.html&#34;&gt;StateThreads&lt;/a&gt;为代表的coroutine / fiber类库中，一般是以&lt;a href=&#34;https://en.wikipedia.org/wiki/Heap_(data_structure)&#34;&gt;小顶堆&lt;/a&gt;记录触发时间。&lt;a href=&#34;http://man7.org/linux/man-pages/man2/epoll_wait.2.html&#34;&gt;epoll_wait&lt;/a&gt;前以堆顶的时间计算出参数timeout的值，如果在该时间内没有其他事件，epoll_wait也会醒来，从堆中弹出已超时的元素，调用相应的回调函数。整个框架周而复始地这么运转，timer的建立，等待，删除都发生在一个线程中。只要所有的回调都是非阻塞的，且逻辑不复杂，这套机制就能提供基本准确的timer。不过就像&lt;a href=&#34;../threading-overview/&#34;&gt;Threading Overview&lt;/a&gt;中说的那样，这不是RPC的场景。&lt;/li&gt;
&lt;li&gt;在多线程框架中，任何线程都可能被用户逻辑阻塞较长的时间，我们需要独立的线程实现timer，这种线程我们叫它TimerThread。一个非常自然的做法，就是使用用锁保护的小顶堆。当一个线程需要创建timer时，它先获得锁，然后把对应的时间插入堆，如果插入的元素成为了最早的，唤醒TimerThread。TimerThread中的逻辑和单线程类似，就是等着堆顶的元素超时，如果在等待过程中有更早的时间插入了，自己会被插入线程唤醒，而不会睡过头。这个方法的问题在于每个timer都需要竞争一把全局锁，操作一个全局小顶堆，就像在其他文章中反复谈到的那样，这会触发cache bouncing。同样数量的timer操作比单线程下的慢10倍是非常正常的，尴尬的是这些timer基本不触发。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;我们重点谈怎么解决多线程下的问题。&lt;/p&gt;
&lt;p&gt;一个惯例思路是把timer的需求散列到多个TimerThread，但这对TimerThread效果不好。注意我们上面提及到了那个“制约因素”：一旦插入的元素是最早的，要唤醒TimerThread。假设TimerThread足够多，以至于每个timer都散列到独立的TimerThread，那么每次它都要唤醒那个TimerThread。 “唤醒”意味着触发linux的调度函数，触发上下文切换。在非常流畅的系统中，这个开销大约是3-5微秒，这可比抢锁和同步cache还慢。这个因素是提高TimerThread扩展性的一个难点。多个TimerThread减少了对单个小顶堆的竞争压力，但同时也引入了更多唤醒。&lt;/p&gt;
&lt;p&gt;另一个难点是删除。一般用id指代一个Timer。通过这个id删除Timer有两种方式：1.抢锁，通过一个map查到对应timer在小顶堆中的位置，定点删除，这个map要和堆同步维护。2.通过id找到Timer的内存结构，做个标记，留待TimerThread自行发现和删除。第一种方法让插入逻辑更复杂了，删除也要抢锁，线程竞争更激烈。第二种方法在小顶堆内留了一大堆已删除的元素，让堆明显变大，插入和删除都变慢。&lt;/p&gt;
&lt;p&gt;第三个难点是TimerThread不应该经常醒。一个极端是TimerThread永远醒着或以较高频率醒过来（比如每1ms醒一次），这样插入timer的线程就不用负责唤醒了，然后我们把插入请求散列到多个堆降低竞争，问题看似解决了。但事实上这个方案提供的timer精度较差，一般高于2ms。你得想这个TimerThread怎么写逻辑，它是没法按堆顶元素的时间等待的，由于插入线程不唤醒，一旦有更早的元素插入，TimerThread就会睡过头。它唯一能做的是睡眠固定的时间，但这和现代OS scheduler的假设冲突：频繁sleep的线程的优先级最低。在linux下的结果就是，即使只sleep很短的时间，最终醒过来也可能超过2ms，因为在OS看来，这个线程不重要。一个高精度的TimerThread有唤醒机制，而不是定期醒。&lt;/p&gt;
&lt;p&gt;另外，更并发的数据结构也难以奏效，感兴趣的同学可以去搜索&amp;quot;concurrent priority queue&amp;quot;或&amp;quot;concurrent skip list&amp;quot;，这些数据结构一般假设插入的数值较为散开，所以可以同时修改结构内的不同部分。但这在RPC场景中也不成立，相互竞争的线程设定的时间往往聚集在同一个区域，因为程序的超时大都是一个值，加上当前时间后都差不多。&lt;/p&gt;
&lt;p&gt;这些因素让TimerThread的设计相当棘手。由于大部分用户的qps较低，不足以明显暴露这个扩展性问题，在r31791前我们一直沿用“用一把锁保护的TimerThread”。TimerThread是brpc在默认配置下唯一的高频竞争点，这个问题是我们一直清楚的技术债。随着brpc在高qps系统中应用越来越多，是时候解决这个问题了。r31791后的TimerThread解决了上述三个难点，timer操作几乎对RPC性能没有影响，我们先看下性能差异。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;在示例程序example/mutli_threaded_echo_c++中，r31791后TimerThread相比老TimerThread在24核E5-2620上（超线程），以50个bthread同步发送时，节省4%cpu（差不多1个核），qps提升10%左右；在400个bthread同步发送时，qps从30万上升到60万。新TimerThread的表现和完全关闭超时时接近。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;那新TimerThread是如何做到的？&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;一个TimerThread而不是多个。&lt;/li&gt;
&lt;li&gt;创建的timer散列到多个Bucket以降低线程间的竞争，默认13个Bucket。&lt;/li&gt;
&lt;li&gt;Bucket内不使用小顶堆管理时间，而是链表 + nearest_run_time字段，当插入的时间早于nearest_run_time时覆盖这个字段，之后去和全局nearest_run_time（和Bucket的nearest_run_time不同）比较，如果也早于这个时间，修改并唤醒TimerThread。链表节点在锁外使用&lt;a href=&#34;../memory-management/&#34;&gt;ResourcePool&lt;/a&gt;分配。&lt;/li&gt;
&lt;li&gt;删除时通过id直接定位到timer内存结构，修改一个标志，timer结构总是由TimerThread释放。&lt;/li&gt;
&lt;li&gt;TimerThread被唤醒后首先把全局nearest_run_time设置为几乎无限大(max of int64)，然后取出所有Bucket内的链表，并把Bucket的nearest_run_time设置为几乎无限大(max of int64)。TimerThread把未删除的timer插入小顶堆中维护，这个堆就它一个线程用。在每次运行回调或准备睡眠前都会检查全局nearest_run_time， 如果全局更早，说明有更早的时间加入了，重复这个过程。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;这里勾勒了TimerThread的大致工作原理，工程实现中还有不少细节问题，具体请阅读&lt;a href=&#34;https://github.com/brpc/brpc/blob/master/src/bthread/timer_thread.h&#34;&gt;timer_thread.h&lt;/a&gt;和&lt;a href=&#34;https://github.com/brpc/brpc/blob/master/src/bthread/timer_thread.cpp&#34;&gt;timer_thread.cpp&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;这个方法之所以有效：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Bucket锁内的操作是O(1)的，就是插入一个链表节点，临界区很小。节点本身的内存分配是在锁外的。&lt;/li&gt;
&lt;li&gt;由于大部分插入的时间是递增的，早于Bucket::nearest_run_time而参与全局竞争的timer很少。&lt;/li&gt;
&lt;li&gt;参与全局竞争的timer也就是和全局nearest_run_time比一下，临界区很小。&lt;/li&gt;
&lt;li&gt;和Bucket内类似，极少数Timer会早于全局nearest_run_time并去唤醒TimerThread。唤醒也在全局锁外。&lt;/li&gt;
&lt;li&gt;删除不参与全局竞争。&lt;/li&gt;
&lt;li&gt;TimerThread自己维护小顶堆，没有任何cache bouncing，效率很高。&lt;/li&gt;
&lt;li&gt;TimerThread醒来的频率大约是RPC超时的倒数，比如超时=100ms，TimerThread一秒内大约醒10次，已经最优。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;至此brpc在默认配置下不再有全局竞争点，在400个线程同时运行时，profiling也显示几乎没有对锁的等待。&lt;/p&gt;
&lt;p&gt;下面是一些和linux下时间管理相关的知识：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;epoll_wait的超时精度是毫秒，较差。pthread_cond_timedwait的超时使用timespec，精度到纳秒，一般是60微秒左右的延时。&lt;/li&gt;
&lt;li&gt;出于性能考虑，TimerThread使用wall-time，而不是单调时间，可能受到系统时间调整的影响。具体来说，如果在测试中把系统时间往前或往后调一个小时，程序行为将完全undefined。未来可能会让用户选择单调时间。&lt;/li&gt;
&lt;li&gt;在cpu支持nonstop_tsc和constant_tsc的机器上，brpc和bthread会优先使用基于rdtsc的cpuwide_time_us。那两个flag表示rdtsc可作为wall-time使用，不支持的机器上会转而使用较慢的内核时间。我们的机器（Intel Xeon系列）大都有那两个flag。rdtsc作为wall-time使用时是否会受到系统调整时间的影响，未测试不清楚。&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: bthread_id</title>
      <link>https://brpc.incubator.apache.org/docs/rpc-in-depth/bthread_id/</link>
      <pubDate>Thu, 12 Aug 2021 00:00:00 +0000</pubDate>
      
      <guid>https://brpc.incubator.apache.org/docs/rpc-in-depth/bthread_id/</guid>
      <description>
        
        
        &lt;p&gt;bthread_id是一个特殊的同步结构，它可以互斥RPC过程中的不同环节，也可以O(1)时间内找到RPC上下文(即Controller)。注意，这里我们谈论的是bthread_id_t，不是bthread_t（bthread的tid），这个名字起的确实不太好，容易混淆。&lt;/p&gt;
&lt;p&gt;具体来说，bthread_id解决的问题有：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;在发送RPC过程中response回来了，处理response的代码和发送代码产生竞争。&lt;/li&gt;
&lt;li&gt;设置timer后很快触发了，超时处理代码和发送代码产生竞争。&lt;/li&gt;
&lt;li&gt;重试产生的多个response同时回来产生的竞争。&lt;/li&gt;
&lt;li&gt;通过correlation_id在O(1)时间内找到对应的RPC上下文，而无需建立从correlation_id到RPC上下文的全局哈希表。&lt;/li&gt;
&lt;li&gt;取消RPC。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;上文提到的bug在其他rpc框架中广泛存在，下面我们来看下brpc是如何通过bthread_id解决这些问题的。&lt;/p&gt;
&lt;p&gt;bthread_id包括两部分，一个是用户可见的64位id，另一个是对应的不可见的bthread::Id结构体。用户接口都是操作id的。从id映射到结构体的方式和brpc中的&lt;a href=&#34;../memory-management/&#34;&gt;其他结构&lt;/a&gt;类似：32位是内存池的位移，32位是version。前者O(1)时间定位，后者防止ABA问题。&lt;/p&gt;
&lt;p&gt;bthread_id的接口不太简洁，有不少API：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;create&lt;/li&gt;
&lt;li&gt;lock&lt;/li&gt;
&lt;li&gt;unlock&lt;/li&gt;
&lt;li&gt;unlock_and_destroy&lt;/li&gt;
&lt;li&gt;join&lt;/li&gt;
&lt;li&gt;error&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;这么多接口是为了满足不同的使用流程。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;发送request的流程：bthread_id_create -&amp;gt; bthread_id_lock -&amp;gt; &amp;hellip; register timer and send RPC &amp;hellip; -&amp;gt; bthread_id_unlock&lt;/li&gt;
&lt;li&gt;接收response的流程：bthread_id_lock -&amp;gt; ..process response -&amp;gt; bthread_id_unlock_and_destroy&lt;/li&gt;
&lt;li&gt;异常处理流程：timeout/socket fail -&amp;gt; bthread_id_error -&amp;gt; 执行on_error回调(这里会加锁)，分两种情况
&lt;ul&gt;
&lt;li&gt;请求重试/backup request： 重新register timer and send RPC -&amp;gt; bthread_id_unlock&lt;/li&gt;
&lt;li&gt;无法重试，最终失败：bthread_id_unlock_and_destroy&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;同步等待RPC结束：bthread_id_join&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;为了减少等待，bthread_id做了一些优化的机制：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;error发生的时候，如果bthread_id已经被锁住，会把error信息放到一个pending queue中，bthread_id_error函数立即返回。当bthread_id_unlock的时候，如果pending queue里面有任务就取出来执行。&lt;/li&gt;
&lt;li&gt;RPC结束的时候，如果存在用户回调，先执行一个bthread_id_about_to_destroy，让正在等待的bthread_id_lock操作立即失败，再执行用户回调（这个可能耗时较长，不可控），最后再执行bthread_id_unlock_and_destroy&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
  </channel>
</rss>
