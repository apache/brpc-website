<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>bRPC – 深入RPC</title>
    <link>https://brpc.incubator.apache.org/zh/docs/rpc-in-depth/</link>
    <description>Recent content in 深入RPC on bRPC</description>
    <generator>Hugo -- gohugo.io</generator>
    <lastBuildDate>Thu, 12 Aug 2021 00:00:00 +0000</lastBuildDate>
    
	  <atom:link href="https://brpc.incubator.apache.org/zh/docs/rpc-in-depth/index.xml" rel="self" type="application/rss+xml" />
    
    
      
        
      
    
    
    <item>
      <title>Docs: 支持新协议</title>
      <link>https://brpc.incubator.apache.org/zh/docs/rpc-in-depth/new-protocol/</link>
      <pubDate>Thu, 12 Aug 2021 00:00:00 +0000</pubDate>
      
      <guid>https://brpc.incubator.apache.org/zh/docs/rpc-in-depth/new-protocol/</guid>
      <description>
        
        
        &lt;h1 id=&#34;server端多协议&#34;&gt;server端多协议&lt;/h1&gt;
&lt;p&gt;brpc server一个端口支持多种协议，大部分时候这对部署和运维更加方便。由于不同协议的格式大相径庭，严格地来说，一个端口很难无二义地支持所有协议。出于解耦和可扩展性的考虑，也不太可能集中式地构建一个针对所有协议的分类器。我们的做法就是把协议归三类后逐个尝试：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;第一类协议：标记或特殊字符在最前面，比如&lt;a href=&#34;baidu_std.md&#34;&gt;baidu_std&lt;/a&gt;，hulu_pbrpc的前4个字符分别是PRPC和HULU，解析代码只需要检查前4个字节就可以知道协议是否匹配，最先尝试这类协议。这些协议在同一个连接上也可以共存。&lt;/li&gt;
&lt;li&gt;第二类协议：有较为复杂的语法，没有固定的协议标记或特殊字符，可能在解析一段输入后才能判断是否匹配，目前此类协议只有http。&lt;/li&gt;
&lt;li&gt;第三类协议：协议标记或特殊字符在中间，比如nshead的magic_num在第25-28字节。由于之前的字段均为二进制，难以判断正确性，在没有读取完28字节前，我们无法判定消息是不是nshead格式的，所以处理起来很麻烦，若其解析排在http之前，那么&amp;lt;=28字节的http消息便可能无法被解析，因为程序以为是“还未完整的nshead消息”。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;考虑到大多数链接上只会有一种协议，我们会记录前一次的协议选择结果，下次首先尝试。对于长连接，这几乎把甄别协议的开销降到了0；虽然短连接每次都得运行这段逻辑，但由于短连接的瓶颈也往往不在于此，这套方法仍旧是足够快的。在未来如果有大量的协议加入，我们可能得考虑一些更复杂的启发式的区分方法。&lt;/p&gt;
&lt;h1 id=&#34;client端多协议&#34;&gt;client端多协议&lt;/h1&gt;
&lt;p&gt;不像server端必须根据连接上的数据动态地判定协议，client端作为发起端，自然清楚自己的协议格式，只要某种协议只通过连接池或短链接发送，即独占那个连接，那么它可以是任意复杂（或糟糕的）格式。因为client端发出时会记录所用的协议，等到response回来时直接调用对应的解析函数，没有任何甄别代价。像memcache，redis这类协议中基本没有magic number，在server端较难和其他协议区分开，但让client端支持却没什么问题。&lt;/p&gt;
&lt;h1 id=&#34;支持新协议&#34;&gt;支持新协议&lt;/h1&gt;
&lt;p&gt;brpc就是设计为可随时扩展新协议的，步骤如下：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;以nshead开头的协议有统一支持，看&lt;a href=&#34;nshead_service.md&#34;&gt;这里&lt;/a&gt;。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;增加protocoltype&#34;&gt;增加ProtocolType&lt;/h2&gt;
&lt;p&gt;在&lt;a href=&#34;https://github.com/brpc/brpc/blob/master/src/brpc/options.proto&#34;&gt;options.proto&lt;/a&gt;的ProtocolType中增加新协议类型，如果你需要的话可以联系我们增加，以确保不会和其他人的需求重合。&lt;/p&gt;
&lt;p&gt;目前的ProtocolType（18年中）:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-c++&#34; data-lang=&#34;c++&#34;&gt;&lt;span style=&#34;color:#204a87;font-weight:bold&#34;&gt;enum&lt;/span&gt; &lt;span style=&#34;color:#000&#34;&gt;ProtocolType&lt;/span&gt; &lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;{&lt;/span&gt;
    &lt;span style=&#34;color:#000&#34;&gt;PROTOCOL_UNKNOWN&lt;/span&gt; &lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#0000cf;font-weight:bold&#34;&gt;0&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;;&lt;/span&gt;
    &lt;span style=&#34;color:#000&#34;&gt;PROTOCOL_BAIDU_STD&lt;/span&gt; &lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#0000cf;font-weight:bold&#34;&gt;1&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;;&lt;/span&gt;
    &lt;span style=&#34;color:#000&#34;&gt;PROTOCOL_STREAMING_RPC&lt;/span&gt; &lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#0000cf;font-weight:bold&#34;&gt;2&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;;&lt;/span&gt;
    &lt;span style=&#34;color:#000&#34;&gt;PROTOCOL_HULU_PBRPC&lt;/span&gt; &lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#0000cf;font-weight:bold&#34;&gt;3&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;;&lt;/span&gt;
    &lt;span style=&#34;color:#000&#34;&gt;PROTOCOL_SOFA_PBRPC&lt;/span&gt; &lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#0000cf;font-weight:bold&#34;&gt;4&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;;&lt;/span&gt;
    &lt;span style=&#34;color:#000&#34;&gt;PROTOCOL_RTMP&lt;/span&gt; &lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#0000cf;font-weight:bold&#34;&gt;5&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;;&lt;/span&gt;
    &lt;span style=&#34;color:#000&#34;&gt;PROTOCOL_HTTP&lt;/span&gt; &lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#0000cf;font-weight:bold&#34;&gt;6&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;;&lt;/span&gt;
    &lt;span style=&#34;color:#000&#34;&gt;PROTOCOL_PUBLIC_PBRPC&lt;/span&gt; &lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#0000cf;font-weight:bold&#34;&gt;7&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;;&lt;/span&gt;
    &lt;span style=&#34;color:#000&#34;&gt;PROTOCOL_NOVA_PBRPC&lt;/span&gt; &lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#0000cf;font-weight:bold&#34;&gt;8&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;;&lt;/span&gt;
    &lt;span style=&#34;color:#000&#34;&gt;PROTOCOL_NSHEAD_CLIENT&lt;/span&gt; &lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#0000cf;font-weight:bold&#34;&gt;9&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;;&lt;/span&gt;        &lt;span style=&#34;color:#8f5902;font-style:italic&#34;&gt;// implemented in brpc-ub
&lt;/span&gt;&lt;span style=&#34;color:#8f5902;font-style:italic&#34;&gt;&lt;/span&gt;    &lt;span style=&#34;color:#000&#34;&gt;PROTOCOL_NSHEAD&lt;/span&gt; &lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#0000cf;font-weight:bold&#34;&gt;10&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;;&lt;/span&gt;
    &lt;span style=&#34;color:#000&#34;&gt;PROTOCOL_HADOOP_RPC&lt;/span&gt; &lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#0000cf;font-weight:bold&#34;&gt;11&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;;&lt;/span&gt;
    &lt;span style=&#34;color:#000&#34;&gt;PROTOCOL_HADOOP_SERVER_RPC&lt;/span&gt; &lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#0000cf;font-weight:bold&#34;&gt;12&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;;&lt;/span&gt;
    &lt;span style=&#34;color:#000&#34;&gt;PROTOCOL_MONGO&lt;/span&gt; &lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#0000cf;font-weight:bold&#34;&gt;13&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;;&lt;/span&gt;               &lt;span style=&#34;color:#8f5902;font-style:italic&#34;&gt;// server side only
&lt;/span&gt;&lt;span style=&#34;color:#8f5902;font-style:italic&#34;&gt;&lt;/span&gt;    &lt;span style=&#34;color:#000&#34;&gt;PROTOCOL_UBRPC_COMPACK&lt;/span&gt; &lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#0000cf;font-weight:bold&#34;&gt;14&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;;&lt;/span&gt;
    &lt;span style=&#34;color:#000&#34;&gt;PROTOCOL_DIDX_CLIENT&lt;/span&gt; &lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#0000cf;font-weight:bold&#34;&gt;15&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;;&lt;/span&gt;         &lt;span style=&#34;color:#8f5902;font-style:italic&#34;&gt;// Client side only
&lt;/span&gt;&lt;span style=&#34;color:#8f5902;font-style:italic&#34;&gt;&lt;/span&gt;    &lt;span style=&#34;color:#000&#34;&gt;PROTOCOL_REDIS&lt;/span&gt; &lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#0000cf;font-weight:bold&#34;&gt;16&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;;&lt;/span&gt;               &lt;span style=&#34;color:#8f5902;font-style:italic&#34;&gt;// Client side only
&lt;/span&gt;&lt;span style=&#34;color:#8f5902;font-style:italic&#34;&gt;&lt;/span&gt;    &lt;span style=&#34;color:#000&#34;&gt;PROTOCOL_MEMCACHE&lt;/span&gt; &lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#0000cf;font-weight:bold&#34;&gt;17&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;;&lt;/span&gt;            &lt;span style=&#34;color:#8f5902;font-style:italic&#34;&gt;// Client side only
&lt;/span&gt;&lt;span style=&#34;color:#8f5902;font-style:italic&#34;&gt;&lt;/span&gt;    &lt;span style=&#34;color:#000&#34;&gt;PROTOCOL_ITP&lt;/span&gt; &lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#0000cf;font-weight:bold&#34;&gt;18&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;;&lt;/span&gt;
    &lt;span style=&#34;color:#000&#34;&gt;PROTOCOL_NSHEAD_MCPACK&lt;/span&gt; &lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#0000cf;font-weight:bold&#34;&gt;19&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;;&lt;/span&gt;
    &lt;span style=&#34;color:#000&#34;&gt;PROTOCOL_DISP_IDL&lt;/span&gt; &lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#0000cf;font-weight:bold&#34;&gt;20&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;;&lt;/span&gt;            &lt;span style=&#34;color:#8f5902;font-style:italic&#34;&gt;// Client side only
&lt;/span&gt;&lt;span style=&#34;color:#8f5902;font-style:italic&#34;&gt;&lt;/span&gt;    &lt;span style=&#34;color:#000&#34;&gt;PROTOCOL_ERSDA_CLIENT&lt;/span&gt; &lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#0000cf;font-weight:bold&#34;&gt;21&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;;&lt;/span&gt;        &lt;span style=&#34;color:#8f5902;font-style:italic&#34;&gt;// Client side only
&lt;/span&gt;&lt;span style=&#34;color:#8f5902;font-style:italic&#34;&gt;&lt;/span&gt;    &lt;span style=&#34;color:#000&#34;&gt;PROTOCOL_UBRPC_MCPACK2&lt;/span&gt; &lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#0000cf;font-weight:bold&#34;&gt;22&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;;&lt;/span&gt;       &lt;span style=&#34;color:#8f5902;font-style:italic&#34;&gt;// Client side only
&lt;/span&gt;&lt;span style=&#34;color:#8f5902;font-style:italic&#34;&gt;&lt;/span&gt;    &lt;span style=&#34;color:#8f5902;font-style:italic&#34;&gt;// Reserve special protocol for cds-agent, which depends on FIFO right now
&lt;/span&gt;&lt;span style=&#34;color:#8f5902;font-style:italic&#34;&gt;&lt;/span&gt;    &lt;span style=&#34;color:#000&#34;&gt;PROTOCOL_CDS_AGENT&lt;/span&gt; &lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#0000cf;font-weight:bold&#34;&gt;23&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;;&lt;/span&gt;           &lt;span style=&#34;color:#8f5902;font-style:italic&#34;&gt;// Client side only
&lt;/span&gt;&lt;span style=&#34;color:#8f5902;font-style:italic&#34;&gt;&lt;/span&gt;    &lt;span style=&#34;color:#000&#34;&gt;PROTOCOL_ESP&lt;/span&gt; &lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#0000cf;font-weight:bold&#34;&gt;24&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;;&lt;/span&gt;                 &lt;span style=&#34;color:#8f5902;font-style:italic&#34;&gt;// Client side only
&lt;/span&gt;&lt;span style=&#34;color:#8f5902;font-style:italic&#34;&gt;&lt;/span&gt;    &lt;span style=&#34;color:#000&#34;&gt;PROTOCOL_THRIFT&lt;/span&gt; &lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#0000cf;font-weight:bold&#34;&gt;25&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;;&lt;/span&gt;              &lt;span style=&#34;color:#8f5902;font-style:italic&#34;&gt;// Server side only
&lt;/span&gt;&lt;span style=&#34;color:#8f5902;font-style:italic&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;实现回调&#34;&gt;实现回调&lt;/h2&gt;
&lt;p&gt;均定义在struct Protocol中，该结构定义在&lt;a href=&#34;https://github.com/brpc/brpc/blob/master/src/brpc/protocol.h&#34;&gt;protocol.h&lt;/a&gt;。其中的parse必须实现，除此之外server端至少要实现process_request，client端至少要实现serialize_request，pack_request，process_response。&lt;/p&gt;
&lt;p&gt;实现协议回调还是比较困难的，这块的代码不会像供普通用户使用的那样，有较好的提示和保护，你得先靠自己搞清楚其他协议中的类似代码，然后再动手，最后发给我们做code review。&lt;/p&gt;
&lt;h3 id=&#34;parse&#34;&gt;parse&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-c++&#34; data-lang=&#34;c++&#34;&gt;&lt;span style=&#34;color:#204a87;font-weight:bold&#34;&gt;typedef&lt;/span&gt; &lt;span style=&#34;color:#000&#34;&gt;ParseResult&lt;/span&gt; &lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;(&lt;/span&gt;&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;*&lt;/span&gt;&lt;span style=&#34;color:#000&#34;&gt;Parse&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;)(&lt;/span&gt;&lt;span style=&#34;color:#000&#34;&gt;butil&lt;/span&gt;&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;::&lt;/span&gt;&lt;span style=&#34;color:#000&#34;&gt;IOBuf&lt;/span&gt;&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;*&lt;/span&gt; &lt;span style=&#34;color:#000&#34;&gt;source&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;,&lt;/span&gt; &lt;span style=&#34;color:#000&#34;&gt;Socket&lt;/span&gt; &lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;*&lt;/span&gt;&lt;span style=&#34;color:#000&#34;&gt;socket&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;,&lt;/span&gt; &lt;span style=&#34;color:#204a87;font-weight:bold&#34;&gt;bool&lt;/span&gt; &lt;span style=&#34;color:#000&#34;&gt;read_eof&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;,&lt;/span&gt; &lt;span style=&#34;color:#204a87;font-weight:bold&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#204a87;font-weight:bold&#34;&gt;void&lt;/span&gt; &lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;*&lt;/span&gt;&lt;span style=&#34;color:#000&#34;&gt;arg&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;);&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;用于把消息从source上切割下来，client端和server端使用同一个parse函数。返回的消息会被递给process_request(server端)或process_response(client端)。&lt;/p&gt;
&lt;p&gt;参数：source是读取到的二进制内容，socket是对应的连接，read_eof为true表示连接已被对端关闭，arg在server端是对应server的指针，在client端是NULL。&lt;/p&gt;
&lt;p&gt;ParseResult可能是错误，也可能包含一个切割下来的message，可能的值有：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;PARSE_ERROR_TRY_OTHERS ：不是这个协议，框架会尝试下一个协议。source不能被消费。&lt;/li&gt;
&lt;li&gt;PARSE_ERROR_NOT_ENOUGH_DATA : 到目前为止数据内容不违反协议，但不构成完整的消息。等到连接上有新数据时，新数据会被append入source并重新调用parse。如果不确定数据是否一定属于这个协议，source不应被消费，如果确定数据属于这个协议，也可以把source的内容转移到内部的状态中去。比如http协议解析中即使source不包含一个完整的http消息，它也会被http parser消费掉，以避免下一次重复解析。&lt;/li&gt;
&lt;li&gt;PARSE_ERROR_TOO_BIG_DATA : 消息太大，拒绝掉以保护server，连接会被关闭。&lt;/li&gt;
&lt;li&gt;PARSE_ERROR_NO_RESOURCE  : 内部错误，比如资源分配失败。连接会被关闭。&lt;/li&gt;
&lt;li&gt;PARSE_ERROR_ABSOLUTELY_WRONG  : 应该是这个协议（比如magic number匹配了），但是格式不符合预期。连接会被关闭。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;serialize_request&#34;&gt;serialize_request&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-c++&#34; data-lang=&#34;c++&#34;&gt;&lt;span style=&#34;color:#204a87;font-weight:bold&#34;&gt;typedef&lt;/span&gt; &lt;span style=&#34;color:#000&#34;&gt;bool&lt;/span&gt; &lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;(&lt;/span&gt;&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;*&lt;/span&gt;&lt;span style=&#34;color:#000&#34;&gt;SerializeRequest&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;)(&lt;/span&gt;&lt;span style=&#34;color:#000&#34;&gt;butil&lt;/span&gt;&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;::&lt;/span&gt;&lt;span style=&#34;color:#000&#34;&gt;IOBuf&lt;/span&gt;&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;*&lt;/span&gt; &lt;span style=&#34;color:#000&#34;&gt;request_buf&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;,&lt;/span&gt;
                                 &lt;span style=&#34;color:#000&#34;&gt;Controller&lt;/span&gt;&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;*&lt;/span&gt; &lt;span style=&#34;color:#000&#34;&gt;cntl&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;,&lt;/span&gt;
                                 &lt;span style=&#34;color:#204a87;font-weight:bold&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#000&#34;&gt;google&lt;/span&gt;&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;::&lt;/span&gt;&lt;span style=&#34;color:#000&#34;&gt;protobuf&lt;/span&gt;&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;::&lt;/span&gt;&lt;span style=&#34;color:#000&#34;&gt;Message&lt;/span&gt;&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;*&lt;/span&gt; &lt;span style=&#34;color:#000&#34;&gt;request&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;);&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;把request序列化进request_buf，client端必须实现。发生在pack_request之前，一次RPC中只会调用一次。cntl包含某些协议（比如http）需要的信息。成功返回true，否则false。&lt;/p&gt;
&lt;h3 id=&#34;pack_request&#34;&gt;pack_request&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-c++&#34; data-lang=&#34;c++&#34;&gt;&lt;span style=&#34;color:#204a87;font-weight:bold&#34;&gt;typedef&lt;/span&gt; &lt;span style=&#34;color:#000&#34;&gt;int&lt;/span&gt; &lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;(&lt;/span&gt;&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;*&lt;/span&gt;&lt;span style=&#34;color:#000&#34;&gt;PackRequest&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;)(&lt;/span&gt;&lt;span style=&#34;color:#000&#34;&gt;butil&lt;/span&gt;&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;::&lt;/span&gt;&lt;span style=&#34;color:#000&#34;&gt;IOBuf&lt;/span&gt;&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;*&lt;/span&gt; &lt;span style=&#34;color:#000&#34;&gt;msg&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;,&lt;/span&gt; 
                           &lt;span style=&#34;color:#204a87;font-weight:bold&#34;&gt;uint64_t&lt;/span&gt; &lt;span style=&#34;color:#000&#34;&gt;correlation_id&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;,&lt;/span&gt;
                           &lt;span style=&#34;color:#204a87;font-weight:bold&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#000&#34;&gt;google&lt;/span&gt;&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;::&lt;/span&gt;&lt;span style=&#34;color:#000&#34;&gt;protobuf&lt;/span&gt;&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;::&lt;/span&gt;&lt;span style=&#34;color:#000&#34;&gt;MethodDescriptor&lt;/span&gt;&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;*&lt;/span&gt; &lt;span style=&#34;color:#000&#34;&gt;method&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;,&lt;/span&gt;
                           &lt;span style=&#34;color:#000&#34;&gt;Controller&lt;/span&gt;&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;*&lt;/span&gt; &lt;span style=&#34;color:#000&#34;&gt;controller&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;,&lt;/span&gt;
                           &lt;span style=&#34;color:#204a87;font-weight:bold&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#000&#34;&gt;butil&lt;/span&gt;&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;::&lt;/span&gt;&lt;span style=&#34;color:#000&#34;&gt;IOBuf&lt;/span&gt;&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;&amp;amp;&lt;/span&gt; &lt;span style=&#34;color:#000&#34;&gt;request_buf&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;,&lt;/span&gt;
                           &lt;span style=&#34;color:#204a87;font-weight:bold&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#000&#34;&gt;Authenticator&lt;/span&gt;&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;*&lt;/span&gt; &lt;span style=&#34;color:#000&#34;&gt;auth&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;);&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;把request_buf打包入msg，每次向server发送消息前（包括重试）都会调用。当auth不为空时，需要打包认证信息。成功返回0，否则-1。&lt;/p&gt;
&lt;h3 id=&#34;process_request&#34;&gt;process_request&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-c++&#34; data-lang=&#34;c++&#34;&gt;&lt;span style=&#34;color:#204a87;font-weight:bold&#34;&gt;typedef&lt;/span&gt; &lt;span style=&#34;color:#000&#34;&gt;void&lt;/span&gt; &lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;(&lt;/span&gt;&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;*&lt;/span&gt;&lt;span style=&#34;color:#000&#34;&gt;ProcessRequest&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;)(&lt;/span&gt;&lt;span style=&#34;color:#000&#34;&gt;InputMessageBase&lt;/span&gt;&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;*&lt;/span&gt; &lt;span style=&#34;color:#000&#34;&gt;msg_base&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;);&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;处理server端parse返回的消息，server端必须实现。可能会在和parse()不同的线程中运行。多个process_request可能同时运行。&lt;/p&gt;
&lt;p&gt;在r34386后必须在处理结束时调用msg_base-&amp;gt;Destroy()，为了防止漏调，考虑使用DestroyingPtr&amp;lt;&amp;gt;。&lt;/p&gt;
&lt;h3 id=&#34;process_response&#34;&gt;process_response&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-c++&#34; data-lang=&#34;c++&#34;&gt;&lt;span style=&#34;color:#204a87;font-weight:bold&#34;&gt;typedef&lt;/span&gt; &lt;span style=&#34;color:#000&#34;&gt;void&lt;/span&gt; &lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;(&lt;/span&gt;&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;*&lt;/span&gt;&lt;span style=&#34;color:#000&#34;&gt;ProcessResponse&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;)(&lt;/span&gt;&lt;span style=&#34;color:#000&#34;&gt;InputMessageBase&lt;/span&gt;&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;*&lt;/span&gt; &lt;span style=&#34;color:#000&#34;&gt;msg_base&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;);&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;处理client端parse返回的消息，client端必须实现。可能会在和parse()不同的线程中运行。多个process_response可能同时运行。&lt;/p&gt;
&lt;p&gt;在r34386后必须在处理结束时调用msg_base-&amp;gt;Destroy()，为了防止漏调，考虑使用DestroyingPtr&amp;lt;&amp;gt;。&lt;/p&gt;
&lt;h3 id=&#34;verify&#34;&gt;verify&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-c++&#34; data-lang=&#34;c++&#34;&gt;&lt;span style=&#34;color:#204a87;font-weight:bold&#34;&gt;typedef&lt;/span&gt; &lt;span style=&#34;color:#000&#34;&gt;bool&lt;/span&gt; &lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;(&lt;/span&gt;&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;*&lt;/span&gt;&lt;span style=&#34;color:#000&#34;&gt;Verify&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;)(&lt;/span&gt;&lt;span style=&#34;color:#204a87;font-weight:bold&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#000&#34;&gt;InputMessageBase&lt;/span&gt;&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;*&lt;/span&gt; &lt;span style=&#34;color:#000&#34;&gt;msg&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;);&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;处理连接的认证，只会对连接上的第一个消息调用，需要支持认证的server端必须实现，不需要认证或仅支持client端的协议可填NULL。成功返回true，否则false。&lt;/p&gt;
&lt;h3 id=&#34;parse_server_address&#34;&gt;parse_server_address&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-c++&#34; data-lang=&#34;c++&#34;&gt;&lt;span style=&#34;color:#204a87;font-weight:bold&#34;&gt;typedef&lt;/span&gt; &lt;span style=&#34;color:#000&#34;&gt;bool&lt;/span&gt; &lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;(&lt;/span&gt;&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;*&lt;/span&gt;&lt;span style=&#34;color:#000&#34;&gt;ParseServerAddress&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;)(&lt;/span&gt;&lt;span style=&#34;color:#000&#34;&gt;butil&lt;/span&gt;&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;::&lt;/span&gt;&lt;span style=&#34;color:#000&#34;&gt;EndPoint&lt;/span&gt;&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;*&lt;/span&gt; &lt;span style=&#34;color:#000&#34;&gt;out&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;,&lt;/span&gt; &lt;span style=&#34;color:#204a87;font-weight:bold&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#204a87;font-weight:bold&#34;&gt;char&lt;/span&gt;&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;*&lt;/span&gt; &lt;span style=&#34;color:#000&#34;&gt;server_addr_and_port&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;);&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;把server_addr_and_port(Channel.Init的一个参数)转化为butil::EndPoint，可选。一些协议对server地址的表达和理解可能是不同的。&lt;/p&gt;
&lt;h3 id=&#34;get_method_name&#34;&gt;get_method_name&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-c++&#34; data-lang=&#34;c++&#34;&gt;&lt;span style=&#34;color:#204a87;font-weight:bold&#34;&gt;typedef&lt;/span&gt; &lt;span style=&#34;color:#204a87;font-weight:bold&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#000&#34;&gt;std&lt;/span&gt;&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;::&lt;/span&gt;&lt;span style=&#34;color:#000&#34;&gt;string&lt;/span&gt;&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;&amp;amp;&lt;/span&gt; &lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;(&lt;/span&gt;&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;*&lt;/span&gt;&lt;span style=&#34;color:#000&#34;&gt;GetMethodName&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;)(&lt;/span&gt;&lt;span style=&#34;color:#204a87;font-weight:bold&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#000&#34;&gt;google&lt;/span&gt;&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;::&lt;/span&gt;&lt;span style=&#34;color:#000&#34;&gt;protobuf&lt;/span&gt;&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;::&lt;/span&gt;&lt;span style=&#34;color:#000&#34;&gt;MethodDescriptor&lt;/span&gt;&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;*&lt;/span&gt; &lt;span style=&#34;color:#000&#34;&gt;method&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;,&lt;/span&gt;
                                            &lt;span style=&#34;color:#204a87;font-weight:bold&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#000&#34;&gt;Controller&lt;/span&gt;&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;*&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;);&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;定制method name，可选。&lt;/p&gt;
&lt;h3 id=&#34;supported_connection_type&#34;&gt;supported_connection_type&lt;/h3&gt;
&lt;p&gt;标记支持的连接方式。如果支持所有连接方式，设为CONNECTION_TYPE_ALL。如果只支持连接池和短连接，设为CONNECTION_TYPE_POOLED_AND_SHORT。&lt;/p&gt;
&lt;h3 id=&#34;name&#34;&gt;name&lt;/h3&gt;
&lt;p&gt;协议的名称，会出现在各种配置和显示中，越简短越好，必须是字符串常量。&lt;/p&gt;
&lt;h2 id=&#34;注册到全局&#34;&gt;注册到全局&lt;/h2&gt;
&lt;p&gt;实现好的协议要调用RegisterProtocol&lt;a href=&#34;https://github.com/brpc/brpc/blob/master/src/brpc/global.cpp&#34;&gt;注册到全局&lt;/a&gt;，以便brpc发现。就像这样：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-c++&#34; data-lang=&#34;c++&#34;&gt;&lt;span style=&#34;color:#000&#34;&gt;Protocol&lt;/span&gt; &lt;span style=&#34;color:#000&#34;&gt;http_protocol&lt;/span&gt; &lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;{&lt;/span&gt; &lt;span style=&#34;color:#000&#34;&gt;ParseHttpMessage&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;,&lt;/span&gt;
                           &lt;span style=&#34;color:#000&#34;&gt;SerializeHttpRequest&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;,&lt;/span&gt; &lt;span style=&#34;color:#000&#34;&gt;PackHttpRequest&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;,&lt;/span&gt;
                           &lt;span style=&#34;color:#000&#34;&gt;ProcessHttpRequest&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;,&lt;/span&gt; &lt;span style=&#34;color:#000&#34;&gt;ProcessHttpResponse&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;,&lt;/span&gt;
                           &lt;span style=&#34;color:#000&#34;&gt;VerifyHttpRequest&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;,&lt;/span&gt; &lt;span style=&#34;color:#000&#34;&gt;ParseHttpServerAddress&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;,&lt;/span&gt;
                           &lt;span style=&#34;color:#000&#34;&gt;GetHttpMethodName&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;,&lt;/span&gt;
                           &lt;span style=&#34;color:#000&#34;&gt;CONNECTION_TYPE_POOLED_AND_SHORT&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;,&lt;/span&gt;
                           &lt;span style=&#34;color:#4e9a06&#34;&gt;&amp;#34;http&amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;};&lt;/span&gt;
&lt;span style=&#34;color:#204a87;font-weight:bold&#34;&gt;if&lt;/span&gt; &lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;(&lt;/span&gt;&lt;span style=&#34;color:#000&#34;&gt;RegisterProtocol&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;(&lt;/span&gt;&lt;span style=&#34;color:#000&#34;&gt;PROTOCOL_HTTP&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;,&lt;/span&gt; &lt;span style=&#34;color:#000&#34;&gt;http_protocol&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;)&lt;/span&gt; &lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;!=&lt;/span&gt; &lt;span style=&#34;color:#0000cf;font-weight:bold&#34;&gt;0&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;)&lt;/span&gt; &lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;{&lt;/span&gt;
    &lt;span style=&#34;color:#000&#34;&gt;exit&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;(&lt;/span&gt;&lt;span style=&#34;color:#0000cf;font-weight:bold&#34;&gt;1&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;);&lt;/span&gt;
&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
      </description>
    </item>
    
    <item>
      <title>Docs: 原子指令</title>
      <link>https://brpc.incubator.apache.org/zh/docs/rpc-in-depth/atomic-instructions/</link>
      <pubDate>Thu, 12 Aug 2021 00:00:00 +0000</pubDate>
      
      <guid>https://brpc.incubator.apache.org/zh/docs/rpc-in-depth/atomic-instructions/</guid>
      <description>
        
        
        &lt;p&gt;我们都知道多核编程常用锁避免多个线程在修改同一个数据时产生&lt;a href=&#34;http://en.wikipedia.org/wiki/Race_condition&#34;&gt;race condition&lt;/a&gt;。当锁成为性能瓶颈时，我们又总想试着绕开它，而不可避免地接触了原子指令。但在实践中，用原子指令写出正确的代码是一件非常困难的事，琢磨不透的race condition、&lt;a href=&#34;https://en.wikipedia.org/wiki/ABA_problem&#34;&gt;ABA problem&lt;/a&gt;、&lt;a href=&#34;https://en.wikipedia.org/wiki/Memory_barrier&#34;&gt;memory fence&lt;/a&gt;很烧脑，这篇文章试图通过介绍&lt;a href=&#34;http://en.wikipedia.org/wiki/Symmetric_multiprocessing&#34;&gt;SMP&lt;/a&gt;架构下的原子指令帮助大家入门。C++11正式引入了&lt;a href=&#34;http://en.cppreference.com/w/cpp/atomic/atomic&#34;&gt;原子指令&lt;/a&gt;，我们就以其语法描述。&lt;/p&gt;
&lt;p&gt;顾名思义，原子指令是&lt;strong&gt;对软件&lt;/strong&gt;不可再分的指令，比如x.fetch_add(n)指原子地给x加上n，这个指令&lt;strong&gt;对软件&lt;/strong&gt;要么没做，要么完成，不会观察到中间状态。常见的原子指令有：&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;原子指令 (x均为std::atomic&lt;int&gt;)&lt;/th&gt;
&lt;th&gt;作用&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;x.load()&lt;/td&gt;
&lt;td&gt;返回x的值。&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;x.store(n)&lt;/td&gt;
&lt;td&gt;把x设为n，什么都不返回。&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;x.exchange(n)&lt;/td&gt;
&lt;td&gt;把x设为n，返回设定之前的值。&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;x.compare_exchange_strong(expected_ref, desired)&lt;/td&gt;
&lt;td&gt;若x等于expected_ref，则设为desired，返回成功；否则把最新值写入expected_ref，返回失败。&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;x.compare_exchange_weak(expected_ref, desired)&lt;/td&gt;
&lt;td&gt;相比compare_exchange_strong可能有&lt;a href=&#34;http://en.wikipedia.org/wiki/Spurious_wakeup&#34;&gt;spurious wakeup&lt;/a&gt;。&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;x.fetch_add(n), x.fetch_sub(n)&lt;/td&gt;
&lt;td&gt;原子地做x += n, x-= n，返回修改之前的值。&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;你已经可以用这些指令做原子计数，比如多个线程同时累加一个原子变量，以统计这些线程对一些资源的操作次数。但是，这可能会有两个问题：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;这个操作没有你想象地快。&lt;/li&gt;
&lt;li&gt;如果你尝试通过看似简单的原子操作控制对一些资源的访问，你的程序有很大几率会crash。&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;cacheline&#34;&gt;Cacheline&lt;/h1&gt;
&lt;p&gt;没有任何竞争或只被一个线程访问的原子操作是比较快的，“竞争”指的是多个线程同时访问同一个&lt;a href=&#34;https://en.wikipedia.org/wiki/CPU_cache#Cache_entries&#34;&gt;cacheline&lt;/a&gt;。现代CPU为了以低价格获得高性能，大量使用了cache，并把cache分了多级。百度内常见的Intel E5-2620拥有32K的L1 dcache和icache，256K的L2 cache和15M的L3 cache。其中L1和L2 cache为每个核心独有，L3则所有核心共享。一个核心写入自己的L1 cache是极快的(4 cycles, ~2ns)，但当另一个核心读或写同一处内存时，它得确认看到其他核心中对应的cacheline。对于软件来说，这个过程是原子的，不能在中间穿插其他代码，只能等待CPU完成&lt;a href=&#34;https://en.wikipedia.org/wiki/Cache_coherence&#34;&gt;一致性同步&lt;/a&gt;，这个复杂的硬件算法使得原子操作会变得很慢，在E5-2620上竞争激烈时fetch_add会耗费700纳秒左右。访问被多个线程频繁共享的内存往往是比较慢的。比如像一些场景临界区看着很小，但保护它的spinlock性能不佳，因为spinlock使用的exchange, fetch_add等指令必须等待最新的cacheline，看上去只有几条指令，花费若干微秒并不奇怪。&lt;/p&gt;
&lt;p&gt;要提高性能，就要避免让CPU频繁同步cacheline。这不单和原子指令本身的性能有关，还会影响到程序的整体性能。最有效的解决方法很直白：&lt;strong&gt;尽量避免共享&lt;/strong&gt;。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;一个依赖全局多生产者多消费者队列(MPMC)的程序难有很好的多核扩展性，因为这个队列的极限吞吐取决于同步cache的延时，而不是核心的个数。最好是用多个SPMC或多个MPSC队列，甚至多个SPSC队列代替，在源头就规避掉竞争。&lt;/li&gt;
&lt;li&gt;另一个例子是计数器，如果所有线程都频繁修改一个计数器，性能就会很差，原因同样在于不同的核心在不停地同步同一个cacheline。如果这个计数器只是用作打打日志之类的，那我们完全可以让每个线程修改thread-local变量，在需要时再合并所有线程中的值，性能可能有&lt;a href=&#34;bvar.md&#34;&gt;几十倍的差别&lt;/a&gt;。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;一个相关的编程陷阱是false sharing：对那些不怎么被修改甚至只读变量的访问，由于同一个cacheline中的其他变量被频繁修改，而不得不经常等待cacheline同步而显著变慢了。多线程中的变量尽量按访问规律排列，频繁被其他线程修改的变量要放在独立的cacheline中。要让一个变量或结构体按cacheline对齐，可以include &amp;lt;butil/macros.h&amp;gt;后使用BAIDU_CACHELINE_ALIGNMENT宏，请自行grep brpc的代码了解用法。&lt;/p&gt;
&lt;h1 id=&#34;memory-fence&#34;&gt;Memory fence&lt;/h1&gt;
&lt;p&gt;仅靠原子技术实现不了对资源的访问控制，即使简单如&lt;a href=&#34;https://en.wikipedia.org/wiki/Spinlock&#34;&gt;spinlock&lt;/a&gt;或&lt;a href=&#34;https://en.wikipedia.org/wiki/Reference_counting&#34;&gt;引用计数&lt;/a&gt;，看上去正确的代码也可能会crash。这里的关键在于&lt;strong&gt;重排指令&lt;/strong&gt;导致了读写顺序的变化。只要没有依赖，代码中在后面的指令就可能跑到前面去，&lt;a href=&#34;http://preshing.com/20120625/memory-ordering-at-compile-time/&#34;&gt;编译器&lt;/a&gt;和&lt;a href=&#34;https://en.wikipedia.org/wiki/Out-of-order_execution&#34;&gt;CPU&lt;/a&gt;都会这么做。&lt;/p&gt;
&lt;p&gt;这么做的动机非常自然，CPU要尽量塞满每个cycle，在单位时间内运行尽量多的指令。如上节中提到的，访存指令在等待cacheline同步时要花费数百纳秒，最高效地自然是同时同步多个cacheline，而不是一个个做。一个线程在代码中对多个变量的依次修改，可能会以不同的次序同步到另一个线程所在的核心上。不同线程对数据的需求不同，按需同步也会导致cacheline的读序和写序不同。&lt;/p&gt;
&lt;p&gt;如果其中第一个变量扮演了开关的作用，控制对后续变量的访问。那么当这些变量被一起同步到其他核心时，更新顺序可能变了，第一个变量未必是第一个更新的，然而其他线程还认为它代表着其他变量有效，去访问了实际已被删除的变量，从而导致未定义的行为。比如下面的代码片段：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-c++&#34; data-lang=&#34;c++&#34;&gt;&lt;span style=&#34;color:#8f5902;font-style:italic&#34;&gt;// Thread 1
&lt;/span&gt;&lt;span style=&#34;color:#8f5902;font-style:italic&#34;&gt;// ready was initialized to false
&lt;/span&gt;&lt;span style=&#34;color:#8f5902;font-style:italic&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#000&#34;&gt;p&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;.&lt;/span&gt;&lt;span style=&#34;color:#000&#34;&gt;init&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;();&lt;/span&gt;
&lt;span style=&#34;color:#000&#34;&gt;ready&lt;/span&gt; &lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#204a87&#34;&gt;true&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-c++&#34; data-lang=&#34;c++&#34;&gt;&lt;span style=&#34;color:#8f5902;font-style:italic&#34;&gt;// Thread2
&lt;/span&gt;&lt;span style=&#34;color:#8f5902;font-style:italic&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#204a87;font-weight:bold&#34;&gt;if&lt;/span&gt; &lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;(&lt;/span&gt;&lt;span style=&#34;color:#000&#34;&gt;ready&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;)&lt;/span&gt; &lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;{&lt;/span&gt;
    &lt;span style=&#34;color:#000&#34;&gt;p&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;.&lt;/span&gt;&lt;span style=&#34;color:#000&#34;&gt;bar&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;();&lt;/span&gt;
&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;从人的角度，这是对的，因为线程2在ready为true时才会访问p，按线程1的逻辑，此时p应该初始化好了。但对多核机器而言，这段代码可能难以正常运行：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;线程1中的ready = true可能会被编译器或cpu重排到p.init()之前，从而使线程2看到ready为true时，p仍然未初始化。这种情况同样也会在线程2中发生，p.bar()中的一些代码可能被重排到检查ready之前。&lt;/li&gt;
&lt;li&gt;即使没有重排，ready和p的值也会独立地同步到线程2所在核心的cache，线程2仍然可能在看到ready为true时看到未初始化的p。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;注：x86/x64的load带acquire语意，store带release语意，上面的代码刨除编译器和CPU因素可以正确运行。&lt;/p&gt;
&lt;p&gt;通过这个简单例子，你可以窥见原子指令编程的复杂性了吧。为了解决这个问题，CPU和编译器提供了&lt;a href=&#34;http://en.wikipedia.org/wiki/Memory_barrier&#34;&gt;memory fence&lt;/a&gt;，让用户可以声明访存指令间的可见性(visibility)关系，boost和C++11对memory fence做了抽象，总结为如下几种&lt;a href=&#34;http://en.cppreference.com/w/cpp/atomic/memory_order&#34;&gt;memory order&lt;/a&gt;.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;memory order&lt;/th&gt;
&lt;th&gt;作用&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;memory_order_relaxed&lt;/td&gt;
&lt;td&gt;没有fencing作用&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;memory_order_consume&lt;/td&gt;
&lt;td&gt;后面依赖此原子变量的访存指令勿重排至此条指令之前&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;memory_order_acquire&lt;/td&gt;
&lt;td&gt;后面访存指令勿重排至此条指令之前&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;memory_order_release&lt;/td&gt;
&lt;td&gt;前面访存指令勿重排至此条指令之后。当此条指令的结果对其他线程可见后，之前的所有指令都可见&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;memory_order_acq_rel&lt;/td&gt;
&lt;td&gt;acquire + release语意&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;memory_order_seq_cst&lt;/td&gt;
&lt;td&gt;acq_rel语意外加所有使用seq_cst的指令有严格地全序关系&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;有了memory order，上面的例子可以这么更正：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-c++&#34; data-lang=&#34;c++&#34;&gt;&lt;span style=&#34;color:#8f5902;font-style:italic&#34;&gt;// Thread1
&lt;/span&gt;&lt;span style=&#34;color:#8f5902;font-style:italic&#34;&gt;// ready was initialized to false
&lt;/span&gt;&lt;span style=&#34;color:#8f5902;font-style:italic&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#000&#34;&gt;p&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;.&lt;/span&gt;&lt;span style=&#34;color:#000&#34;&gt;init&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;();&lt;/span&gt;
&lt;span style=&#34;color:#000&#34;&gt;ready&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;.&lt;/span&gt;&lt;span style=&#34;color:#000&#34;&gt;store&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;(&lt;/span&gt;&lt;span style=&#34;color:#204a87&#34;&gt;true&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;,&lt;/span&gt; &lt;span style=&#34;color:#000&#34;&gt;std&lt;/span&gt;&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;::&lt;/span&gt;&lt;span style=&#34;color:#000&#34;&gt;memory_order_release&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;);&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-c++&#34; data-lang=&#34;c++&#34;&gt;&lt;span style=&#34;color:#8f5902;font-style:italic&#34;&gt;// Thread2
&lt;/span&gt;&lt;span style=&#34;color:#8f5902;font-style:italic&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#204a87;font-weight:bold&#34;&gt;if&lt;/span&gt; &lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;(&lt;/span&gt;&lt;span style=&#34;color:#000&#34;&gt;ready&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;.&lt;/span&gt;&lt;span style=&#34;color:#000&#34;&gt;load&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;(&lt;/span&gt;&lt;span style=&#34;color:#000&#34;&gt;std&lt;/span&gt;&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;::&lt;/span&gt;&lt;span style=&#34;color:#000&#34;&gt;memory_order_acquire&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;))&lt;/span&gt; &lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;{&lt;/span&gt;
    &lt;span style=&#34;color:#000&#34;&gt;p&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;.&lt;/span&gt;&lt;span style=&#34;color:#000&#34;&gt;bar&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;();&lt;/span&gt;
&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;线程2中的acquire和线程1的release配对，确保线程2在看到ready==true时能看到线程1 release之前所有的访存操作。&lt;/p&gt;
&lt;p&gt;注意，memory fence不等于可见性，即使线程2恰好在线程1在把ready设置为true后读取了ready也不意味着它能看到true，因为同步cache是有延时的。memory fence保证的是可见性的顺序：“假如我看到了a的最新值，那么我一定也得看到b的最新值”。&lt;/p&gt;
&lt;p&gt;一个相关问题是：如何知道看到的值是新还是旧？一般分两种情况：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;值是特殊的。比如在上面的例子中，ready=true是个特殊值，只要线程2看到ready为true就意味着更新了。只要设定了特殊值，读到或没有读到特殊值都代表了一种含义。&lt;/li&gt;
&lt;li&gt;总是累加。一些场景下没有特殊值，那我们就用fetch_add之类的指令累加一个变量，只要变量的值域足够大，在很长一段时间内，新值和之前所有的旧值都会不相同，我们就能区分彼此了。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;原子指令的例子可以看boost.atomic的&lt;a href=&#34;http://www.boost.org/doc/libs/1_56_0/doc/html/atomic/usage_examples.html&#34;&gt;Example&lt;/a&gt;，atomic的官方描述可以看&lt;a href=&#34;http://en.cppreference.com/w/cpp/atomic/atomic&#34;&gt;这里&lt;/a&gt;。&lt;/p&gt;
&lt;h1 id=&#34;wait-free--lock-free&#34;&gt;wait-free &amp;amp; lock-free&lt;/h1&gt;
&lt;p&gt;原子指令能为我们的服务赋予两个重要属性：&lt;a href=&#34;http://en.wikipedia.org/wiki/Non-blocking_algorithm#Wait-freedom&#34;&gt;wait-free&lt;/a&gt;和&lt;a href=&#34;http://en.wikipedia.org/wiki/Non-blocking_algorithm#Lock-freedom&#34;&gt;lock-free&lt;/a&gt;。前者指不管OS如何调度线程，每个线程都始终在做有用的事；后者比前者弱一些，指不管OS如何调度线程，至少有一个线程在做有用的事。如果我们的服务中使用了锁，那么OS可能把一个刚获得锁的线程切换出去，这时候所有依赖这个锁的线程都在等待，而没有做有用的事，所以用了锁就不是lock-free，更不会是wait-free。为了确保一件事情总在确定时间内完成，实时系统的关键代码至少是lock-free的。在百度广泛又多样的在线服务中，对时效性也有着严苛的要求，如果RPC中最关键的部分满足wait-free或lock-free，就可以提供更稳定的服务质量。事实上，brpc中的读写都是wait-free的，具体见&lt;a href=&#34;../io&#34;&gt;IO&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;值得提醒的是，常见想法是lock-free或wait-free的算法会更快，但事实可能相反，因为：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;lock-free和wait-free必须处理更多更复杂的race condition和ABA problem，完成相同目的的代码比用锁更复杂。代码越多，耗时就越长。&lt;/li&gt;
&lt;li&gt;使用mutex的算法变相带“后退”效果。后退(backoff)指出现竞争时尝试另一个途径以临时避免竞争，mutex出现竞争时会使调用者睡眠，使拿到锁的那个线程可以很快地独占完成一系列流程，总体吞吐可能反而高了。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;mutex导致低性能往往是因为临界区过大（限制了并发度），或竞争过于激烈（上下文切换开销变得突出）。lock-free/wait-free算法的价值在于其保证了一个或所有线程始终在做有用的事，而不是绝对的高性能。但在一种情况下lock-free和wait-free算法的性能多半更高：就是算法本身可以用少量原子指令实现。实现锁也是要用原子指令的，当算法本身用一两条指令就能完成的时候，相比额外用锁肯定是更快了。&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: IO</title>
      <link>https://brpc.incubator.apache.org/zh/docs/rpc-in-depth/io/</link>
      <pubDate>Thu, 12 Aug 2021 00:00:00 +0000</pubDate>
      
      <guid>https://brpc.incubator.apache.org/zh/docs/rpc-in-depth/io/</guid>
      <description>
        
        
        &lt;p&gt;一般有三种操作IO的方式：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;blocking IO: 发起IO操作后阻塞当前线程直到IO结束，标准的同步IO，如默认行为的posix &lt;a href=&#34;http://linux.die.net/man/2/read&#34;&gt;read&lt;/a&gt;和&lt;a href=&#34;http://linux.die.net/man/2/write&#34;&gt;write&lt;/a&gt;。&lt;/li&gt;
&lt;li&gt;non-blocking IO: 发起IO操作后不阻塞，用户可阻塞等待多个IO操作同时结束。non-blocking也是一种同步IO：“批量的同步”。如linux下的&lt;a href=&#34;http://linux.die.net/man/2/poll&#34;&gt;poll&lt;/a&gt;,&lt;a href=&#34;http://linux.die.net/man/2/select&#34;&gt;select&lt;/a&gt;, &lt;a href=&#34;http://linux.die.net/man/4/epoll&#34;&gt;epoll&lt;/a&gt;，BSD下的&lt;a href=&#34;https://www.freebsd.org/cgi/man.cgi?query=kqueue&amp;amp;sektion=2&#34;&gt;kqueue&lt;/a&gt;。&lt;/li&gt;
&lt;li&gt;asynchronous IO: 发起IO操作后不阻塞，用户得递一个回调待IO结束后被调用。如windows下的&lt;a href=&#34;https://msdn.microsoft.com/en-us/library/windows/desktop/ms684342(v=vs.85).aspx&#34;&gt;OVERLAPPED&lt;/a&gt; + &lt;a href=&#34;https://msdn.microsoft.com/en-us/library/windows/desktop/aa365198(v=vs.85).aspx&#34;&gt;IOCP&lt;/a&gt;。linux的native AIO只对文件有效。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;linux一般使用non-blocking IO提高IO并发度。当IO并发度很低时，non-blocking IO不一定比blocking IO更高效，因为后者完全由内核负责，而read/write这类系统调用已高度优化，效率显然高于一般得多个线程协作的non-blocking IO。但当IO并发度愈发提高时，blocking IO阻塞一个线程的弊端便显露出来：内核得不停地在线程间切换才能完成有效的工作，一个cpu core上可能只做了一点点事情，就马上又换成了另一个线程，cpu cache没得到充分利用，另外大量的线程会使得依赖thread-local加速的代码性能明显下降，如tcmalloc，一旦malloc变慢，程序整体性能往往也会随之下降。而non-blocking IO一般由少量event dispatching线程和一些运行用户逻辑的worker线程组成，这些线程往往会被复用（换句话说调度工作转移到了用户态），event dispatching和worker可以同时在不同的核运行（流水线化），内核不用频繁的切换就能完成有效的工作。线程总量也不用很多，所以对thread-local的使用也比较充分。这时候non-blocking IO就往往比blocking IO快了。不过non-blocking IO也有自己的问题，它需要调用更多系统调用，比如&lt;a href=&#34;http://man7.org/linux/man-pages/man2/epoll_ctl.2.html&#34;&gt;epoll_ctl&lt;/a&gt;，由于epoll实现为一棵红黑树，epoll_ctl并不是一个很快的操作，特别在多核环境下，依赖epoll_ctl的实现往往会面临棘手的扩展性问题。non-blocking需要更大的缓冲，否则就会触发更多的事件而影响效率。non-blocking还得解决不少多线程问题，代码比blocking复杂很多。&lt;/p&gt;
&lt;h1 id=&#34;收消息&#34;&gt;收消息&lt;/h1&gt;
&lt;p&gt;“消息”指从连接读入的有边界的二进制串，可能是来自上游client的request或来自下游server的response。brpc使用一个或多个&lt;a href=&#34;https://github.com/brpc/brpc/blob/master/src/brpc/event_dispatcher.h&#34;&gt;EventDispatcher&lt;/a&gt;(简称为EDISP)等待任一fd发生事件。和常见的“IO线程”不同，EDISP不负责读取。IO线程的问题在于一个线程同时只能读一个fd，当多个繁忙的fd聚集在一个IO线程中时，一些读取就被延迟了。多租户、复杂分流算法，&lt;a href=&#34;../../client/streaming-rpc/&#34;&gt;Streaming RPC&lt;/a&gt;等功能会加重这个问题。高负载下常见的某次读取卡顿会拖慢一个IO线程中所有fd的读取，对可用性的影响幅度较大。&lt;/p&gt;
&lt;p&gt;由于epoll的&lt;a href=&#34;https://patchwork.kernel.org/patch/1970231/&#34;&gt;一个bug&lt;/a&gt;(开发brpc时仍有)及epoll_ctl较大的开销，EDISP使用Edge triggered模式。当收到事件时，EDISP给一个原子变量加1，只有当加1前的值是0时启动一个bthread处理对应fd上的数据。在背后，EDISP把所在的pthread让给了新建的bthread，使其有更好的cache locality，可以尽快地读取fd上的数据。而EDISP所在的bthread会被偷到另外一个pthread继续执行，这个过程即是bthread的work stealing调度。要准确理解那个原子变量的工作方式可以先阅读&lt;a href=&#34;../atomic-instructions/&#34;&gt;atomic instructions&lt;/a&gt;，再看&lt;a href=&#34;https://github.com/brpc/brpc/blob/master/src/brpc/socket.cpp&#34;&gt;Socket::StartInputEvent&lt;/a&gt;。这些方法使得brpc读取同一个fd时产生的竞争是&lt;a href=&#34;http://en.wikipedia.org/wiki/Non-blocking_algorithm#Wait-freedom&#34;&gt;wait-free&lt;/a&gt;的。&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/brpc/brpc/blob/master/src/brpc/input_messenger.h&#34;&gt;InputMessenger&lt;/a&gt;负责从fd上切割和处理消息，它通过用户回调函数理解不同的格式。Parse一般是把消息从二进制流上切割下来，运行时间较固定；Process则是进一步解析消息(比如反序列化为protobuf)后调用用户回调，时间不确定。若一次从某个fd读取出n个消息(n &amp;gt; 1)，InputMessenger会启动n-1个bthread分别处理前n-1个消息，最后一个消息则会在原地被Process。InputMessenger会逐一尝试多种协议，由于一个连接上往往只有一种消息格式，InputMessenger会记录下上次的选择，而避免每次都重复尝试。&lt;/p&gt;
&lt;p&gt;可以看到，fd间和fd内的消息都会在brpc中获得并发，这使brpc非常擅长大消息的读取，在高负载时仍能及时处理不同来源的消息，减少长尾的存在。&lt;/p&gt;
&lt;h1 id=&#34;发消息&#34;&gt;发消息&lt;/h1&gt;
&lt;p&gt;&amp;ldquo;消息”指向连接写出的有边界的二进制串，可能是发向上游client的response或下游server的request。多个线程可能会同时向一个fd发送消息，而写fd又是非原子的，所以如何高效率地排队不同线程写出的数据包是这里的关键。brpc使用一种wait-free MPSC链表来实现这个功能。所有待写出的数据都放在一个单链表节点中，next指针初始化为一个特殊值(Socket::WriteRequest::UNCONNECTED)。当一个线程想写出数据前，它先尝试和对应的链表头(Socket::_write_head)做原子交换，返回值是交换前的链表头。如果返回值为空，说明它获得了写出的权利，它会在原地写一次数据。否则说明有另一个线程在写，它把next指针指向返回的头以让链表连通。正在写的线程之后会看到新的头并写出这块数据。&lt;/p&gt;
&lt;p&gt;这套方法可以让写竞争是wait-free的，而获得写权利的线程虽然在原理上不是wait-free也不是lock-free，可能会被一个值仍为UNCONNECTED的节点锁定（这需要发起写的线程正好在原子交换后，在设置next指针前，仅仅一条指令的时间内被OS换出），但在实践中很少出现。在当前的实现中，如果获得写权利的线程一下子无法写出所有的数据，会启动一个KeepWrite线程继续写，直到所有的数据都被写出。这套逻辑非常复杂，大致原理如下图，细节请阅读&lt;a href=&#34;https://github.com/brpc/brpc/blob/master/src/brpc/socket.cpp&#34;&gt;socket.cpp&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://brpc.incubator.apache.org/images/docs/write.png&#34; alt=&#34;img&#34;&gt;&lt;/p&gt;
&lt;p&gt;由于brpc的写出总能很快地返回，调用线程可以更快地处理新任务，后台KeepWrite写线程也能每次拿到一批任务批量写出，在大吞吐时容易形成流水线效应而提高IO效率。&lt;/p&gt;
&lt;h1 id=&#34;socket&#34;&gt;Socket&lt;/h1&gt;
&lt;p&gt;和fd相关的数据均在&lt;a href=&#34;https://github.com/brpc/brpc/blob/master/src/brpc/socket.h&#34;&gt;Socket&lt;/a&gt;中，是rpc最复杂的结构之一，这个结构的独特之处在于用64位的SocketId指代Socket对象以方便在多线程环境下使用fd。常用的三个方法：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Create：创建Socket，并返回其SocketId。&lt;/li&gt;
&lt;li&gt;Address：取得id对应的Socket，包装在一个会自动释放的unique_ptr中(SocketUniquePtr)，当Socket被SetFailed后，返回指针为空。只要Address返回了非空指针，其内容保证不会变化，直到指针自动析构。这个函数是wait-free的。&lt;/li&gt;
&lt;li&gt;SetFailed：标记一个Socket为失败，之后所有对那个SocketId的Address会返回空指针（直到健康检查成功）。当Socket对象没人使用后会被回收。这个函数是lock-free的。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;可以看到Socket类似&lt;a href=&#34;http://en.cppreference.com/w/cpp/memory/shared_ptr&#34;&gt;shared_ptr&lt;/a&gt;，SocketId类似&lt;a href=&#34;http://en.cppreference.com/w/cpp/memory/weak_ptr&#34;&gt;weak_ptr&lt;/a&gt;，但Socket独有的SetFailed可以在需要时确保Socket不能被继续Address而最终引用计数归0，单纯使用shared_ptr/weak_ptr则无法保证这点，当一个server需要退出时，如果请求仍频繁地到来，对应Socket的引用计数可能迟迟无法清0而导致server无法退出。另外weak_ptr无法直接作为epoll的data，而SocketId可以。这些因素使我们设计了Socket，这个类的核心部分自14年完成后很少改动，非常稳定。&lt;/p&gt;
&lt;p&gt;存储SocketUniquePtr还是SocketId取决于是否需要强引用。像Controller贯穿了RPC的整个流程，和Socket中的数据有大量交互，它存放的是SocketUniquePtr。epoll主要是提醒对应fd上发生了事件，如果Socket回收了，那这个事件是可有可无的，所以它存放了SocketId。&lt;/p&gt;
&lt;p&gt;由于SocketUniquePtr只要有效，其中的数据就不会变，这个机制使用户不用关心麻烦的race conditon和ABA problem，可以放心地对共享的fd进行操作。这种方法也规避了隐式的引用计数，内存的ownership明确，程序的质量有很好的保证。brpc中有大量的SocketUniquePtr和SocketId，它们确实简化了我们的开发。&lt;/p&gt;
&lt;p&gt;事实上，Socket不仅仅用于管理原生的fd，它也被用来管理其他资源。比如SelectiveChannel中的每个Sub Channel都被置入了一个Socket中，这样SelectiveChannel可以像普通channel选择下游server那样选择一个Sub Channel进行发送。这个假Socket甚至还实现了健康检查。Streaming RPC也使用了Socket以复用wait-free的写出过程。&lt;/p&gt;
&lt;h1 id=&#34;the-full-picture&#34;&gt;The full picture&lt;/h1&gt;
&lt;p&gt;&lt;img src=&#34;https://brpc.incubator.apache.org/images/docs/rpc_flow.png&#34; alt=&#34;img&#34;&gt;&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: 线程模型简介</title>
      <link>https://brpc.incubator.apache.org/zh/docs/rpc-in-depth/threading-overview/</link>
      <pubDate>Thu, 12 Aug 2021 00:00:00 +0000</pubDate>
      
      <guid>https://brpc.incubator.apache.org/zh/docs/rpc-in-depth/threading-overview/</guid>
      <description>
        
        
        &lt;h1 id=&#34;常见线程模型&#34;&gt;常见线程模型&lt;/h1&gt;
&lt;h2 id=&#34;连接独占线程或进程&#34;&gt;连接独占线程或进程&lt;/h2&gt;
&lt;p&gt;在这个模型中，线程/进程处理来自绑定连接的消息，在连接断开前不退也不做其他事情。当连接数逐渐增多时，线程/进程占用的资源和上下文切换成本会越来越大，性能很差，这就是&lt;a href=&#34;http://en.wikipedia.org/wiki/C10k_problem&#34;&gt;C10K问题&lt;/a&gt;的来源。这种方法常见于早期的web server，现在很少使用。&lt;/p&gt;
&lt;h2 id=&#34;单线程reactorhttpenwikipediaorgwikireactor_pattern&#34;&gt;单线程&lt;a href=&#34;http://en.wikipedia.org/wiki/Reactor_pattern&#34;&gt;reactor&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;以&lt;a href=&#34;http://libevent.org/&#34;&gt;libevent&lt;/a&gt;, &lt;a href=&#34;http://software.schmorp.de/pkg/libev.html&#34;&gt;libev&lt;/a&gt;等event-loop库为典型。这个模型一般由一个event dispatcher等待各类事件，待事件发生后&lt;strong&gt;原地&lt;/strong&gt;调用对应的event handler，全部调用完后等待更多事件，故为&amp;quot;loop&amp;quot;。这个模型的实质是把多段逻辑按事件触发顺序交织在一个系统线程中。一个event-loop只能使用一个核，故此类程序要么是IO-bound，要么是每个handler有确定的较短的运行时间（比如http server)，否则一个耗时漫长的回调就会卡住整个程序，产生高延时。在实践中这类程序不适合多开发者参与，一个人写了阻塞代码可能就会拖慢其他代码的响应。由于event handler不会同时运行，不太会产生复杂的race condition，一些代码不需要锁。此类程序主要靠部署更多进程增加扩展性。&lt;/p&gt;
&lt;p&gt;单线程reactor的运行方式及问题如下图所示：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://brpc.incubator.apache.org/images/docs/threading_overview_1.png&#34; alt=&#34;img&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;n1线程库&#34;&gt;N:1线程库&lt;/h2&gt;
&lt;p&gt;又称为&lt;a href=&#34;http://en.wikipedia.org/wiki/Fiber_(computer_science)&#34;&gt;Fiber&lt;/a&gt;，以&lt;a href=&#34;http://www.gnu.org/software/pth/pth-manual.html&#34;&gt;GNU Pth&lt;/a&gt;, &lt;a href=&#34;http://state-threads.sourceforge.net/index.html&#34;&gt;StateThreads&lt;/a&gt;等为典型，一般是把N个用户线程映射入一个系统线程。同时只运行一个用户线程，调用阻塞函数时才会切换至其他用户线程。N:1线程库与单线程reactor在能力上等价，但事件回调被替换为了上下文(栈,寄存器,signals)，运行回调变成了跳转至上下文。和event loop库一样，单个N:1线程库无法充分发挥多核性能，只适合一些特定的程序。只有一个系统线程对CPU cache较为友好，加上舍弃对signal mask的支持的话，用户线程间的上下文切换可以很快(100~200ns)。N:1线程库的性能一般和event loop库差不多，扩展性也主要靠多进程。&lt;/p&gt;
&lt;h2 id=&#34;多线程reactor&#34;&gt;多线程reactor&lt;/h2&gt;
&lt;p&gt;以&lt;a href=&#34;http://www.boost.org/doc/libs/1_56_0/doc/html/boost_asio.html&#34;&gt;boost::asio&lt;/a&gt;为典型。一般由一个或多个线程分别运行event dispatcher，待事件发生后把event handler交给一个worker线程执行。 这个模型是单线程reactor的自然扩展，可以利用多核。由于共用地址空间使得线程间交互变得廉价，worker thread间一般会更及时地均衡负载，而多进程一般依赖更前端的服务来分割流量，一个设计良好的多线程reactor程序往往能比同一台机器上的多个单线程reactor进程更均匀地使用不同核心。不过由于&lt;a href=&#34;../atomic-instructions/#cacheline&#34;&gt;cache一致性&lt;/a&gt;的限制，多线程reactor并不能获得线性于核心数的性能，在特定的场景中，粗糙的多线程reactor实现跑在24核上甚至没有精致的单线程reactor实现跑在1个核上快。由于多线程reactor包含多个worker线程，单个event handler阻塞未必会延缓其他handler，所以event handler未必得非阻塞，除非所有的worker线程都被阻塞才会影响到整体进展。事实上，大部分RPC框架都使用了这个模型，且回调中常有阻塞部分，比如同步等待访问下游的RPC返回。&lt;/p&gt;
&lt;p&gt;多线程reactor的运行方式及问题如下：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://brpc.incubator.apache.org/images/docs/threading_overview_2.png&#34; alt=&#34;img&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;mn线程库&#34;&gt;M:N线程库&lt;/h2&gt;
&lt;p&gt;即把M个用户线程映射入N个系统线程。M:N线程库可以决定一段代码何时开始在哪运行，并何时结束，相比多线程reactor在调度上具备更多的灵活度。但实现全功能的M:N线程库是困难的，它一直是个活跃的研究话题。我们这里说的M:N线程库特别针对编写网络服务，在这一前提下一些需求可以简化，比如没有时间片抢占，没有(完备的)优先级等。M:N线程库可以在用户态也可以在内核中实现，用户态的实现以新语言为主，比如GHC threads和goroutine，这些语言可以围绕线程库设计全新的关键字并拦截所有相关的API。而在现有语言中的实现往往得修改内核，比如&lt;a href=&#34;https://msdn.microsoft.com/en-us/library/windows/desktop/dd627187(v=vs.85).aspx&#34;&gt;Windows UMS&lt;/a&gt;和google SwitchTo(虽然是1:1，但基于它可以实现M:N的效果)。相比N:1线程库，M:N线程库在使用上更类似于系统线程，需要用锁或消息传递保证代码的线程安全。&lt;/p&gt;
&lt;h1 id=&#34;问题&#34;&gt;问题&lt;/h1&gt;
&lt;h2 id=&#34;多核扩展性&#34;&gt;多核扩展性&lt;/h2&gt;
&lt;p&gt;理论上代码都写成事件驱动型能最大化reactor模型的能力，但实际由于编码难度和可维护性，用户的使用方式大都是混合的：回调中往往会发起同步操作，阻塞住worker线程使其无法处理其他请求。一个请求往往要经过几十个服务，线程把大量时间花在了等待下游请求上，用户得开几百个线程以维持足够的吞吐，这造成了高强度的调度开销，并降低了TLS相关代码的效率。任务的分发大都是使用全局mutex + condition保护的队列，当所有线程都在争抢时，效率显然好不到哪去。更好的办法也许是使用更多的任务队列，并调整调度算法以减少全局竞争。比如每个系统线程有独立的runqueue，由一个或多个scheduler把用户线程分发到不同的runqueue，每个系统线程优先运行自己runqueue中的用户线程，然后再考虑其他线程的runqueue。这当然更复杂，但比全局mutex + condition有更好的扩展性。这种结构也更容易支持NUMA。&lt;/p&gt;
&lt;p&gt;当event dispatcher把任务递给worker线程时，用户逻辑很可能从一个核心跳到另一个核心，并等待相应的cacheline同步过来，并不很快。如果worker的逻辑能直接运行于event dispatcher所在的核心上就好了，因为大部分时候尽快运行worker的优先级高于获取新事件。类似的是收到response后最好在当前核心唤醒正在同步等待RPC的线程。&lt;/p&gt;
&lt;h2 id=&#34;异步编程&#34;&gt;异步编程&lt;/h2&gt;
&lt;p&gt;异步编程中的流程控制对于专家也充满了陷阱。任何挂起操作，如sleep一会儿或等待某事完成，都意味着用户需要显式地保存状态，并在回调函数中恢复状态。异步代码往往得写成状态机的形式。当挂起较少时，这有点麻烦，但还是可把握的。问题在于一旦挂起发生在条件判断、循环、子函数中，写出这样的状态机并能被很多人理解和维护，几乎是不可能的，而这在分布式系统中又很常见，因为一个节点往往要与多个节点同时交互。另外如果唤醒可由多种事件触发（比如fd有数据或超时了），挂起和恢复的过程容易出现race condition，对多线程编码能力要求很高。语法糖(比如lambda)可以让编码不那么“麻烦”，但无法降低难度。&lt;/p&gt;
&lt;p&gt;共享指针在异步编程中很普遍，这看似方便，但也使内存的ownership变得难以捉摸，如果内存泄漏了，很难定位哪里没有释放；如果segment fault了，也不知道哪里多释放了一下。大量使用引用计数的用户代码很难控制代码质量，容易长期在内存问题上耗费时间。如果引用计数还需要手动维护，保持质量就更难了，维护者也不会愿意改进。没有上下文会使得&lt;a href=&#34;http://en.wikipedia.org/wiki/Resource_Acquisition_Is_Initialization&#34;&gt;RAII&lt;/a&gt;无法充分发挥作用, 有时需要在callback之外lock，callback之内unlock，实践中很容易出错。&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: 负载均衡</title>
      <link>https://brpc.incubator.apache.org/zh/docs/rpc-in-depth/load-balancing/</link>
      <pubDate>Thu, 12 Aug 2021 00:00:00 +0000</pubDate>
      
      <guid>https://brpc.incubator.apache.org/zh/docs/rpc-in-depth/load-balancing/</guid>
      <description>
        
        
        &lt;p&gt;上游一般通过命名服务发现所有的下游节点，并通过多种负载均衡方法把流量分配给下游节点。当下游节点出现问题时，它可能会被隔离以提高负载均衡的效率。被隔离的节点定期被健康检查，成功后重新加入正常节点。&lt;/p&gt;
&lt;h1 id=&#34;命名服务&#34;&gt;命名服务&lt;/h1&gt;
&lt;p&gt;在brpc中，&lt;a href=&#34;https://github.com/brpc/brpc/blob/master/src/brpc/naming_service.h&#34;&gt;NamingService&lt;/a&gt;用于获得服务名对应的所有节点。一个直观的做法是定期调用一个函数以获取最新的节点列表。但这会带来一定的延时（定期调用的周期一般在若干秒左右），作为通用接口不太合适。特别当命名服务提供事件通知时(比如zk)，这个特性没有被利用。所以我们反转了控制权：不是我们调用用户函数，而是用户在获得列表后调用我们的接口，对应&lt;a href=&#34;https://github.com/brpc/brpc/blob/master/src/brpc/naming_service.h&#34;&gt;NamingServiceActions&lt;/a&gt;。当然我们还是得启动进行这一过程的函数，对应NamingService::RunNamingService。下面以三个实现解释这套方式：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;bns：没有事件通知，所以我们只能定期去获得最新列表，默认间隔是&lt;a href=&#34;http://brpc.baidu.com:8765/flags/ns_access_interval&#34;&gt;5秒&lt;/a&gt;。为了简化这类定期获取的逻辑，brpc提供了&lt;a href=&#34;https://github.com/brpc/brpc/blob/master/src/brpc/periodic_naming_service.h&#34;&gt;PeriodicNamingService&lt;/a&gt; 供用户继承，用户只需要实现单次如何获取（GetServers）。获取后调用NamingServiceActions::ResetServers告诉框架。框架会对列表去重，和之前的列表比较，通知对列表有兴趣的观察者(NamingServiceWatcher)。这套逻辑会运行在独立的bthread中，即NamingServiceThread。一个NamingServiceThread可能被多个Channel共享，通过intrusive_ptr管理ownership。&lt;/li&gt;
&lt;li&gt;file：列表即文件。合理的方式是在文件更新后重新读取。&lt;a href=&#34;https://github.com/brpc/brpc/blob/master/src/brpc/policy/file_naming_service.cpp&#34;&gt;该实现&lt;/a&gt;使用&lt;a href=&#34;https://github.com/brpc/brpc/blob/master/src/butil/files/file_watcher.h&#34;&gt;FileWatcher&lt;/a&gt;关注文件的修改时间，当文件修改后，读取并调用NamingServiceActions::ResetServers告诉框架。&lt;/li&gt;
&lt;li&gt;list：列表就在服务名里（逗号分隔）。在读取完一次并调用NamingServiceActions::ResetServers后就退出了，因为列表再不会改变了。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;如果用户需要建立这些对象仍然是不够方便的，因为总是需要一些工厂代码根据配置项建立不同的对象，鉴于此，我们把工厂类做进了框架，并且是非常方便的形式：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;&amp;quot;protocol://service-name&amp;quot;
 
e.g.
bns://&amp;lt;node-name&amp;gt;            # baidu naming service
file://&amp;lt;file-path&amp;gt;           # load addresses from the file
list://addr1,addr2,...       # use the addresses separated by comma
http://&amp;lt;url&amp;gt;                 # Domain Naming Service, aka DNS.
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;这套方式是可扩展的，实现了新的NamingService后在&lt;a href=&#34;https://github.com/brpc/brpc/blob/master/src/brpc/global.cpp&#34;&gt;global.cpp&lt;/a&gt;中依葫芦画瓢注册下就行了，如下图所示：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://brpc.incubator.apache.org/images/docs/register_ns.png&#34; alt=&#34;img&#34;&gt;&lt;/p&gt;
&lt;p&gt;看到这些熟悉的字符串格式，容易联想到ftp:// zk:// galileo://等等都是可以支持的。用户在新建Channel时传入这类NamingService描述，并能把这些描述写在各类配置文件中。&lt;/p&gt;
&lt;h1 id=&#34;负载均衡&#34;&gt;负载均衡&lt;/h1&gt;
&lt;p&gt;brpc中&lt;a href=&#34;https://github.com/brpc/brpc/blob/master/src/brpc/load_balancer.h&#34;&gt;LoadBalancer&lt;/a&gt;从多个服务节点中选择一个节点，目前的实现见&lt;a href=&#34;client.md#%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1&#34;&gt;负载均衡&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;Load balancer最重要的是如何让不同线程中的负载均衡不互斥，解决这个问题的技术是&lt;a href=&#34;../locality-aware/#doublybuffereddata&#34;&gt;DoublyBufferedData&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;和NamingService类似，我们使用字符串来指代一个load balancer，在global.cpp中注册：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://brpc.incubator.apache.org/images/docs/register_lb.png&#34; alt=&#34;img&#34;&gt;&lt;/p&gt;
&lt;h1 id=&#34;健康检查&#34;&gt;健康检查&lt;/h1&gt;
&lt;p&gt;对于那些无法连接却仍在NamingService的节点，brpc会定期连接它们，成功后对应的Socket将被”复活“，并可能被LoadBalancer选择上，这个过程就是健康检查。注意：被健康检查或在LoadBalancer中的节点一定在NamingService中。换句话说，只要一个节点不从NamingService删除，它要么是正常的（会被LoadBalancer选上），要么在做健康检查。&lt;/p&gt;
&lt;p&gt;传统的做法是使用一个线程做所有连接的健康检查，brpc简化了这个过程：为需要的连接动态创建一个bthread专门做健康检查（Socket::HealthCheckThread）。这个线程的生命周期被对应连接管理。具体来说，当Socket被SetFailed后，健康检查线程就可能启动（如果SocketOptions.health_check_interval为正数的话）：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;健康检查线程先在确保没有其他人在使用Socket了后关闭连接。目前是通过对Socket的引用计数判断的。这个方法之所以有效在于Socket被SetFailed后就不能被Address了，所以引用计数只减不增。&lt;/li&gt;
&lt;li&gt;定期连接直到远端机器被连接上，在这个过程中，如果Socket析构了，那么该线程也就随之退出了。&lt;/li&gt;
&lt;li&gt;连上后复活Socket(Socket::Revive)，这样Socket就又能被其他地方，包括LoadBalancer访问到了（通过Socket::Address）。&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: Locality-aware</title>
      <link>https://brpc.incubator.apache.org/zh/docs/rpc-in-depth/locality-aware/</link>
      <pubDate>Thu, 12 Aug 2021 00:00:00 +0000</pubDate>
      
      <guid>https://brpc.incubator.apache.org/zh/docs/rpc-in-depth/locality-aware/</guid>
      <description>
        
        
        &lt;h1 id=&#34;概述&#34;&gt;概述&lt;/h1&gt;
&lt;p&gt;LALB全称Locality-aware load balancing，是一个能把请求及时、自动地送到延时最低的下游的负载均衡算法，特别适合混合部署环境。该算法产生自DP系统，现已加入brpc！&lt;/p&gt;
&lt;p&gt;LALB可以解决的问题：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;下游的机器配置不同，访问延时不同，round-robin和随机分流效果不佳。&lt;/li&gt;
&lt;li&gt;下游服务和离线服务或其他服务混部，性能难以预测。&lt;/li&gt;
&lt;li&gt;自动地把大部分流量送给同机部署的模块，当同机模块出问题时，再跨机器。&lt;/li&gt;
&lt;li&gt;优先访问本机房服务，出问题时再跨机房。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;…&lt;/strong&gt;&lt;/p&gt;
&lt;h1 id=&#34;背景&#34;&gt;背景&lt;/h1&gt;
&lt;p&gt;最常见的分流算法是round robin和随机。这两个方法的前提是下游的机器和网络都是类似的，但在目前的线上环境下，特别是混部的产品线中，已经很难成立，因为：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;每台机器运行着不同的程序组合，并伴随着一些离线任务，机器的可用资源在持续动态地变化着。&lt;/li&gt;
&lt;li&gt;机器配置不同。&lt;/li&gt;
&lt;li&gt;网络延时不同。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;这些问题其实一直有，但往往被OP辛勤的机器监控和替换给隐藏了。框架层面也有过一些努力，比如UB中的&lt;a href=&#34;https://svn.baidu.com/public/trunk/ub/ub_client/ubclient_weightstrategy.h&#34;&gt;WeightedStrategy&lt;/a&gt;是根据下游的cpu占用率来进行分流，但明显地它解决不了延时相关的问题，甚至cpu的问题也解决不了：因为它被实现为定期reload一个权值列表，可想而知更新频率高不了，等到负载均衡反应过来，一大堆请求可能都超时了。并且这儿有个数学问题：怎么把cpu占用率转为权值。假设下游差异仅仅由同机运行的其他程序导致，机器配置和网络完全相同，两台机器权值之比是cpu idle之比吗？假如是的，当我们以这个比例给两台机器分流之后，它们的cpu idle应该会更接近对吧？而这会导致我们的分流比例也变得接近，从而使两台机器的cpu idle又出现差距。你注意到这个悖论了吗？这些因素使得这类算法的实际效果和那两个基本算法没什么差距，甚至更差，用者甚少。&lt;/p&gt;
&lt;p&gt;我们需要一个能自适应下游负载、规避慢节点的通用分流算法。&lt;/p&gt;
&lt;h1 id=&#34;locality-aware&#34;&gt;Locality-aware&lt;/h1&gt;
&lt;p&gt;在DP 2.0中我们使用了一种新的算法: Locality-aware load balancing，能根据下游节点的负载分配流量，还能快速规避失效的节点，在很大程度上，这种算法的延时也是全局最优的。基本原理非常简单：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;以下游节点的吞吐除以延时作为分流权值。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;比如只有两台下游节点，W代表权值，QPS代表吞吐，L代表延时，那么W1 = QPS1 / L1和W2 = QPS2 / L2分别是这两个节点的分流权值，分流时随机数落入的权值区间就是流量的目的地了。&lt;/p&gt;
&lt;p&gt;一种分析方法如下：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;稳定状态时的QPS显然和其分流权值W成正比，即W1 / W2 ≈ QPS1 / QPS2。&lt;/li&gt;
&lt;li&gt;根据分流公式又有：W1 / W2 = QPS1 / QPS2 * (L2 / L1)。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;故稳定状态时L1和L2应当是趋同的。当L1小于L2时，节点1会更获得相比其QPS1更大的W1，从而在未来获得更多的流量，直到&lt;strong&gt;其延时高于平均值或没有更多的流量。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;注意这个算法并不是按照延时的比例来分流，不是说一个下游30ms，另一个60ms，它们的流量比例就是60 / 30。而是30ms的节点会一直获得流量直到它的延时高于60ms，或者没有更多流量了。以下图为例，曲线1和曲线2分别是节点1和节点2的延时与吞吐关系图，随着吞吐增大延时会逐渐升高，接近极限吞吐时，延时会飙升。左下的虚线标记了QPS=400时的延时，此时虽然节点1的延时有所上升，但还未高于节点2的基本延时（QPS=0时的延时），所以所有流量都会分给节点1，而不是按它们基本延时的比例（图中大约2:1）。当QPS继续上升达到1600时，分流比例会在两个节点延时相等时平衡，图中为9 : 7。很明显这个比例是高度非线性的，取决于不同曲线的组合，和单一指标的比例关系没有直接关联。在真实系统中，延时和吞吐的曲线也在动态变化着，分流比例更加动态。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://brpc.incubator.apache.org/images/docs/lalb_1.png&#34; alt=&#34;img&#34;&gt;&lt;/p&gt;
&lt;p&gt;我们用一个例子来看一下具体的分流过程。启动3台server，逻辑分别是sleep 1ms，2ms，3ms，对于client来说这些值就是延时。启动client（50个同步访问线程）后每秒打印的分流结果如下：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://brpc.incubator.apache.org/images/docs/lalb_2.png&#34; alt=&#34;img&#34;&gt;&lt;/p&gt;
&lt;p&gt;S[n]代表第n台server。由于S[1]和S[2]的平均延时大于1ms，LALB会发现这点并降低它们的权值。它们的权值会继续下降，直到被算法设定的最低值拦住。这时停掉server，反转延时并重新启动，即逻辑分别为sleep 3ms，2ms，1ms，运行一段时候后分流效果如下：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://brpc.incubator.apache.org/images/docs/lalb_3.png&#34; alt=&#34;img&#34;&gt;&lt;/p&gt;
&lt;p&gt;刚重连上server时，client还是按之前的权值把大部分流量都分给了S[0]，但由于S[0]的延时从1ms上升到了3ms，client的qps也降到了原来的1/3。随着数据积累，LALB逐渐发现S[2]才是最快的，而把大部分流量切换了过去。同样的服务如果用rr或random访问，则qps会显著下降：&lt;/p&gt;
&lt;p&gt;&amp;ldquo;rr&amp;rdquo; or &amp;ldquo;random&amp;rdquo;: &lt;img src=&#34;https://brpc.incubator.apache.org/images/docs/lalb_4.png&#34; alt=&#34;img&#34;&gt;&lt;/p&gt;
&lt;p&gt;&amp;ldquo;la&amp;rdquo; :                       &lt;img src=&#34;https://brpc.incubator.apache.org/images/docs/lalb_5.png&#34; alt=&#34;img&#34;&gt;&lt;/p&gt;
&lt;p&gt;真实的场景中不会有这么显著的差异，但你应该能看到差别了。&lt;/p&gt;
&lt;p&gt;这有很多应用场景：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;如果本机有下游服务，LALB会优先访问这些最近的节点。比如CTR应用中有一个计算在1ms左右的receiver模块，被model模块访问，很多model和receiver是同机部署的，以前的分流算法必须走网络，使得receiver的延时开销较大(3-5ms)，特别是在晚上由于离线任务起来，很不稳定，失败率偏高，而LALB会优先访问本机或最近的receiver模块，很多流量都不走网络了，成功率一下子提升了很多。&lt;/li&gt;
&lt;li&gt;如果同rack有下游服务，LALB也会优先访问，减少机房核心路由器的压力。甚至不同机房的服务可能不再需要隔离，LALB会优先走本机房的下游，当本机房下游出问题时再自动访问另一些机房。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;但我们也不能仅看到“基本原理”，这个算法有它复杂的一面：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;传统的经验告诉我们，不能把所有鸡蛋放一个篮子里，而按延时优化不可避免地会把很多流量送到同一个节点，如果这个节点出问题了，我们如何尽快知道并绕开它。&lt;/li&gt;
&lt;li&gt;对吞吐和延时的统计都需要统计窗口，窗口越大数据越可信，噪声越少，但反应也慢了，一个异常的请求可能对统计值造不成什么影响，等我们看到统计值有显著变化时可能已经太晚了。&lt;/li&gt;
&lt;li&gt;我们也不能只统计已经回来的，还得盯着路上的请求，否则我们可能会向一个已经出问题（总是不回）的节点傻傻地浪费请求。&lt;/li&gt;
&lt;li&gt;”按权值分流”听上去好简单，但你能写出多线程和可能修改节点的前提下，在O(logN)时间内尽量不互斥的查找算法吗？&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;这些问题可以归纳为以下几个方面。&lt;/p&gt;
&lt;h2 id=&#34;doublybuffereddata&#34;&gt;DoublyBufferedData&lt;/h2&gt;
&lt;p&gt;LoadBalancer是一个读远多于写的数据结构：大部分时候，所有线程从一个不变的server列表中选取一台server。如果server列表真是“不变的”，那么选取server的过程就不用加锁，我们可以写更复杂的分流算法。一个方法是用读写锁，但当读临界区不是特别大时（毫秒级），读写锁并不比mutex快，而实用的分流算法不可能到毫秒级，否则开销也太大了。另一个方法是双缓冲，很多检索端用类似的方法实现无锁的查找过程，它大概这么工作：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;数据分前台和后台。&lt;/li&gt;
&lt;li&gt;检索线程只读前台，不用加锁。&lt;/li&gt;
&lt;li&gt;只有一个写线程：修改后台数据，切换前后台，睡眠一段时间，以确保老前台（新后台）不再被检索线程访问。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;这个方法的问题在于它假定睡眠一段时间后就能避免和前台读线程发生竞争，这个时间一般是若干秒。由于多次写之间有间隔，这儿的写往往是批量写入，睡眠时正好用于积累数据增量。&lt;/p&gt;
&lt;p&gt;但这套机制对“server列表”不太好用：总不能插入一个server就得等几秒钟才能插入下一个吧，即使我们用批量插入，这个&amp;quot;冷却&amp;quot;间隔多少会让用户觉得疑惑：短了担心安全性，长了觉得没有必要。我们能尽量降低这个时间并使其安全么？&lt;/p&gt;
&lt;p&gt;我们需要&lt;strong&gt;写以某种形式和读同步，但读之间相互没竞争&lt;/strong&gt;。一种解法是，读拿一把thread-local锁，写需要拿到所有的thread-local锁。具体过程如下：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;数据分前台和后台。&lt;/li&gt;
&lt;li&gt;读拿到自己所在线程的thread-local锁，执行查询逻辑后释放锁。&lt;/li&gt;
&lt;li&gt;同时只有一个写：修改后台数据，切换前后台，&lt;strong&gt;挨个&lt;/strong&gt;获得所有thread-local锁并立刻释放，结束后再改一遍新后台（老前台）。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;我们来分析下这个方法的基本原理：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;当一个读正在发生时，它会拿着所在线程的thread-local锁，这把锁会挡住同时进行的写，从而保证前台数据不会被修改。&lt;/li&gt;
&lt;li&gt;在大部分时候thread-local锁都没有竞争，对性能影响很小。&lt;/li&gt;
&lt;li&gt;逐个获取thread-local锁并立刻释放是为了&lt;strong&gt;确保对应的读线程看到了切换后的新前台&lt;/strong&gt;。如果所有的读线程都看到了新前台，写线程便可以安全地修改老前台（新后台）了。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;其他特点：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;不同的读之间没有竞争，高度并发。&lt;/li&gt;
&lt;li&gt;如果没有写，读总是能无竞争地获取和释放thread-local锁，一般小于25ns，对延时基本无影响。如果有写，由于其临界区极小（拿到立刻释放），读在大部分时候仍能快速地获得锁，少数时候释放锁时可能有唤醒写线程的代价。由于写本身就是少数情况，读整体上几乎不会碰到竞争锁。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;完成这些功能的数据结构是&lt;a href=&#34;https://github.com/brpc/brpc/blob/master/src/butil/containers/doubly_buffered_data.h&#34;&gt;DoublyBufferedData&amp;lt;&amp;gt;&lt;/a&gt;，我们常简称为DBD。brpc中的所有load balancer都使用了这个数据结构，使不同线程在分流时几乎不会互斥。而其他rpc实现往往使用了全局锁，这使得它们无法写出复杂的分流算法：否则分流代码将会成为竞争热点。&lt;/p&gt;
&lt;p&gt;这个结构有广泛的应用场景：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;reload词典。大部分时候词典都是只读的，不同线程同时查询时不应互斥。&lt;/li&gt;
&lt;li&gt;可替换的全局callback。像butil/logging.cpp支持配置全局LogSink以重定向日志，这个LogSink就是一个带状态的callback。如果只是简单的全局变量，在替换后我们无法直接删除LogSink，因为可能还有都写线程在用。用DBD可以解决这个问题。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;weight-tree&#34;&gt;weight tree&lt;/h2&gt;
&lt;p&gt;LALB的查找过程是按权值分流，O(N)方法如下：获得所有权值的和total，产生一个间于[0, total-1]的随机数R，逐个遍历权值，直到当前权值之和不大于R，而下一个权值之和大于R。&lt;/p&gt;
&lt;p&gt;这个方法可以工作，也好理解，但当N达到几百时性能已经很差，这儿的主要因素是cache一致性：LALB是一个基于反馈的算法，RPC结束时信息会被反馈入LALB，被遍历的数据结构也一直在被修改。这意味着前台的O(N)读必须刷新每一行cacheline。当N达到数百时，一次查找过程可能会耗时百微秒，更别提更大的N了，LALB（将）作为brpc的默认分流算法，这个性能开销是无法接受的。&lt;/p&gt;
&lt;p&gt;另一个办法是用完全二叉树。每个节点记录了左子树的权值之和，这样我们就能在O(logN)时间内完成查找。当N为1024时，我们最多跳转10次内存，总耗时可控制在1微秒内，这个性能是可接受的。这个方法的难点是如何和DoublyBufferedData结合。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;我们不考虑不使用DoublyBufferedData，那样要么绕不开锁，要么写不出正确的算法。&lt;/li&gt;
&lt;li&gt;前台后必须共享权值数据，否则切换前后台时，前台积累的权值数据没法同步到后台。&lt;/li&gt;
&lt;li&gt;“左子树权值之和”也被前后台共享，但和权值数据不同，它和位置绑定。比如权值结构的指针可能从位置10移动到位置5，但“左子树权值之和”的指针不会移动，算法需要从原位置减掉差值，而向新位置加上差值。&lt;/li&gt;
&lt;li&gt;我们不追求一致性，只要最终一致即可，这能让我们少加锁。这也意味着“权值之和”，“左子树权值之和”，“节点权值”未必能精确吻合，查找算法要能适应这一点。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;最困难的部分是增加和删除节点，它们需要在整体上对前台查找不造成什么影响，详细过程请参考代码。&lt;/p&gt;
&lt;h2 id=&#34;base_weight&#34;&gt;base_weight&lt;/h2&gt;
&lt;p&gt;QPS和latency使用一个循环队列统计，默认容量128。我们可以使用这么小的统计窗口，是因为inflight delay能及时纠正过度反应，而128也具备了一定的统计可信度。不过，这么计算latency的缺点是：如果server的性能出现很大的变化，那么我们需要积累一段时间才能看到平均延时的变化。就像上节例子中那样，server反转延时后client需要积累很多秒的数据才能看到的平均延时的变化。目前我们并么有处理这个问题，因为真实生产环境中的server不太会像例子中那样跳变延时，大都是缓缓变慢。当集群有几百台机器时，即使我们反应慢点给个别机器少分流点也不会导致什么问题。如果在产品线中确实出现了性能跳变，并且集群规模不大，我们再处理这个问题。&lt;/p&gt;
&lt;p&gt;权值的计算方法是base_weight = QPS * WEIGHT_SCALE / latency ^ p。其中WEIGHT_SCALE是一个放大系数，为了能用整数存储权值，又能让权值有足够的精度，类似定点数。p默认为2，延时的收敛速度大约为p=1时的p倍，选项quadratic_latency=false可使p=1。&lt;/p&gt;
&lt;p&gt;权值计算在各个环节都有最小值限制，为了防止某个节点的权值过低而使其完全没有访问机会。即使一些延时远大于平均延时的节点，也应该有足够的权值，以确保它们可以被定期访问，否则即使它们变快了，我们也不会知道。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;除了待删除节点，所有节点的权值绝对不会为0。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;这也制造了一个问题：即使一个server非常缓慢（但没有断开连接），它的权值也不会为0，所以总会有一些请求被定期送过去而铁定超时。当qps不高时，为了降低影响面，探测间隔必须拉长。比如为了把对qps=1000的影响控制在1%%内，故障server的权值必须低至使其探测间隔为10秒以上，这降低了我们发现server变快的速度。这个问题的解决方法有：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;什么都不干。这个问题也许没有想象中那么严重，由于持续的资源监控，线上服务很少出现“非常缓慢”的情况，一般性的变慢并不会导致请求超时。&lt;/li&gt;
&lt;li&gt;保存一些曾经发向缓慢server的请求，用这些请求探测。这个办法的好处是不浪费请求。但实现起来耦合很多，比较麻烦。&lt;/li&gt;
&lt;li&gt;强制backup request。&lt;/li&gt;
&lt;li&gt;再选一次。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;inflight-delay&#34;&gt;inflight delay&lt;/h2&gt;
&lt;p&gt;我们必须追踪还未结束的RPC，否则我们就必须等待到超时或其他错误发生，而这可能会很慢（超时一般会是正常延时的若干倍），在这段时间内我们可能做出了很多错误的分流。最简单的方法是统计未结束RPC的耗时：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;选择server时累加发出时间和未结束次数。&lt;/li&gt;
&lt;li&gt;反馈时扣除发出时间和未结束次数。&lt;/li&gt;
&lt;li&gt;框架保证每个选择总对应一次反馈。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;这样“当前时间 - 发出时间之和 / 未结束次数”便是未结束RPC的平均耗时，我们称之为inflight delay。当inflight delay大于平均延时时，我们就线性地惩罚节点权值，即weight = base_weight * avg_latency / inflight_delay。当发向一个节点的请求没有在平均延时内回来时，它的权值就会很快下降，从而纠正我们的行为，这比等待超时快多了。不过这没有考虑延时的正常抖动，我们还得有方差，方差可以来自统计，也可简单线性于平均延时。不管怎样，有了方差bound后，当inflight delay &amp;gt; avg_latency + max(bound * 3, MIN_BOUND)时才会惩罚权值。3是正态分布中的经验数值。&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: 一致性哈希</title>
      <link>https://brpc.incubator.apache.org/zh/docs/rpc-in-depth/consistent-hashing/</link>
      <pubDate>Thu, 12 Aug 2021 00:00:00 +0000</pubDate>
      
      <guid>https://brpc.incubator.apache.org/zh/docs/rpc-in-depth/consistent-hashing/</guid>
      <description>
        
        
        &lt;h1 id=&#34;概述&#34;&gt;概述&lt;/h1&gt;
&lt;p&gt;一些场景希望同样的请求尽量落到一台机器上，比如访问缓存集群时，我们往往希望同一种请求能落到同一个后端上，以充分利用其上已有的缓存，不同的机器承载不同的稳定working set。而不是随机地散落到所有机器上，那样的话会迫使所有机器缓存所有的内容，最终由于存不下形成颠簸而表现糟糕。 我们都知道hash能满足这个要求，比如当有n台服务器时，输入x总是会发送到第hash(x) % n台服务器上。但当服务器变为m台时，hash(x) % n和hash(x) % m很可能都不相等，这会使得几乎所有请求的发送目的地都发生变化，如果目的地是缓存服务，所有缓存将失效，继而对原本被缓存遮挡的数据库或计算服务造成请求风暴，触发雪崩。一致性哈希是一种特殊的哈希算法，在增加服务器时，发向每个老节点的请求中只会有一部分转向新节点，从而实现平滑的迁移。&lt;a href=&#34;http://blog.phpdr.net/wp-content/uploads/2012/08/Consistent-Hashing-and-Random-Trees.pdf&#34;&gt;这篇论文&lt;/a&gt;中提出了一致性hash的概念。&lt;/p&gt;
&lt;p&gt;一致性hash满足以下四个性质：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;平衡性 (Balance) : 每个节点被选到的概率是O(1/n)。&lt;/li&gt;
&lt;li&gt;单调性 (Monotonicity) : 当新节点加入时， 不会有请求在老节点间移动， 只会从老节点移动到新节点。当有节点被删除时，也不会影响落在别的节点上的请求。&lt;/li&gt;
&lt;li&gt;分散性 (Spread) : 当上游的机器看到不同的下游列表时(在上线时及不稳定的网络中比较常见),  同一个请求尽量映射到少量的节点中。&lt;/li&gt;
&lt;li&gt;负载 (Load) : 当上游的机器看到不同的下游列表的时候， 保证每台下游分到的请求数量尽量一致。&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;实现方式&#34;&gt;实现方式&lt;/h1&gt;
&lt;p&gt;所有server的32位hash值在32位整数值域上构成一个环(Hash Ring)，环上的每个区间和一个server唯一对应，如果一个key落在某个区间内， 它就被分流到对应的server上。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://brpc.incubator.apache.org/images/docs/chash.png&#34; alt=&#34;img&#34;&gt;&lt;/p&gt;
&lt;p&gt;当删除一个server的，它对应的区间会归属于相邻的server，所有的请求都会跑过去。当增加一个server时，它会分割某个server的区间并承载落在这个区间上的所有请求。单纯使用Hash Ring很难满足我们上节提到的属性，主要两个问题：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;在机器数量较少的时候， 区间大小会不平衡。&lt;/li&gt;
&lt;li&gt;当一台机器故障的时候， 它的压力会完全转移到另外一台机器， 可能无法承载。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;为了解决这个问题，我们为每个server计算m个hash值，从而把32位整数值域划分为n*m个区间，当key落到某个区间时，分流到对应的server上。那些额外的hash值使得区间划分更加均匀，被称为虚拟节点（Virtual Node）。当删除一个server时，它对应的m个区间会分别合入相邻的区间中，那个server上的请求会较为平均地转移到其他server上。当增加server时，它会分割m个现有区间，从对应server上分别转移一些请求过来。&lt;/p&gt;
&lt;p&gt;由于节点故障和变化不常发生，我们选择了修改复杂度为O(n)的有序数组来存储hash ring，每次分流使用二分查找来选择对应的机器，由于存储是连续的，查找效率比基于平衡二叉树的实现高。线程安全性请参照&lt;a href=&#34;../locality-aware/#doublybuffereddata&#34;&gt;Double Buffered Data&lt;/a&gt;章节.&lt;/p&gt;
&lt;h1 id=&#34;使用方式&#34;&gt;使用方式&lt;/h1&gt;
&lt;p&gt;我们内置了分别基于murmurhash3和md5两种hash算法的实现，使用要做两件事：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;在Channel.Init 时指定&lt;em&gt;load_balancer_name&lt;/em&gt;为 &amp;ldquo;c_murmurhash&amp;rdquo; 或 &amp;ldquo;c_md5&amp;rdquo;。&lt;/li&gt;
&lt;li&gt;发起rpc时通过Controller::set_request_code(uint64_t)填入请求的hash code。&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;request的hash算法并不需要和lb的hash算法保持一致，只需要hash的值域是32位无符号整数。由于memcache默认使用md5，访问memcached集群时请选择c_md5保证兼容性，其他场景可以选择c_murmurhash以获得更高的性能和更均匀的分布。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h1 id=&#34;虚拟节点个数&#34;&gt;虚拟节点个数&lt;/h1&gt;
&lt;p&gt;通过-chash_num_replicas可设置默认的虚拟节点个数，默认值为100。对于某些特殊场合，对虚拟节点个数有自定义的需求，可以通过将&lt;em&gt;load_balancer_name&lt;/em&gt;加上参数replicas=&lt;num&gt;配置，如：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-c++&#34; data-lang=&#34;c++&#34;&gt;&lt;span style=&#34;color:#000&#34;&gt;channel&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;.&lt;/span&gt;&lt;span style=&#34;color:#000&#34;&gt;Init&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;(&lt;/span&gt;&lt;span style=&#34;color:#4e9a06&#34;&gt;&amp;#34;http://...&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;,&lt;/span&gt; &lt;span style=&#34;color:#4e9a06&#34;&gt;&amp;#34;c_murmurhash:replicas=150&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;,&lt;/span&gt; &lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;&amp;amp;&lt;/span&gt;&lt;span style=&#34;color:#000&#34;&gt;options&lt;/span&gt;&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;);&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
      </description>
    </item>
    
    <item>
      <title>Docs: 内存管理</title>
      <link>https://brpc.incubator.apache.org/zh/docs/rpc-in-depth/memory-management/</link>
      <pubDate>Thu, 12 Aug 2021 00:00:00 +0000</pubDate>
      
      <guid>https://brpc.incubator.apache.org/zh/docs/rpc-in-depth/memory-management/</guid>
      <description>
        
        
        &lt;p&gt;内存管理总是程序中的重要一环，在多线程时代，一个好的内存分配大都在如下两点间权衡：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;线程间竞争少。内存分配的粒度大都比较小，对性能敏感，如果不同的线程在大多数分配时会竞争同一份资源或同一把锁，性能将会非常糟糕，原因无外乎和cache一致性有关，已被大量的malloc方案证明。&lt;/li&gt;
&lt;li&gt;浪费的空间少。如果每个线程各申请各的，速度也许不错，但万一一个线程总是申请，另一个线程总是释放，内存就爆炸了。线程之间总是要共享内存的，如何共享就是方案的关键了。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;一般的应用可以使用&lt;a href=&#34;http://goog-perftools.sourceforge.net/doc/tcmalloc.html&#34;&gt;tcmalloc&lt;/a&gt;、&lt;a href=&#34;https://github.com/jemalloc/jemalloc&#34;&gt;jemalloc&lt;/a&gt;等成熟的内存分配方案，但这对于较为底层，关注性能长尾的应用是不够的。多线程框架广泛地通过传递对象的ownership来让问题异步化，如何让分配这些小对象的开销变的更小是值得研究的问题。其中的一个特点较为显著：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;大多数结构是等长的。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;这个属性可以大幅简化内存分配的过程，获得比通用malloc更稳定、快速的性能。brpc中的ResourcePool&lt;T&gt;和ObjectPool&lt;T&gt;即提供这类分配。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;这篇文章不鼓励用户使用ResourcePool&lt;T&gt;或ObjectPool&lt;T&gt;，事实上我们反对用户在程序中使用这两个类。因为”等长“的副作用是某个类型独占了一部分内存，这些内存无法再被其他类型使用，如果不加控制的滥用，反而会在程序中产生大量彼此隔离的内存分配体系，既浪费内存也不见得会有更好的性能。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h1 id=&#34;resourcepoolt&#34;&gt;ResourcePool&lt;T&gt;&lt;/h1&gt;
&lt;p&gt;创建一个类型为T的对象并返回一个偏移量，这个偏移量可以在O(1)时间内转换为对象指针。这个偏移量相当于指针，但它的值在一般情况下小于2^32，所以我们可以把它作为64位id的一部分。对象可以被归还，但归还后对象并没有删除，也没有被析构，而是仅仅进入freelist。下次申请时可能会取到这种使用过的对象，需要重置后才能使用。当对象被归还后，通过对应的偏移量仍可以访问到对象，即ResourcePool只负责内存分配，并不解决ABA问题。但对于越界的偏移量，ResourcePool会返回空。&lt;/p&gt;
&lt;p&gt;由于对象等长，ResourcePool通过批量分配和归还内存以避免全局竞争，并降低单次的开销。每个线程的分配流程如下：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;查看thread-local free block。如果还有free的对象，返回。没有的话步骤2。&lt;/li&gt;
&lt;li&gt;尝试从全局取一个free block，若取到的话回到步骤1，否则步骤3。&lt;/li&gt;
&lt;li&gt;从全局取一个block，返回其中第一个对象。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;原理是比较简单的。工程实现上数据结构、原子变量、memory fence等问题会复杂一些。下面以bthread_t的生成过程说明ResourcePool是如何被应用的。&lt;/p&gt;
&lt;h1 id=&#34;objectpoolt&#34;&gt;ObjectPool&lt;T&gt;&lt;/h1&gt;
&lt;p&gt;这是ResourcePool&lt;T&gt;的变种，不返回偏移量，而直接返回对象指针。内部结构和ResourcePool类似，一些代码更加简单。对于用户来说，这就是一个多线程下的对象池，brpc里也是这么用的。比如Socket::Write中把每个待写出的请求包装为WriteRequest，这个对象就是用ObjectPool&lt;WriteRequest&gt;分配的。&lt;/p&gt;
&lt;h1 id=&#34;生成bthread_t&#34;&gt;生成bthread_t&lt;/h1&gt;
&lt;p&gt;用户期望通过创建bthread获得更高的并发度，所以创建bthread必须很快。 在目前的实现中创建一个bthread的平均耗时小于200ns。如果每次都要从头创建，是不可能这么快的。创建过程更像是从一个bthread池子中取一个实例，我们又同时需要一个id来指代一个bthread，所以这儿正是ResourcePool的用武之地。bthread在代码中被称作Task，其结构被称为TaskMeta，定义在&lt;a href=&#34;https://github.com/brpc/brpc/blob/master/src/bthread/task_meta.h&#34;&gt;task_meta.h&lt;/a&gt;中，所有的TaskMeta由ResourcePool&lt;TaskMeta&gt;分配。&lt;/p&gt;
&lt;p&gt;bthread的大部分函数都需要在O(1)时间内通过bthread_t访问到TaskMeta，并且当bthread_t失效后，访问应返回NULL以让函数做出返回错误。解决方法是：bthread_t由32位的版本和32位的偏移量组成。版本解决&lt;a href=&#34;http://en.wikipedia.org/wiki/ABA_problem&#34;&gt;ABA问题&lt;/a&gt;，偏移量由ResourcePool&lt;TaskMeta&gt;分配。查找时先通过偏移量获得TaskMeta，再检查版本，如果版本不匹配，说明bthread失效了。注意：这只是大概的说法，在多线程环境下，即使版本相等，bthread仍可能随时失效，在不同的bthread函数中处理方法都是不同的，有些函数会加锁，有些则能忍受版本不相等。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://brpc.incubator.apache.org/images/docs/resource_pool.png&#34; alt=&#34;img&#34;&gt;&lt;/p&gt;
&lt;p&gt;这种id生成方式在brpc中应用广泛，brpc中的SocketId，bthread_id_t也是用类似的方法分配的。&lt;/p&gt;
&lt;h1 id=&#34;栈&#34;&gt;栈&lt;/h1&gt;
&lt;p&gt;使用ResourcePool加快创建的副作用是：一个pool中所有bthread的栈必须是一样大的。这似乎限制了用户的选择，不过基于我们的观察，大部分用户并不关心栈的具体大小，而只需要两种大小的栈：尺寸普通但数量较少，尺寸小但数量众多。所以我们用不同的pool管理不同大小的栈，用户可以根据场景选择。两种栈分别对应属性BTHREAD_ATTR_NORMAL（栈默认为1M）和BTHREAD_ATTR_SMALL（栈默认为32K）。用户还可以指定BTHREAD_ATTR_LARGE，这个属性的栈大小和pthread一样，由于尺寸较大，bthread不会对其做caching，创建速度较慢。server默认使用BTHREAD_ATTR_NORMAL运行用户代码。&lt;/p&gt;
&lt;p&gt;栈使用&lt;a href=&#34;http://linux.die.net/man/2/mmap&#34;&gt;mmap&lt;/a&gt;分配，bthread还会用mprotect分配4K的guard page以检测栈溢出。由于mmap+mprotect不能超过max_map_count（默认为65536），当bthread非常多后可能要调整此参数。另外当有很多bthread时，内存问题可能不仅仅是栈，也包括各类用户和系统buffer。&lt;/p&gt;
&lt;p&gt;goroutine在1.3前通过&lt;a href=&#34;https://gcc.gnu.org/wiki/SplitStacks&#34;&gt;segmented stacks&lt;/a&gt;动态地调整栈大小，发现有&lt;a href=&#34;https://docs.google.com/document/d/1wAaf1rYoM4S4gtnPh0zOlGzWtrZFQ5suE8qr2sD8uWQ/pub&#34;&gt;hot split&lt;/a&gt;问题后换成了变长连续栈（类似于vector resizing，只适合内存托管的语言）。由于bthread基本只会在64位平台上使用，虚存空间庞大，对变长栈需求不明确。加上segmented stacks的性能有影响，bthread暂时没有变长栈的计划。&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: Timer keeping</title>
      <link>https://brpc.incubator.apache.org/zh/docs/rpc-in-depth/timer-keeping/</link>
      <pubDate>Thu, 12 Aug 2021 00:00:00 +0000</pubDate>
      
      <guid>https://brpc.incubator.apache.org/zh/docs/rpc-in-depth/timer-keeping/</guid>
      <description>
        
        
        &lt;p&gt;在几点几分做某件事是RPC框架的基本需求，这件事比看上去难。&lt;/p&gt;
&lt;p&gt;让我们先来看看系统提供了些什么： posix系统能以&lt;a href=&#34;http://man7.org/linux/man-pages/man2/timer_create.2.html&#34;&gt;signal方式&lt;/a&gt;告知timer触发，不过signal逼迫我们使用全局变量，写&lt;a href=&#34;https://docs.oracle.com/cd/E19455-01/806-5257/gen-26/index.html&#34;&gt;async-signal-safe&lt;/a&gt;的函数，在面向用户的编程框架中，我们应当尽力避免使用signal。linux自2.6.27后能以&lt;a href=&#34;http://man7.org/linux/man-pages/man2/timerfd_create.2.html&#34;&gt;fd方式&lt;/a&gt;通知timer触发，这个fd可以放到epoll中和传输数据的fd统一管理。唯一问题是：这是个系统调用，且我们不清楚它在多线程下的表现。&lt;/p&gt;
&lt;p&gt;为什么这么关注timer的开销?让我们先来看一下RPC场景下一般是怎么使用timer的：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;在发起RPC过程中设定一个timer，在超时时间后取消还在等待中的RPC。几乎所有的RPC调用都有超时限制，都会设置这个timer。&lt;/li&gt;
&lt;li&gt;RPC结束前删除timer。大部分RPC都由正常返回的response导致结束，timer很少触发。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;你注意到了么，在RPC中timer更像是”保险机制”，在大部分情况下都不会发挥作用，自然地我们希望它的开销越小越好。一个几乎不触发的功能需要两次系统调用似乎并不理想。那在应用框架中一般是如何实现timer的呢？谈论这个问题需要区分“单线程”和“多线程”:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;在单线程框架中，比如以&lt;a href=&#34;http://libevent.org/&#34;&gt;libevent&lt;/a&gt;&lt;a href=&#34;http://en.wikipedia.org/wiki/Reactor_pattern&#34;&gt;, &lt;/a&gt;&lt;a href=&#34;http://software.schmorp.de/pkg/libev.html&#34;&gt;libev&lt;/a&gt;为代表的eventloop类库，或以&lt;a href=&#34;http://www.gnu.org/software/pth/pth-manual.html&#34;&gt;GNU Pth&lt;/a&gt;, &lt;a href=&#34;http://state-threads.sourceforge.net/index.html&#34;&gt;StateThreads&lt;/a&gt;为代表的coroutine / fiber类库中，一般是以&lt;a href=&#34;https://en.wikipedia.org/wiki/Heap_(data_structure)&#34;&gt;小顶堆&lt;/a&gt;记录触发时间。&lt;a href=&#34;http://man7.org/linux/man-pages/man2/epoll_wait.2.html&#34;&gt;epoll_wait&lt;/a&gt;前以堆顶的时间计算出参数timeout的值，如果在该时间内没有其他事件，epoll_wait也会醒来，从堆中弹出已超时的元素，调用相应的回调函数。整个框架周而复始地这么运转，timer的建立，等待，删除都发生在一个线程中。只要所有的回调都是非阻塞的，且逻辑不复杂，这套机制就能提供基本准确的timer。不过就像&lt;a href=&#34;threading_overview.md&#34;&gt;Threading Overview&lt;/a&gt;中说的那样，这不是RPC的场景。&lt;/li&gt;
&lt;li&gt;在多线程框架中，任何线程都可能被用户逻辑阻塞较长的时间，我们需要独立的线程实现timer，这种线程我们叫它TimerThread。一个非常自然的做法，就是使用用锁保护的小顶堆。当一个线程需要创建timer时，它先获得锁，然后把对应的时间插入堆，如果插入的元素成为了最早的，唤醒TimerThread。TimerThread中的逻辑和单线程类似，就是等着堆顶的元素超时，如果在等待过程中有更早的时间插入了，自己会被插入线程唤醒，而不会睡过头。这个方法的问题在于每个timer都需要竞争一把全局锁，操作一个全局小顶堆，就像在其他文章中反复谈到的那样，这会触发cache bouncing。同样数量的timer操作比单线程下的慢10倍是非常正常的，尴尬的是这些timer基本不触发。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;我们重点谈怎么解决多线程下的问题。&lt;/p&gt;
&lt;p&gt;一个惯例思路是把timer的需求散列到多个TimerThread，但这对TimerThread效果不好。注意我们上面提及到了那个“制约因素”：一旦插入的元素是最早的，要唤醒TimerThread。假设TimerThread足够多，以至于每个timer都散列到独立的TimerThread，那么每次它都要唤醒那个TimerThread。 “唤醒”意味着触发linux的调度函数，触发上下文切换。在非常流畅的系统中，这个开销大约是3-5微秒，这可比抢锁和同步cache还慢。这个因素是提高TimerThread扩展性的一个难点。多个TimerThread减少了对单个小顶堆的竞争压力，但同时也引入了更多唤醒。&lt;/p&gt;
&lt;p&gt;另一个难点是删除。一般用id指代一个Timer。通过这个id删除Timer有两种方式：1.抢锁，通过一个map查到对应timer在小顶堆中的位置，定点删除，这个map要和堆同步维护。2.通过id找到Timer的内存结构，做个标记，留待TimerThread自行发现和删除。第一种方法让插入逻辑更复杂了，删除也要抢锁，线程竞争更激烈。第二种方法在小顶堆内留了一大堆已删除的元素，让堆明显变大，插入和删除都变慢。&lt;/p&gt;
&lt;p&gt;第三个难点是TimerThread不应该经常醒。一个极端是TimerThread永远醒着或以较高频率醒过来（比如每1ms醒一次），这样插入timer的线程就不用负责唤醒了，然后我们把插入请求散列到多个堆降低竞争，问题看似解决了。但事实上这个方案提供的timer精度较差，一般高于2ms。你得想这个TimerThread怎么写逻辑，它是没法按堆顶元素的时间等待的，由于插入线程不唤醒，一旦有更早的元素插入，TimerThread就会睡过头。它唯一能做的是睡眠固定的时间，但这和现代OS scheduler的假设冲突：频繁sleep的线程的优先级最低。在linux下的结果就是，即使只sleep很短的时间，最终醒过来也可能超过2ms，因为在OS看来，这个线程不重要。一个高精度的TimerThread有唤醒机制，而不是定期醒。&lt;/p&gt;
&lt;p&gt;另外，更并发的数据结构也难以奏效，感兴趣的同学可以去搜索&amp;quot;concurrent priority queue&amp;quot;或&amp;quot;concurrent skip list&amp;quot;，这些数据结构一般假设插入的数值较为散开，所以可以同时修改结构内的不同部分。但这在RPC场景中也不成立，相互竞争的线程设定的时间往往聚集在同一个区域，因为程序的超时大都是一个值，加上当前时间后都差不多。&lt;/p&gt;
&lt;p&gt;这些因素让TimerThread的设计相当棘手。由于大部分用户的qps较低，不足以明显暴露这个扩展性问题，在r31791前我们一直沿用“用一把锁保护的TimerThread”。TimerThread是brpc在默认配置下唯一的高频竞争点，这个问题是我们一直清楚的技术债。随着brpc在高qps系统中应用越来越多，是时候解决这个问题了。r31791后的TimerThread解决了上述三个难点，timer操作几乎对RPC性能没有影响，我们先看下性能差异。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;在示例程序example/mutli_threaded_echo_c++中，r31791后TimerThread相比老TimerThread在24核E5-2620上（超线程），以50个bthread同步发送时，节省4%cpu（差不多1个核），qps提升10%左右；在400个bthread同步发送时，qps从30万上升到60万。新TimerThread的表现和完全关闭超时时接近。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;那新TimerThread是如何做到的？&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;一个TimerThread而不是多个。&lt;/li&gt;
&lt;li&gt;创建的timer散列到多个Bucket以降低线程间的竞争，默认13个Bucket。&lt;/li&gt;
&lt;li&gt;Bucket内不使用小顶堆管理时间，而是链表 + nearest_run_time字段，当插入的时间早于nearest_run_time时覆盖这个字段，之后去和全局nearest_run_time（和Bucket的nearest_run_time不同）比较，如果也早于这个时间，修改并唤醒TimerThread。链表节点在锁外使用&lt;a href=&#34;memory_management.md&#34;&gt;ResourcePool&lt;/a&gt;分配。&lt;/li&gt;
&lt;li&gt;删除时通过id直接定位到timer内存结构，修改一个标志，timer结构总是由TimerThread释放。&lt;/li&gt;
&lt;li&gt;TimerThread被唤醒后首先把全局nearest_run_time设置为几乎无限大(max of int64)，然后取出所有Bucket内的链表，并把Bucket的nearest_run_time设置为几乎无限大(max of int64)。TimerThread把未删除的timer插入小顶堆中维护，这个堆就它一个线程用。在每次运行回调或准备睡眠前都会检查全局nearest_run_time， 如果全局更早，说明有更早的时间加入了，重复这个过程。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;这里勾勒了TimerThread的大致工作原理，工程实现中还有不少细节问题，具体请阅读&lt;a href=&#34;https://github.com/brpc/brpc/blob/master/src/bthread/timer_thread.h&#34;&gt;timer_thread.h&lt;/a&gt;和&lt;a href=&#34;https://github.com/brpc/brpc/blob/master/src/bthread/timer_thread.cpp&#34;&gt;timer_thread.cpp&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;这个方法之所以有效：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Bucket锁内的操作是O(1)的，就是插入一个链表节点，临界区很小。节点本身的内存分配是在锁外的。&lt;/li&gt;
&lt;li&gt;由于大部分插入的时间是递增的，早于Bucket::nearest_run_time而参与全局竞争的timer很少。&lt;/li&gt;
&lt;li&gt;参与全局竞争的timer也就是和全局nearest_run_time比一下，临界区很小。&lt;/li&gt;
&lt;li&gt;和Bucket内类似，极少数Timer会早于全局nearest_run_time并去唤醒TimerThread。唤醒也在全局锁外。&lt;/li&gt;
&lt;li&gt;删除不参与全局竞争。&lt;/li&gt;
&lt;li&gt;TimerThread自己维护小顶堆，没有任何cache bouncing，效率很高。&lt;/li&gt;
&lt;li&gt;TimerThread醒来的频率大约是RPC超时的倒数，比如超时=100ms，TimerThread一秒内大约醒10次，已经最优。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;至此brpc在默认配置下不再有全局竞争点，在400个线程同时运行时，profiling也显示几乎没有对锁的等待。&lt;/p&gt;
&lt;p&gt;下面是一些和linux下时间管理相关的知识：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;epoll_wait的超时精度是毫秒，较差。pthread_cond_timedwait的超时使用timespec，精度到纳秒，一般是60微秒左右的延时。&lt;/li&gt;
&lt;li&gt;出于性能考虑，TimerThread使用wall-time，而不是单调时间，可能受到系统时间调整的影响。具体来说，如果在测试中把系统时间往前或往后调一个小时，程序行为将完全undefined。未来可能会让用户选择单调时间。&lt;/li&gt;
&lt;li&gt;在cpu支持nonstop_tsc和constant_tsc的机器上，brpc和bthread会优先使用基于rdtsc的cpuwide_time_us。那两个flag表示rdtsc可作为wall-time使用，不支持的机器上会转而使用较慢的内核时间。我们的机器（Intel Xeon系列）大都有那两个flag。rdtsc作为wall-time使用时是否会受到系统调整时间的影响，未测试不清楚。&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: bthread_id</title>
      <link>https://brpc.incubator.apache.org/zh/docs/rpc-in-depth/bthread_id/</link>
      <pubDate>Thu, 12 Aug 2021 00:00:00 +0000</pubDate>
      
      <guid>https://brpc.incubator.apache.org/zh/docs/rpc-in-depth/bthread_id/</guid>
      <description>
        
        
        &lt;p&gt;bthread_id是一个特殊的同步结构，它可以互斥RPC过程中的不同环节，也可以O(1)时间内找到RPC上下文(即Controller)。注意，这里我们谈论的是bthread_id_t，不是bthread_t（bthread的tid），这个名字起的确实不太好，容易混淆。&lt;/p&gt;
&lt;p&gt;具体来说，bthread_id解决的问题有：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;在发送RPC过程中response回来了，处理response的代码和发送代码产生竞争。&lt;/li&gt;
&lt;li&gt;设置timer后很快触发了，超时处理代码和发送代码产生竞争。&lt;/li&gt;
&lt;li&gt;重试产生的多个response同时回来产生的竞争。&lt;/li&gt;
&lt;li&gt;通过correlation_id在O(1)时间内找到对应的RPC上下文，而无需建立从correlation_id到RPC上下文的全局哈希表。&lt;/li&gt;
&lt;li&gt;取消RPC。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;上文提到的bug在其他rpc框架中广泛存在，下面我们来看下brpc是如何通过bthread_id解决这些问题的。&lt;/p&gt;
&lt;p&gt;bthread_id包括两部分，一个是用户可见的64位id，另一个是对应的不可见的bthread::Id结构体。用户接口都是操作id的。从id映射到结构体的方式和brpc中的&lt;a href=&#34;memory_management.md&#34;&gt;其他结构&lt;/a&gt;类似：32位是内存池的位移，32位是version。前者O(1)时间定位，后者防止ABA问题。&lt;/p&gt;
&lt;p&gt;bthread_id的接口不太简洁，有不少API：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;create&lt;/li&gt;
&lt;li&gt;lock&lt;/li&gt;
&lt;li&gt;unlock&lt;/li&gt;
&lt;li&gt;unlock_and_destroy&lt;/li&gt;
&lt;li&gt;join&lt;/li&gt;
&lt;li&gt;error&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;这么多接口是为了满足不同的使用流程。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;发送request的流程：bthread_id_create -&amp;gt; bthread_id_lock -&amp;gt; &amp;hellip; register timer and send RPC &amp;hellip; -&amp;gt; bthread_id_unlock&lt;/li&gt;
&lt;li&gt;接收response的流程：bthread_id_lock -&amp;gt; ..process response -&amp;gt; bthread_id_unlock_and_destroy&lt;/li&gt;
&lt;li&gt;异常处理流程：timeout/socket fail -&amp;gt; bthread_id_error -&amp;gt; 执行on_error回调(这里会加锁)，分两种情况
&lt;ul&gt;
&lt;li&gt;请求重试/backup request： 重新register timer and send RPC -&amp;gt; bthread_id_unlock&lt;/li&gt;
&lt;li&gt;无法重试，最终失败：bthread_id_unlock_and_destroy&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;同步等待RPC结束：bthread_id_join&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;为了减少等待，bthread_id做了一些优化的机制：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;error发生的时候，如果bthread_id已经被锁住，会把error信息放到一个pending queue中，bthread_id_error函数立即返回。当bthread_id_unlock的时候，如果pending queue里面有任务就取出来执行。&lt;/li&gt;
&lt;li&gt;RPC结束的时候，如果存在用户回调，先执行一个bthread_id_about_to_destroy，让正在等待的bthread_id_lock操作立即失败，再执行用户回调（这个可能耗时较长，不可控），最后再执行bthread_id_unlock_and_destroy&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
  </channel>
</rss>
